{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPM8gD9kml9GCm/1QiH0EoT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/pipelines/blob/main/pipelines_00_what_are_pipelines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What are sklearn pipelines?\n",
        "\n",
        "Sklearn pipelines are a way to streamline and automate the process of building and evaluating machine learning models. They allow you to chain together a sequence of data processing steps and a final estimator (like a classifier or regressor) into a single object. This way, you can treat the entire workflow as a single unit.\n",
        "\n",
        "### What do sklearn pipelines do?\n",
        "\n",
        "Pipelines in sklearn help you:\n",
        "\n",
        "1. **Streamline Workflows**: Combine multiple steps (e.g., preprocessing, feature extraction, model training) into one cohesive workflow.\n",
        "2. **Ensure Consistency**: Guarantee that the same transformations are applied to the training data and any new data (e.g., during prediction).\n",
        "3. **Reduce Errors**: Minimize the risk of data leakage and inconsistencies by encapsulating the steps in a fixed order.\n",
        "4. **Improve Code Maintainability**: Simplify the process of modifying or extending your workflow by having a single object to manage.\n",
        "\n",
        "### Why are sklearn pipelines important?\n",
        "\n",
        "1. **Prevent Data Leakage**: By ensuring that transformations are applied consistently, pipelines help prevent the inadvertent use of information from the test data in the training process.\n",
        "2. **Simplify Code**: They make the code more readable and modular, reducing redundancy.\n",
        "3. **Facilitate Hyperparameter Tuning**: Pipelines integrate smoothly with sklearn's hyperparameter tuning tools like `GridSearchCV` or `RandomizedSearchCV`, enabling the tuning of parameters across multiple steps of the workflow.\n",
        "\n",
        "### Role of sklearn pipelines in Machine Learning\n",
        "\n",
        "In machine learning, pipelines play a crucial role in:\n",
        "\n",
        "1. **Preprocessing Data**: Transforming raw data into a suitable format for model training (e.g., scaling features, encoding categorical variables).\n",
        "2. **Feature Engineering**: Creating new features or modifying existing ones to improve model performance.\n",
        "3. **Model Training and Evaluation**: Simplifying the process of training models and evaluating their performance on test data.\n",
        "4. **Productionizing Models**: Making it easier to deploy models in a consistent and reliable manner."
      ],
      "metadata": {
        "id": "mh5xFctHtMWZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combining Multiple Preprocessing Steps"
      ],
      "metadata": {
        "id": "Pzgu121CxwDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load Titanic dataset\n",
        "df = sns.load_dataset('titanic')\n",
        "\n",
        "# Select features and target\n",
        "X = df[['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']]\n",
        "y = df['survived']\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define preprocessing for numeric columns (impute missing values and scale)\n",
        "numeric_features = ['age', 'fare']\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())])\n",
        "\n",
        "# Define preprocessing for categorical columns (impute missing values and one-hot encode)\n",
        "categorical_features = ['embarked', 'sex', 'pclass']\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "# Combine preprocessing steps\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)])\n",
        "\n",
        "# Create the full pipeline with a classifier\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    # ('classifier', LogisticRegression(max_iter=300))])\n",
        "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))])\n",
        "\n",
        "# Train the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = pipeline.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzRMgwMktv-o",
        "outputId": "aaa04bec-520a-4582-a3de-29b74cd93c8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.82      0.82       105\n",
            "           1       0.74      0.74      0.74        74\n",
            "\n",
            "    accuracy                           0.79       179\n",
            "   macro avg       0.78      0.78      0.78       179\n",
            "weighted avg       0.79      0.79      0.79       179\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Explanation of the Code\n",
        "\n",
        "1. **Data Loading and Splitting**: Load the Titanic dataset and split it into training and testing sets.\n",
        "2. **Preprocessing Pipelines**:\n",
        "   - **Numeric Features**: Impute missing values with the median and scale the features.\n",
        "   - **Categorical Features**: Impute missing values with 'missing' and one-hot encode the features.\n",
        "3. **ColumnTransformer**: Combine the preprocessing steps for numeric and categorical features.\n",
        "4. **Pipeline**: Chain the preprocessor and the classifier into one pipeline.\n",
        "5. **Training and Evaluation**: Fit the pipeline on the training data, make predictions on the test data, and evaluate the model.\n",
        "\n",
        "This example demonstrates how sklearn pipelines can make your machine learning workflow more efficient, consistent, and easy to manage."
      ],
      "metadata": {
        "id": "j5-IYNkZtanb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Custom Transformers"
      ],
      "metadata": {
        "id": "T8dY2ZQ1wwj0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Custom transformer to add a feature\n",
        "class FamilySizeAdder(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    def transform(self, X):\n",
        "        X['family_size'] = X['sibsp'] + X['parch'] + 1\n",
        "        return X\n",
        "\n",
        "# Load Titanic dataset\n",
        "df = sns.load_dataset('titanic')\n",
        "\n",
        "# Select features and target\n",
        "X = df[['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']]\n",
        "y = df['survived']\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define preprocessing for numeric columns (impute missing values and scale)\n",
        "numeric_features = ['age', 'fare', 'family_size']\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())])\n",
        "\n",
        "# Define preprocessing for categorical columns (impute missing values and one-hot encode)\n",
        "categorical_features = ['embarked', 'sex', 'pclass']\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "# Combine preprocessing steps\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)])\n",
        "\n",
        "# Create the full pipeline with a custom transformer and a classifier\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('family_size_adder', FamilySizeAdder()),\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))])\n",
        "\n",
        "# Train the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = pipeline.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OJL9cgDwsMl",
        "outputId": "77829e77-2a7d-4eaf-dba1-74dca9e2af14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.85      0.85       105\n",
            "           1       0.79      0.80      0.79        74\n",
            "\n",
            "    accuracy                           0.83       179\n",
            "   macro avg       0.82      0.82      0.82       179\n",
            "weighted avg       0.83      0.83      0.83       179\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Integrating Grid Search for Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "lqRcyidrw4S5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Custom transformer to add a feature\n",
        "class FamilySizeAdder(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    def transform(self, X):\n",
        "        X['family_size'] = X['sibsp'] + X['parch'] + 1\n",
        "        return X\n",
        "\n",
        "# Load Titanic dataset\n",
        "df = sns.load_dataset('titanic')\n",
        "\n",
        "# Select features and target\n",
        "X = df[['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']]\n",
        "y = df['survived']\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define preprocessing for numeric columns (impute missing values and scale)\n",
        "numeric_features = ['age', 'fare']\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())])\n",
        "\n",
        "# Define preprocessing for categorical columns (impute missing values and one-hot encode)\n",
        "categorical_features = ['embarked', 'sex', 'pclass']\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "# Combine preprocessing steps\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)])\n",
        "\n",
        "# Create the full pipeline with a classifier\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('family_size_adder', FamilySizeAdder()),\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', RandomForestClassifier(random_state=42))])\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'classifier__n_estimators': [50, 100, 200],\n",
        "    'classifier__max_features': ['sqrt', 'log2'],\n",
        "    'classifier__max_depth': [4, 6, 8, 10]\n",
        "}\n",
        "\n",
        "# Apply grid search\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = grid_search.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQVaVMSCw5Mt",
        "outputId": "1c90df1d-1312-4b40-807e-798a3b6087a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.91      0.85       105\n",
            "           1       0.85      0.68      0.75        74\n",
            "\n",
            "    accuracy                           0.82       179\n",
            "   macro avg       0.82      0.79      0.80       179\n",
            "weighted avg       0.82      0.82      0.81       179\n",
            "\n",
            "Best parameters: {'classifier__max_depth': 6, 'classifier__max_features': 'sqrt', 'classifier__n_estimators': 200}\n"
          ]
        }
      ]
    }
  ]
}