{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN8p2tbTwc0k6/nKvzUVEpj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/pipelines/blob/main/pipelines_00_why_use_piplelines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What are sklearn Pipelines?\n",
        "\n",
        "In machine learning, pipelines help streamline the process of preprocessing data and training models. A pipeline chains together multiple steps into a single object, making the workflow cleaner, more maintainable, and less error-prone.\n",
        "\n",
        "### Why use sklearn Pipelines?\n",
        "\n",
        "1. **Consistency**:\n",
        "   - **End-to-End Workflow**: Pipelines ensure that the same preprocessing steps are consistently applied during both training and testing phases.\n",
        "   - **No Data Leakage**: Pipelines help prevent data leakage by ensuring that each step in the preprocessing pipeline is applied correctly.\n",
        "\n",
        "2. **Clean and Maintainable Code**:\n",
        "   - **Modular Design**: Pipelines organize code into modular, reusable components, making it easier to read, debug, and maintain.\n",
        "   - **Single Object**: The entire workflow, including preprocessing and modeling, is encapsulated in a single object, simplifying the overall process.\n",
        "\n",
        "3. **Simplified Workflow**:\n",
        "   - **Chaining Steps**: Pipelines allow you to chain multiple preprocessing steps and the estimator into a single object, simplifying the workflow.\n",
        "   - **Ease of Use**: Once defined, pipelines can be easily used for fitting, predicting, and evaluating without repeatedly applying the same preprocessing steps.\n",
        "\n",
        "### Basic Example\n",
        "\n",
        "Let's create a basic pipeline that includes data preprocessing and a simple classifier.\n",
        "\n",
        "1. **StandardScaler:** This step standardizes the features by removing the mean and scaling to unit variance.\n",
        "2. **LogisticRegression:** This is the classifier used in the pipeline.\n",
        "\n"
      ],
      "metadata": {
        "id": "Q3VyV15y48RB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Fetch the data\n",
        "data = fetch_openml(data_id=1590, as_frame=True, parser='auto')\n",
        "df = data.frame\n",
        "\n",
        "# Features and target\n",
        "X = df.drop(columns='class')\n",
        "y = df['class']\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Identify categorical and numeric columns\n",
        "categorical_features = X.select_dtypes(include=['category']).columns\n",
        "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "# Preprocessing for numeric features\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# Preprocessing for categorical features\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# Combine preprocessing steps\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "# Create a pipeline\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(max_iter=300))\n",
        "])\n",
        "\n",
        "# Train the pipeline\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "# print(f'Accuracy: {accuracy}')\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4oK9F7Ba52M0",
        "outputId": "ce0efbc8-bb31-4fd2-8455-88ecc10409b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       <=50K       0.88      0.93      0.91      7479\n",
            "        >50K       0.73      0.60      0.66      2290\n",
            "\n",
            "    accuracy                           0.86      9769\n",
            "   macro avg       0.81      0.77      0.78      9769\n",
            "weighted avg       0.85      0.86      0.85      9769\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "8Xjkn4pb7FD9",
        "outputId": "7911fe14-668b-4e3a-9682-530bc280e0ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('preprocessor',\n",
              "                 ColumnTransformer(transformers=[('num',\n",
              "                                                  Pipeline(steps=[('imputer',\n",
              "                                                                   SimpleImputer(strategy='median')),\n",
              "                                                                  ('scaler',\n",
              "                                                                   StandardScaler())]),\n",
              "                                                  Index(['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss',\n",
              "       'hours-per-week'],\n",
              "      dtype='object')),\n",
              "                                                 ('cat',\n",
              "                                                  Pipeline(steps=[('imputer',\n",
              "                                                                   SimpleImputer(strategy='most_frequent')),\n",
              "                                                                  ('onehot',\n",
              "                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
              "                                                  Index([], dtype='object'))])),\n",
              "                ('classifier', LogisticRegression(max_iter=200))])"
            ],
            "text/html": [
              "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
              "                 ColumnTransformer(transformers=[(&#x27;num&#x27;,\n",
              "                                                  Pipeline(steps=[(&#x27;imputer&#x27;,\n",
              "                                                                   SimpleImputer(strategy=&#x27;median&#x27;)),\n",
              "                                                                  (&#x27;scaler&#x27;,\n",
              "                                                                   StandardScaler())]),\n",
              "                                                  Index([&#x27;age&#x27;, &#x27;fnlwgt&#x27;, &#x27;education-num&#x27;, &#x27;capital-gain&#x27;, &#x27;capital-loss&#x27;,\n",
              "       &#x27;hours-per-week&#x27;],\n",
              "      dtype=&#x27;object&#x27;)),\n",
              "                                                 (&#x27;cat&#x27;,\n",
              "                                                  Pipeline(steps=[(&#x27;imputer&#x27;,\n",
              "                                                                   SimpleImputer(strategy=&#x27;most_frequent&#x27;)),\n",
              "                                                                  (&#x27;onehot&#x27;,\n",
              "                                                                   OneHotEncoder(handle_unknown=&#x27;ignore&#x27;))]),\n",
              "                                                  Index([], dtype=&#x27;object&#x27;))])),\n",
              "                (&#x27;classifier&#x27;, LogisticRegression(max_iter=200))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
              "                 ColumnTransformer(transformers=[(&#x27;num&#x27;,\n",
              "                                                  Pipeline(steps=[(&#x27;imputer&#x27;,\n",
              "                                                                   SimpleImputer(strategy=&#x27;median&#x27;)),\n",
              "                                                                  (&#x27;scaler&#x27;,\n",
              "                                                                   StandardScaler())]),\n",
              "                                                  Index([&#x27;age&#x27;, &#x27;fnlwgt&#x27;, &#x27;education-num&#x27;, &#x27;capital-gain&#x27;, &#x27;capital-loss&#x27;,\n",
              "       &#x27;hours-per-week&#x27;],\n",
              "      dtype=&#x27;object&#x27;)),\n",
              "                                                 (&#x27;cat&#x27;,\n",
              "                                                  Pipeline(steps=[(&#x27;imputer&#x27;,\n",
              "                                                                   SimpleImputer(strategy=&#x27;most_frequent&#x27;)),\n",
              "                                                                  (&#x27;onehot&#x27;,\n",
              "                                                                   OneHotEncoder(handle_unknown=&#x27;ignore&#x27;))]),\n",
              "                                                  Index([], dtype=&#x27;object&#x27;))])),\n",
              "                (&#x27;classifier&#x27;, LogisticRegression(max_iter=200))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">preprocessor: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;num&#x27;,\n",
              "                                 Pipeline(steps=[(&#x27;imputer&#x27;,\n",
              "                                                  SimpleImputer(strategy=&#x27;median&#x27;)),\n",
              "                                                 (&#x27;scaler&#x27;, StandardScaler())]),\n",
              "                                 Index([&#x27;age&#x27;, &#x27;fnlwgt&#x27;, &#x27;education-num&#x27;, &#x27;capital-gain&#x27;, &#x27;capital-loss&#x27;,\n",
              "       &#x27;hours-per-week&#x27;],\n",
              "      dtype=&#x27;object&#x27;)),\n",
              "                                (&#x27;cat&#x27;,\n",
              "                                 Pipeline(steps=[(&#x27;imputer&#x27;,\n",
              "                                                  SimpleImputer(strategy=&#x27;most_frequent&#x27;)),\n",
              "                                                 (&#x27;onehot&#x27;,\n",
              "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;))]),\n",
              "                                 Index([], dtype=&#x27;object&#x27;))])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">num</label><div class=\"sk-toggleable__content\"><pre>Index([&#x27;age&#x27;, &#x27;fnlwgt&#x27;, &#x27;education-num&#x27;, &#x27;capital-gain&#x27;, &#x27;capital-loss&#x27;,\n",
              "       &#x27;hours-per-week&#x27;],\n",
              "      dtype=&#x27;object&#x27;)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer(strategy=&#x27;median&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">cat</label><div class=\"sk-toggleable__content\"><pre>Index([], dtype=&#x27;object&#x27;)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer(strategy=&#x27;most_frequent&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)</pre></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=200)</pre></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Combined Benefits of GridSearchCV and Pipelines\n",
        "\n",
        "1. **Integrated Hyperparameter Tuning**:\n",
        "   - **Simultaneous Optimization**: You can tune hyperparameters for both preprocessing steps and the model within a single grid search, optimizing the entire workflow.\n",
        "   - **Comprehensive Search**: Allows for a comprehensive search over multiple preprocessing and model parameters simultaneously.\n",
        "\n",
        "2. **Enhanced Reproducibility**:\n",
        "   - **Consistent Application**: Ensures that all preprocessing and model steps are consistently applied across different runs, enhancing reproducibility.\n",
        "   - **Documented Workflow**: A pipeline combined with GridSearchCV provides a clear and documented workflow, making it easier to reproduce results.\n",
        "\n",
        "3. **Reduced Risk of Errors**:\n",
        "   - **Automated Handling**: Automates the handling of data preprocessing and model fitting, reducing the risk of manual errors.\n",
        "   - **Improved Validation**: Cross-validation within GridSearchCV ensures robust validation, reducing the likelihood of overfitting or underfitting.\n",
        "\n",
        "### Example Scenario\n",
        "\n",
        "Imagine you are building a machine learning model to predict whether an individual earns more than $50,000 a year based on various demographic features (like age, education, and occupation).\n",
        "\n",
        "- **Without Pipelines and GridSearchCV**: You might manually preprocess the data (handling missing values, scaling features, encoding categories), split the data, fit the model, and manually tune hyperparameters. This approach can lead to inconsistencies, data leakage, and manual errors.\n",
        "  \n",
        "- **With Pipelines and GridSearchCV**: You define a pipeline that handles all preprocessing steps and the model. GridSearchCV is used to automatically tune hyperparameters for both preprocessing steps (like scaling parameters) and the model. This ensures a consistent, error-free, and efficient workflow, leading to better model performance and reliability.\n"
      ],
      "metadata": {
        "id": "hEiUelBY-TZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch the data\n",
        "data = fetch_openml(data_id=1590, as_frame=True, parser='auto')\n",
        "df = data.frame\n",
        "\n",
        "# Features and target\n",
        "X = df.drop(columns='class')\n",
        "y = df['class']\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Identify categorical and numeric columns\n",
        "categorical_features = X.select_dtypes(include=['category']).columns\n",
        "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "# Preprocessing for numeric features\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# Preprocessing for categorical features\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# Combine preprocessing steps\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "# Create a pipeline\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(max_iter=300))\n",
        "])\n",
        "\n",
        "# Define the grid search parameters\n",
        "param_grid = {\n",
        "    'classifier__C': [0.1, 1.0, 10],\n",
        "    'classifier__solver': ['liblinear', 'saga']\n",
        "}\n",
        "\n",
        "# Create a GridSearchCV object\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1, verbose=1)\n",
        "\n",
        "# Train the pipeline with grid search\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = grid_search.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(f'Best parameters: {grid_search.best_params_}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOANgEym724B",
        "outputId": "e7896151-5250-482f-9513-45c307ebd923"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       <=50K       0.88      0.93      0.91      7479\n",
            "        >50K       0.73      0.60      0.66      2290\n",
            "\n",
            "    accuracy                           0.86      9769\n",
            "   macro avg       0.81      0.77      0.78      9769\n",
            "weighted avg       0.85      0.86      0.85      9769\n",
            "\n",
            "Best parameters: {'classifier__C': 1.0, 'classifier__solver': 'liblinear'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### How GridSearchCV Reduces Overfitting\n",
        "\n",
        "Overfitting occurs when a model learns the noise in the training data rather than the actual underlying patterns. This leads to excellent performance on the training data but poor generalization to new, unseen data.\n",
        "\n",
        "1. **Cross-Validation**:\n",
        "   - **K-Fold Cross-Validation**: GridSearchCV uses k-fold cross-validation, where the training data is split into `k` subsets (folds). The model is trained on `k-1` folds and validated on the remaining fold. This process is repeated `k` times, each time with a different fold as the validation set. The average performance across all `k` folds provides a more reliable estimate of the model's performance.\n",
        "   - **Model Generalization**: By evaluating the model on multiple folds, GridSearchCV ensures that the hyperparameters are chosen based on their performance on multiple data subsets, not just a single training/validation split. This helps in selecting hyperparameters that generalize well to new data.\n",
        "\n",
        "2. **Robust Hyperparameter Tuning**:\n",
        "   - **Comprehensive Search**: GridSearchCV performs an exhaustive search over a specified hyperparameter space. This helps in identifying the best combination of hyperparameters that leads to optimal model performance.\n",
        "   - **Avoiding Overfitting**: By considering multiple hyperparameter combinations and evaluating them through cross-validation, GridSearchCV reduces the risk of overfitting to the training data. It helps in selecting hyperparameters that perform well across different validation sets, indicating better generalization.\n",
        "\n",
        "3. **Automated and Consistent Process**:\n",
        "   - **Consistent Evaluation**: GridSearchCV ensures that each hyperparameter combination is evaluated consistently using the same cross-validation procedure. This uniformity reduces the likelihood of overfitting caused by inconsistent evaluation practices.\n",
        "   - **Automated Handling**: The automated process reduces the risk of manual errors that can lead to overfitting, such as reusing validation data or improperly splitting the dataset.\n"
      ],
      "metadata": {
        "id": "9Gh2IeKB8WMo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Integrated hyperparameter tuning allows you to optimize hyperparameters for both the preprocessing steps and the model itself within a single grid search. This is one of the powerful features of scikit-learn's pipeline and GridSearchCV.\n",
        "\n",
        "### Optimize Model Params and Preprocessing\n",
        "\n",
        "1. **Simultaneous Optimization**:\n",
        "   - **Preprocessing and Model**: You can optimize parameters for data preprocessing (e.g., imputation strategy, scaling) and model hyperparameters (e.g., regularization strength, learning rate) together.\n",
        "   - **Unified Search**: Instead of separately tuning preprocessing and model parameters, you define a single parameter grid that includes both, and GridSearchCV will search through all combinations.\n",
        "\n",
        "2. **Comprehensive Workflow Optimization**:\n",
        "   - **Holistic Approach**: The pipeline allows you to consider how preprocessing and model parameters interact and find the best combination that works together optimally.\n",
        "   - **Efficiency**: This approach is more efficient and streamlined, as it avoids the need for separate tuning stages for preprocessing and modeling.\n",
        "\n",
        "### Explanation\n",
        "\n",
        "1. **Hyperparameter Grid**:\n",
        "   - **Preprocessing Parameters**: `preprocessor__num__imputer__strategy` specifies the imputation strategy for numeric features. This allows GridSearchCV to test both 'mean' and 'median' strategies.\n",
        "   - **Model Parameters**: `classifier__C` and `classifier__solver` are hyperparameters for the logistic regression model.\n",
        "\n",
        "2. **Nested Parameters**:\n",
        "   - Parameters are specified in a hierarchical manner using double underscores (`__`). This notation helps GridSearchCV understand which parameters belong to which step in the pipeline.\n",
        "\n",
        "3. **Grid Search Execution**:\n",
        "   - **Unified Tuning**: GridSearchCV searches over the combined hyperparameter space, optimizing both preprocessing and model parameters simultaneously.\n",
        "   - **Cross-Validation**: Each combination of hyperparameters is evaluated using cross-validation, ensuring robust performance estimates.\n",
        "\n",
        "### Benefits\n",
        "\n",
        "- **Comprehensive Optimization**: Finds the best combination of preprocessing and model hyperparameters together, ensuring they work well in concert.\n",
        "- **Efficiency**: Streamlines the tuning process into a single operation, avoiding separate tuning stages.\n",
        "- **Consistency**: Ensures consistent application of preprocessing and model fitting steps across all data splits during cross-validation.\n",
        "\n",
        "By integrating hyperparameter tuning within pipelines, you achieve a more efficient, comprehensive, and reliable machine learning workflow, leading to models that generalize better to new data."
      ],
      "metadata": {
        "id": "gqYo87tYB3GY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Fetch the data\n",
        "data = fetch_openml(data_id=1590, as_frame=True, parser='auto')\n",
        "df = data.frame\n",
        "\n",
        "# Features and target\n",
        "X = df.drop(columns='class')\n",
        "y = df['class']\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Identify categorical and numeric columns\n",
        "categorical_features = X.select_dtypes(include=['category']).columns\n",
        "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "# Preprocessing for numeric features\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer()),  # Imputer with default strategy (mean)\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# Preprocessing for categorical features\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# Combine preprocessing steps\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "# Create a pipeline\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(max_iter=200))\n",
        "])\n",
        "\n",
        "# Define the grid search parameters\n",
        "param_grid = {\n",
        "    'preprocessor__num__imputer__strategy': ['mean', 'median'],  # Tuning imputer strategy\n",
        "    'classifier__C': [0.1, 1.0, 10],  # Tuning regularization strength\n",
        "    'classifier__solver': ['liblinear', 'saga']  # Tuning solver\n",
        "}\n",
        "\n",
        "# Create a GridSearchCV object\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1, verbose=1)\n",
        "\n",
        "# Train the pipeline with grid search\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate the model\n",
        "y_pred = grid_search.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(f'Best parameters: {grid_search.best_params_}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RB59Pbew-pVB",
        "outputId": "676b5bec-9b2f-4ff1-ebdc-d17160e14277"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       <=50K       0.88      0.93      0.91      7479\n",
            "        >50K       0.73      0.60      0.66      2290\n",
            "\n",
            "    accuracy                           0.86      9769\n",
            "   macro avg       0.81      0.77      0.78      9769\n",
            "weighted avg       0.85      0.86      0.85      9769\n",
            "\n",
            "Best parameters: {'classifier__C': 1.0, 'classifier__solver': 'liblinear', 'preprocessor__num__imputer__strategy': 'mean'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom Transformers: Overview and Importance\n",
        "\n",
        "#### What Are Custom Transformers?\n",
        "\n",
        "Custom transformers are user-defined preprocessing components in a machine learning pipeline. They extend the functionality of scikit-learn's built-in transformers, allowing you to implement specific data transformation tasks that are not covered by the default options. These transformers follow the same interface as scikit-learn's built-in transformers, typically by inheriting from `BaseEstimator` and `TransformerMixin`.\n",
        "\n",
        "#### Why Are Custom Transformers Important?\n",
        "\n",
        "1. **Flexibility**:\n",
        "   - Custom transformers allow you to handle unique preprocessing tasks tailored to your specific dataset and problem, which may not be achievable using existing scikit-learn transformers.\n",
        "\n",
        "2. **Reusability**:\n",
        "   - Once created, custom transformers can be reused across different projects and datasets, promoting code reuse and modularity.\n",
        "\n",
        "3. **Maintainability**:\n",
        "   - Encapsulating preprocessing logic within custom transformers makes your code more organized and maintainable. This separation of concerns helps in managing complex preprocessing workflows more effectively.\n",
        "\n",
        "4. **Pipeline Integration**:\n",
        "   - Custom transformers seamlessly integrate with scikit-learn's pipeline API, enabling a unified and consistent preprocessing and modeling workflow. This ensures that preprocessing steps are consistently applied during both training and inference.\n",
        "\n",
        "5. **Enhanced Functionality**:\n",
        "   - They can perform complex data transformations, feature engineering, and data augmentation techniques that go beyond standard preprocessing steps like scaling or encoding.\n",
        "\n",
        "6. **Automated Hyperparameter Tuning**:\n",
        "   - By including custom transformers in a pipeline, you can leverage tools like `GridSearchCV` to tune hyperparameters for both preprocessing steps and model parameters simultaneously, optimizing the entire machine learning workflow.\n",
        "\n",
        "#### Examples of Custom Transformer Use Cases\n",
        "\n",
        "1. **Feature Engineering**:\n",
        "   - Creating interaction terms, polynomial features, or domain-specific features that capture important relationships within the data.\n",
        "\n",
        "2. **Custom Scaling**:\n",
        "   - Implementing scaling methods not provided by default, such as scaling by the maximum absolute value or custom normalization techniques.\n",
        "\n",
        "3. **Data Augmentation**:\n",
        "   - Generating synthetic data points or applying transformations that increase the diversity of the training data, especially useful in computer vision and natural language processing.\n",
        "\n",
        "4. **Data Cleaning**:\n",
        "   - Applying domain-specific cleaning operations, such as correcting data entry errors or handling rare categories in categorical features.\n",
        "\n",
        "5. **Statistical Transformations**:\n",
        "   - Implementing transformations based on statistical properties of the data, such as log transformations or Box-Cox transformations to handle skewed distributions.\n",
        "\n",
        "By creating and using custom transformers, data scientists and machine learning engineers can build more robust, flexible, and maintainable preprocessing pipelines tailored to their specific needs."
      ],
      "metadata": {
        "id": "8mOm8EjkSUuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import numpy as np\n",
        "\n",
        "# Fetch the data\n",
        "data = fetch_openml(data_id=1590, as_frame=True, parser='auto')\n",
        "df = data.frame\n",
        "\n",
        "# Features and target\n",
        "X = df.drop(columns='class')\n",
        "y = df['class']\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Custom Transformer to apply a log transformation to a specified column\n",
        "class LogTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, column):\n",
        "        self.column = column\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        X[self.column] = np.log1p(X[self.column])\n",
        "        return X\n",
        "\n",
        "# Identify categorical and numeric columns\n",
        "categorical_features = X.select_dtypes(include=['category']).columns\n",
        "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "# Preprocessing for numeric features\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# Preprocessing for categorical features\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# Combine preprocessing steps\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "# Create a pipeline including the custom transformer\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('log_transform', LogTransformer(column='age')),  # Example: Log-transform the 'age' column\n",
        "    ('classifier', LogisticRegression(max_iter=200))\n",
        "])\n",
        "\n",
        "# Define the grid search parameters\n",
        "param_grid = {\n",
        "    'preprocessor__num__imputer__strategy': ['mean', 'median'],  # Tuning imputer strategy\n",
        "    'classifier__C': [0.1, 1.0, 10],  # Tuning regularization strength\n",
        "    'classifier__solver': ['liblinear', 'saga']  # Tuning solver\n",
        "}\n",
        "\n",
        "# Create a GridSearchCV object\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1, verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# try:\n",
        "#     # Train the pipeline with grid search\n",
        "#     grid_search.fit(X_train, y_train)\n",
        "#     # Make predictions and evaluate the model\n",
        "#     y_pred = grid_search.predict(X_test)\n",
        "#     accuracy = accuracy_score(y_test, y_pred)\n",
        "#     print(f'Best parameters: {grid_search.best_params_}')\n",
        "#     print(f'Accuracy: {accuracy}')\n",
        "# except ValueError as e:\n",
        "#     print(f\"ValueError during model fitting: {e}\")\n",
        "# except Exception as e:\n",
        "#     print(f\"An error occurred: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOzWz0nET1yb",
        "outputId": "b25b4794-5177-4866-c3bd-b68d277217f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       <=50K       0.88      0.93      0.91      7479\n",
            "        >50K       0.73      0.60      0.66      2290\n",
            "\n",
            "    accuracy                           0.86      9769\n",
            "   macro avg       0.81      0.77      0.78      9769\n",
            "weighted avg       0.85      0.86      0.85      9769\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Union\n",
        "\n",
        "1. **FeatureUnion**:\n",
        "   - Combines the output of multiple transformer objects into a single feature space.\n",
        "   - In this example, we combined the standard numeric and categorical preprocessing with the custom log transformation and adding a constant value.\n",
        "\n",
        "2. **Custom Transformers**:\n",
        "   - `LogTransformer`: Applies a logarithmic transformation to the specified column.\n",
        "   - `AddConstantTransformer`: Adds a specified constant value to the specified column.\n",
        "\n",
        "3. **ColumnTransformer**:\n",
        "   - Combines the feature union with the specified numeric and categorical columns.\n",
        "\n",
        "4. **Pipeline Integration**:\n",
        "   - Integrates the combined preprocessing steps and logistic regression model into a single pipeline.\n",
        "\n",
        "This code demonstrates how to use Feature Union to combine different feature extraction methods within a scikit-learn pipeline, enhancing the flexibility and power of your preprocessing steps.\n",
        "\n"
      ],
      "metadata": {
        "id": "vK7ESjE_VkVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import numpy as np\n",
        "\n",
        "# Fetch the data\n",
        "data = fetch_openml(data_id=1590, as_frame=True, parser='auto')\n",
        "df = data.frame\n",
        "\n",
        "# Features and target\n",
        "X = df.drop(columns='class')\n",
        "y = df['class']\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Custom Transformer to apply a log transformation to a specified column\n",
        "class LogTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, column):\n",
        "        self.column = column\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        X[self.column] = np.log1p(X[self.column])\n",
        "        return X\n",
        "\n",
        "# Custom Transformer to bin a numeric column into categories\n",
        "class BinningTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, column, bins, labels):\n",
        "        self.column = column\n",
        "        self.bins = bins\n",
        "        self.labels = labels\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        X[self.column] = pd.cut(X[self.column], bins=self.bins, labels=self.labels)\n",
        "        return X\n",
        "\n",
        "# Identify categorical and numeric columns\n",
        "categorical_features = X.select_dtypes(include=['category']).columns\n",
        "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "# Preprocessing for numeric features\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# Preprocessing for categorical features\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# Combine preprocessing steps\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "# Feature Union to combine multiple feature extraction methods\n",
        "feature_union = FeatureUnion(transformer_list=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('log_transform', LogTransformer(column='age')),\n",
        "    ('binning', BinningTransformer(column='education-num', bins=[0, 5, 10, 15, 20], labels=['low', 'medium', 'high', 'very high'])),\n",
        "])\n",
        "\n",
        "# Create a pipeline including the custom transformer\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('features', feature_union),\n",
        "    ('classifier', LogisticRegression(max_iter=200))\n",
        "])\n",
        "\n",
        "# Define the grid search parameters\n",
        "param_grid = {\n",
        "    'classifier__C': [0.1, 1.0, 10],  # Tuning regularization strength\n",
        "    'classifier__solver': ['liblinear', 'saga']  # Tuning solver\n",
        "}\n",
        "\n",
        "# Create a GridSearchCV object\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1, verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# try:\n",
        "#     # Train the pipeline with grid search\n",
        "#     grid_search.fit(X_train, y_train)\n",
        "#     # Make predictions and evaluate the model\n",
        "#     y_pred = grid_search.predict(X_test)\n",
        "#     accuracy = accuracy_score(y_test, y_pred)\n",
        "#     print(f'Best parameters: {grid_search.best_params_}')\n",
        "#     print(f'Accuracy: {accuracy}')\n",
        "\n",
        "#     # Evaluate the model\n",
        "#     print(classification_report(y_test, y_pred))\n",
        "# except ValueError as e:\n",
        "#     print(f\"ValueError during model fitting: {e}\")\n",
        "# except Exception as e:\n",
        "#     print(f\"An error occurred: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4azM19K3UK26",
        "outputId": "057d1d02-e85b-4454-d60e-d0ef53d12537"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       <=50K       0.84      0.95      0.89      7479\n",
            "        >50K       0.70      0.39      0.50      2290\n",
            "\n",
            "    accuracy                           0.82      9769\n",
            "   macro avg       0.77      0.67      0.70      9769\n",
            "weighted avg       0.80      0.82      0.80      9769\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Ensembling with Voting Classifier\n",
        "\n",
        "#### What Are They?\n",
        "\n",
        "**Model ensembling** is a machine learning technique where multiple models (often called base models or learners) are combined to produce a single, stronger model. A **Voting Classifier** is a specific type of ensemble method that aggregates the predictions of multiple models and makes a final prediction based on a majority vote (for classification) or an average (for regression).\n",
        "\n",
        "#### What Do They Do?\n",
        "\n",
        "A Voting Classifier combines the predictions from different models to improve the overall performance. There are two main types of voting:\n",
        "\n",
        "1. **Hard Voting**: Each base model makes a prediction (a class label for classification problems), and the final prediction is the one that receives the majority of the votes.\n",
        "2. **Soft Voting**: Each base model outputs a probability (a confidence level for each class), and the final prediction is based on the average of these probabilities.\n",
        "\n",
        "#### Benefits of Using Them\n",
        "\n",
        "1. **Improved Accuracy**:\n",
        "   - By combining the strengths of multiple models, Voting Classifiers can often achieve better accuracy than any single model, especially if the individual models are diverse and perform differently on different parts of the data.\n",
        "\n",
        "2. **Robustness**:\n",
        "   - Combining predictions from multiple models can lead to more robust results. It reduces the risk of a poor model impacting the overall performance because the ensemble can compensate for weaker models.\n",
        "\n",
        "3. **Reduced Overfitting**:\n",
        "   - Ensembling helps in reducing overfitting, particularly when individual models tend to overfit the data. The combined model is less likely to overfit as it averages out the biases of individual models.\n",
        "\n",
        "4. **Ease of Implementation**:\n",
        "   - Voting Classifiers are relatively simple to implement using libraries like scikit-learn. They allow you to leverage existing models without needing to create entirely new algorithms.\n",
        "\n",
        "### Summary\n",
        "\n",
        "A Voting Classifier is a simple yet powerful ensemble technique that improves model performance by combining multiple base models. It enhances accuracy, robustness, and generalization by leveraging the strengths of different models and mitigating their individual weaknesses."
      ],
      "metadata": {
        "id": "W73GtkNDaYh5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Fetch the data\n",
        "data = fetch_openml(data_id=1590, as_frame=True, parser='auto')\n",
        "df = data.frame\n",
        "\n",
        "# Features and target\n",
        "X = df.drop(columns='class')\n",
        "y = df['class']\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Identify categorical and numeric columns\n",
        "categorical_features = X.select_dtypes(include=['category']).columns\n",
        "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "# Preprocessing for numeric features\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# Preprocessing for categorical features\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# Combine preprocessing steps\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "# Define individual models\n",
        "logistic = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(max_iter=300))\n",
        "])\n",
        "\n",
        "decision_tree = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', DecisionTreeClassifier())\n",
        "])\n",
        "\n",
        "random_forest = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', RandomForestClassifier())\n",
        "])\n",
        "\n",
        "# Combine models in a VotingClassifier\n",
        "voting_clf = VotingClassifier(estimators=[\n",
        "    ('lr', logistic),\n",
        "    ('dt', decision_tree),\n",
        "    ('rf', random_forest)\n",
        "], voting='hard')\n",
        "\n",
        "# Train the VotingClassifier\n",
        "voting_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate the model\n",
        "y_pred = voting_clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy}')\n",
        "\n",
        "# Evaluate the model\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_dHE7pzV9AA",
        "outputId": "f84b1709-ab18-4048-f594-c6b935cdad7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8628314054662709\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       <=50K       0.89      0.93      0.91      7479\n",
            "        >50K       0.74      0.64      0.69      2290\n",
            "\n",
            "    accuracy                           0.86      9769\n",
            "   macro avg       0.82      0.78      0.80      9769\n",
            "weighted avg       0.86      0.86      0.86      9769\n",
            "\n"
          ]
        }
      ]
    }
  ]
}