{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jntwBzHi2vJd"
      },
      "source": [
        "### Why Perform Feature Selection Using Pipelines?\n",
        "\n",
        "**Feature selection** is the process of selecting a subset of relevant features for use in model construction. Integrating feature selection within a pipeline offers several benefits:\n",
        "\n",
        "1. **Consistency and Reproducibility**:\n",
        "   - By including feature selection in the pipeline, you ensure that the same selection process is applied consistently across different stages of model development (training, validation, testing).\n",
        "   - This reproducibility is crucial when moving from development to production, as the same steps will be applied without manual intervention.\n",
        "\n",
        "2. **Simplified Workflow**:\n",
        "   - Pipelines encapsulate the entire workflow, reducing the complexity of the code and making it more maintainable.\n",
        "   - This simplification helps in debugging and makes the process easier to understand and document.\n",
        "\n",
        "3. **Avoid Data Leakage**:\n",
        "   - Data leakage occurs when information from outside the training dataset is used to create the model, leading to overly optimistic performance estimates.\n",
        "   - By using a pipeline, you can ensure that feature selection is performed on the training data alone, preventing leakage of test data information into the training process.\n",
        "\n",
        "4. **Automated Hyperparameter Tuning**:\n",
        "   - When you use pipelines, you can easily integrate hyperparameter tuning (e.g., using `GridSearchCV` or `RandomizedSearchCV`) to select the best parameters for both feature selection and the model.\n",
        "   - This integration allows for a more comprehensive search of the best configuration, optimizing both the preprocessing and model training stages simultaneously.\n",
        "\n",
        "5. **Modularity and Reusability**:\n",
        "   - Pipelines make it easy to modify or extend your workflow. For example, you can switch out the feature selection method or add new preprocessing steps with minimal changes to the overall structure.\n",
        "   - This modularity is beneficial when experimenting with different models or preprocessing techniques.\n",
        "\n",
        "### Why Use a Pipeline Instead of Doing It Manually?\n",
        "\n",
        "**Manual Feature Selection**:\n",
        "- **Error-Prone**: Manually applying feature selection can lead to inconsistencies and errors, especially in complex workflows.\n",
        "- **Time-Consuming**: Requires more effort to ensure that the same steps are applied consistently across different datasets (training, validation, testing).\n",
        "- **Less Reproducible**: Manual processes are harder to document and reproduce, leading to potential issues when transitioning from development to production.\n",
        "\n",
        "**Using Pipelines**:\n",
        "- **Automated Workflow**: Encapsulates the entire process, from preprocessing to feature selection to model training, into a single, cohesive workflow.\n",
        "- **Consistent Application**: Ensures that all steps are applied in a consistent manner, reducing the risk of errors and improving reproducibility.\n",
        "- **Ease of Use**: Simplifies the experimentation process, making it easy to swap out or modify different components of the workflow.\n",
        "- **Integrated Hyperparameter Tuning**: Allows for comprehensive optimization of both preprocessing and model training parameters in a unified framework.\n",
        "\n",
        "### Example Benefits of Using a Pipeline for Feature Selection\n",
        "\n",
        "1. **Consistent Training and Evaluation**: Ensures that feature selection is applied consistently during cross-validation, avoiding data leakage and providing a more accurate estimate of model performance.\n",
        "2. **Simplified Code**: Reduces boilerplate code and the potential for bugs by encapsulating feature selection within the pipeline.\n",
        "3. **Efficient Hyperparameter Tuning**: Facilitates the optimization of feature selection parameters alongside model parameters, leading to better overall performance.\n",
        "\n",
        "By leveraging pipelines for feature selection, you gain a more robust, maintainable, and scalable approach to building and evaluating machine learning models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXXe_k2Y-8Mw"
      },
      "source": [
        "### No Feature Selection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKiLuMb6Fhei",
        "outputId": "0d60f0a5-fe99-464e-a525-94710b104b8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.93      0.91      7479\n",
            "           1       0.74      0.61      0.66      2290\n",
            "\n",
            "    accuracy                           0.86      9769\n",
            "   macro avg       0.81      0.77      0.79      9769\n",
            "weighted avg       0.85      0.86      0.85      9769\n",
            "\n",
            "Cross-validation scores:  [0.85323097 0.84926424 0.85527831 0.84873304 0.85026875]\n",
            "Mean cross-validation score:  0.8513550608264019\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "# Load the Adult Census Income dataset from OpenML\n",
        "adult = fetch_openml(data_id=1590, as_frame=True, parser='auto')\n",
        "df = adult.frame\n",
        "\n",
        "# Rename columns to lower case and replace hyphens with underscores\n",
        "df.columns = [col.lower().replace('-', '_') for col in df.columns]\n",
        "\n",
        "# Select features and target\n",
        "target = 'class'\n",
        "X = df.drop(columns=[target])\n",
        "y = df[target].apply(lambda x: 1 if x == '>50K' else 0)  # Convert target to binary\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Identify column types\n",
        "numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "# Define preprocessing for numeric columns\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())])\n",
        "\n",
        "# Define preprocessing for categorical columns\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "# Combine preprocessing steps\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)])\n",
        "\n",
        "# Create the full pipeline with a classifier\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(max_iter=1000))])\n",
        "\n",
        "# Train the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = pipeline.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Perform cross-validation to check for overfitting\n",
        "from sklearn.model_selection import cross_val_score\n",
        "cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5)\n",
        "print(\"Cross-validation scores: \", cv_scores)\n",
        "print(\"Mean cross-validation score: \", cv_scores.mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhiGEioqAqlq"
      },
      "source": [
        "### Feature Selction - Select K Best"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lGQjMAcJT3w",
        "outputId": "c819ab55-7779-4c0b-a0b7-1f9bc1e002bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.93      0.89      7479\n",
            "           1       0.68      0.49      0.57      2290\n",
            "\n",
            "    accuracy                           0.83      9769\n",
            "   macro avg       0.77      0.71      0.73      9769\n",
            "weighted avg       0.82      0.83      0.82      9769\n",
            "\n",
            "Cross-validation scores:  [0.82072937 0.81445937 0.81535509 0.81545943 0.81609931]\n",
            "Mean cross-validation score:  0.8164205133394938\n",
            "Selected features: ['education_num', 'marital_status_Married-civ-spouse', 'relationship_Husband']\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "# Number of features to select\n",
        "n = 3\n",
        "\n",
        "# Load the Adult Census Income dataset from OpenML\n",
        "adult = fetch_openml(data_id=1590, as_frame=True, parser='auto')\n",
        "df = adult.frame\n",
        "\n",
        "# Rename columns to lower case and replace hyphens with underscores\n",
        "df.columns = [col.lower().replace('-', '_') for col in df.columns]\n",
        "\n",
        "# Select features and target\n",
        "target = 'class'\n",
        "X = df.drop(columns=[target])\n",
        "y = df[target].apply(lambda x: 1 if x == '>50K' else 0)  # Convert target to binary\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Identify column types\n",
        "numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "# Define preprocessing for numeric columns\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())])\n",
        "\n",
        "# Define preprocessing for categorical columns\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "# Combine preprocessing steps\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)])\n",
        "\n",
        "# Create the full pipeline with feature selection and a classifier\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('feature_selection', SelectKBest(score_func=f_classif, k=n)),  # Adjust k as needed\n",
        "    ('classifier', LogisticRegression(max_iter=1000))])\n",
        "\n",
        "# Train the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = pipeline.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Perform cross-validation to check for overfitting\n",
        "from sklearn.model_selection import cross_val_score\n",
        "cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5)\n",
        "print(\"Cross-validation scores: \", cv_scores)\n",
        "print(\"Mean cross-validation score: \", cv_scores.mean())\n",
        "\n",
        "# Get the selected feature names\n",
        "selected_features_mask = pipeline.named_steps['feature_selection'].get_support()\n",
        "all_features = numeric_features + pipeline.named_steps['preprocessor'].named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_features).tolist()\n",
        "selected_features = [feature for feature, selected in zip(all_features, selected_features_mask) if selected]\n",
        "\n",
        "print(\"Selected features:\", selected_features)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRIjaIBLLpuE"
      },
      "source": [
        "## Grid Search Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cn3K7AOwKzRs",
        "outputId": "696633e2-a4ae-4cce-8355-54337ee1f5d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters found:  {'feature_selection': SelectKBest(k=15), 'feature_selection__k': 15}\n",
            "Best estimator found:  Pipeline(steps=[('preprocessor',\n",
            "                 ColumnTransformer(transformers=[('num',\n",
            "                                                  Pipeline(steps=[('imputer',\n",
            "                                                                   SimpleImputer(strategy='median')),\n",
            "                                                                  ('scaler',\n",
            "                                                                   StandardScaler())]),\n",
            "                                                  ['age', 'fnlwgt',\n",
            "                                                   'education_num',\n",
            "                                                   'capital_gain',\n",
            "                                                   'capital_loss',\n",
            "                                                   'hours_per_week']),\n",
            "                                                 ('cat',\n",
            "                                                  Pipeline(steps=[('imputer',\n",
            "                                                                   SimpleImputer(fill_value='missing',\n",
            "                                                                                 strategy='constant')),\n",
            "                                                                  ('onehot',\n",
            "                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
            "                                                  ['workclass', 'education',\n",
            "                                                   'marital_status',\n",
            "                                                   'occupation', 'relationship',\n",
            "                                                   'race', 'sex',\n",
            "                                                   'native_country'])])),\n",
            "                ('feature_selection', SelectKBest(k=15)),\n",
            "                ('classifier', LogisticRegression(max_iter=1000))])\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.93      0.90      7479\n",
            "           1       0.72      0.57      0.64      2290\n",
            "\n",
            "    accuracy                           0.85      9769\n",
            "   macro avg       0.80      0.75      0.77      9769\n",
            "weighted avg       0.84      0.85      0.84      9769\n",
            "\n",
            "Selected features:  ['age', 'education_num', 'capital_gain', 'hours_per_week', 'education_Bachelors', 'education_Masters', 'marital_status_Married-civ-spouse', 'marital_status_Never-married', 'occupation_Exec-managerial', 'occupation_Prof-specialty', 'relationship_Husband', 'relationship_Not-in-family', 'relationship_Own-child', 'sex_Female', 'sex_Male']\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "# Number of features to select\n",
        "n = 3\n",
        "\n",
        "# Load the Adult Census Income dataset from OpenML\n",
        "adult = fetch_openml(data_id=1590, as_frame=True, parser='auto')\n",
        "df = adult.frame\n",
        "\n",
        "# Rename columns to lower case and replace hyphens with underscores\n",
        "df.columns = [col.lower().replace('-', '_') for col in df.columns]\n",
        "\n",
        "# Select features and target\n",
        "target = 'class'\n",
        "X = df.drop(columns=[target])\n",
        "y = df[target].apply(lambda x: 1 if x == '>50K' else 0)  # Convert target to binary\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Identify column types\n",
        "numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "# Define preprocessing for numeric columns\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())])\n",
        "\n",
        "# Define preprocessing for categorical columns\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "# Combine preprocessing steps\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)])\n",
        "\n",
        "# Create base logistic regression model\n",
        "logreg = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Create the full pipeline with feature selection and a classifier\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('feature_selection', SelectKBest(score_func=f_classif, k=n)),  # Placeholder for feature selection\n",
        "    ('classifier', logreg)])\n",
        "\n",
        "# Define parameter grid for GridSearchCV\n",
        "param_grid = [\n",
        "    {\n",
        "        'feature_selection': [SelectKBest(score_func=f_classif)],\n",
        "        'feature_selection__k': [5, 10, 15]\n",
        "    },\n",
        "    {\n",
        "        'feature_selection': [RFE(estimator=logreg)],\n",
        "        'feature_selection__n_features_to_select': [5, 10, 15]\n",
        "    }\n",
        "]\n",
        "\n",
        "# Perform grid search\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and best estimator\n",
        "print(\"Best parameters found: \", grid_search.best_params_)\n",
        "print(\"Best estimator found: \", grid_search.best_estimator_)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = grid_search.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Get the selected feature names\n",
        "best_feature_selector = grid_search.best_estimator_.named_steps['feature_selection']\n",
        "if isinstance(best_feature_selector, SelectKBest):\n",
        "    selected_features_mask = best_feature_selector.get_support()\n",
        "    all_features = numeric_features + grid_search.best_estimator_.named_steps['preprocessor'].named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_features).tolist()\n",
        "    selected_features = [feature for feature, selected in zip(all_features, selected_features_mask) if selected]\n",
        "elif isinstance(best_feature_selector, RFE):\n",
        "    selected_features_mask = best_feature_selector.support_\n",
        "    all_features = numeric_features + grid_search.best_estimator_.named_steps['preprocessor'].named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_features).tolist()\n",
        "    selected_features = [feature for feature, selected in zip(all_features, selected_features_mask) if selected]\n",
        "\n",
        "print(\"Selected features: \", selected_features)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kTPYh5SOtW2"
      },
      "source": [
        "There are several feature selection methods available in scikit-learn, each with different strategies for selecting the most relevant features. Here are some common methods:\n",
        "\n",
        "1. **Univariate Feature Selection**:\n",
        "   - **SelectKBest**: Selects the top k features based on univariate statistical tests.\n",
        "   - **SelectPercentile**: Selects the top features based on a percentile of the highest scores.\n",
        "\n",
        "2. **Recursive Feature Elimination (RFE)**:\n",
        "   - **RFE**: Recursively removes features and builds a model on the remaining attributes. It uses the model's coefficients to rank the features.\n",
        "   - **RFECV**: RFE with cross-validation to find the optimal number of features.\n",
        "\n",
        "3. **Model-Based Feature Selection**:\n",
        "   - **SelectFromModel**: Selects features based on importance weights. Can be used with any estimator that exposes a `coef_` or `feature_importances_` attribute.\n",
        "   - **Lasso**: Uses L1 regularization to shrink some coefficients to zero, effectively performing feature selection.\n",
        "   - **Tree-based methods**: Tree-based estimators such as RandomForest, GradientBoosting, etc., can be used to evaluate the importance of features.\n",
        "\n",
        "4. **Principal Component Analysis (PCA)**:\n",
        "   - **PCA**: Transforms the features into a lower-dimensional space. It is often used for dimensionality reduction but can also serve as a feature selection method.\n",
        "\n",
        "Let's integrate a few more feature selection methods into the grid search for comparison:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kezt7_qL1GZ",
        "outputId": "867d95a2-e454-4fac-a429-3027d1bfa051"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters found:  {'feature_selection': SelectFromModel(estimator=RandomForestClassifier(), threshold='median'), 'feature_selection__threshold': 'median'}\n",
            "Best estimator found:  Pipeline(steps=[('preprocessor',\n",
            "                 ColumnTransformer(transformers=[('num',\n",
            "                                                  Pipeline(steps=[('imputer',\n",
            "                                                                   SimpleImputer(strategy='median')),\n",
            "                                                                  ('scaler',\n",
            "                                                                   StandardScaler())]),\n",
            "                                                  ['age', 'fnlwgt',\n",
            "                                                   'education_num',\n",
            "                                                   'capital_gain',\n",
            "                                                   'capital_loss',\n",
            "                                                   'hours_per_week']),\n",
            "                                                 ('cat',\n",
            "                                                  Pipeline(steps=[('imputer',\n",
            "                                                                   SimpleImputer(fill_value='missing',\n",
            "                                                                                 strategy='constant')),\n",
            "                                                                  ('onehot',\n",
            "                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
            "                                                  ['workclass', 'education',\n",
            "                                                   'marital_status',\n",
            "                                                   'occupation', 'relationship',\n",
            "                                                   'race', 'sex',\n",
            "                                                   'native_country'])])),\n",
            "                ('feature_selection',\n",
            "                 SelectFromModel(estimator=RandomForestClassifier(),\n",
            "                                 threshold='median')),\n",
            "                ('classifier', LogisticRegression(max_iter=1000))])\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.93      0.91      7479\n",
            "           1       0.74      0.60      0.66      2290\n",
            "\n",
            "    accuracy                           0.86      9769\n",
            "   macro avg       0.81      0.77      0.79      9769\n",
            "weighted avg       0.85      0.86      0.85      9769\n",
            "\n",
            "Selected features:  ['age', 'fnlwgt', 'education_num', 'capital_gain', 'capital_loss', 'hours_per_week', 'workclass_Federal-gov', 'workclass_Local-gov', 'workclass_Private', 'workclass_Self-emp-inc', 'workclass_Self-emp-not-inc', 'workclass_State-gov', 'workclass_missing', 'education_11th', 'education_7th-8th', 'education_Assoc-acdm', 'education_Assoc-voc', 'education_Bachelors', 'education_Doctorate', 'education_HS-grad', 'education_Masters', 'education_Prof-school', 'education_Some-college', 'marital_status_Divorced', 'marital_status_Married-civ-spouse', 'marital_status_Never-married', 'marital_status_Separated', 'marital_status_Widowed', 'occupation_Adm-clerical', 'occupation_Craft-repair', 'occupation_Exec-managerial', 'occupation_Farming-fishing', 'occupation_Handlers-cleaners', 'occupation_Machine-op-inspct', 'occupation_Other-service', 'occupation_Prof-specialty', 'occupation_Protective-serv', 'occupation_Sales', 'occupation_Tech-support', 'occupation_Transport-moving', 'occupation_missing', 'relationship_Husband', 'relationship_Not-in-family', 'relationship_Own-child', 'relationship_Unmarried', 'relationship_Wife', 'race_Asian-Pac-Islander', 'race_Black', 'race_White', 'sex_Female', 'sex_Male', 'native_country_Mexico', 'native_country_United-States', 'native_country_missing']\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.linear_model import LogisticRegression, LassoCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, RFE, SelectFromModel\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "# Load the Adult Census Income dataset from OpenML\n",
        "adult = fetch_openml(data_id=1590, as_frame=True)\n",
        "df = adult.frame\n",
        "\n",
        "# Rename columns to lower case and replace hyphens with underscores\n",
        "df.columns = [col.lower().replace('-', '_') for col in df.columns]\n",
        "\n",
        "# Select features and target\n",
        "target = 'class'\n",
        "X = df.drop(columns=[target])\n",
        "y = df[target].apply(lambda x: 1 if x == '>50K' else 0)  # Convert target to binary\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Identify column types\n",
        "numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "# Define preprocessing for numeric columns\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())])\n",
        "\n",
        "# Define preprocessing for categorical columns\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "# Combine preprocessing steps\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)])\n",
        "\n",
        "# Create base logistic regression model\n",
        "logreg = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Determine the number of features for selection\n",
        "total_features = len(numeric_features) + len(categorical_features)\n",
        "feature_selection_params = {\n",
        "    'select_kbest': [total_features // i for i in range(2, 6)],\n",
        "    'rfe': [total_features // i for i in range(2, 6)]\n",
        "}\n",
        "\n",
        "# Create the full pipeline with a placeholder for feature selection\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('feature_selection', SelectKBest(score_func=f_classif, k=total_features // 2)),  # Placeholder for feature selection\n",
        "    ('classifier', logreg)])\n",
        "\n",
        "# Define parameter grid for GridSearchCV\n",
        "param_grid = [\n",
        "    {\n",
        "        'feature_selection': [SelectKBest(score_func=f_classif)],\n",
        "        'feature_selection__k': feature_selection_params['select_kbest']\n",
        "    },\n",
        "    {\n",
        "        'feature_selection': [RFE(estimator=logreg)],\n",
        "        'feature_selection__n_features_to_select': feature_selection_params['rfe']\n",
        "    },\n",
        "    {\n",
        "        'feature_selection': [SelectFromModel(estimator=LogisticRegression(penalty=\"l1\", solver='liblinear'))],\n",
        "        'feature_selection__threshold': [\"mean\", \"median\"]\n",
        "    },\n",
        "    {\n",
        "        'feature_selection': [SelectFromModel(estimator=RandomForestClassifier(n_estimators=100))],\n",
        "        'feature_selection__threshold': [\"mean\", \"median\"]\n",
        "    }\n",
        "]\n",
        "\n",
        "# Perform grid search\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and best estimator\n",
        "print(\"Best parameters found: \", grid_search.best_params_)\n",
        "print(\"Best estimator found: \", grid_search.best_estimator_)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = grid_search.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Get the selected feature names\n",
        "best_feature_selector = grid_search.best_estimator_.named_steps['feature_selection']\n",
        "if isinstance(best_feature_selector, SelectKBest):\n",
        "    selected_features_mask = best_feature_selector.get_support()\n",
        "    all_features = numeric_features + grid_search.best_estimator_.named_steps['preprocessor'].named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_features).tolist()\n",
        "    selected_features = [feature for feature, selected in zip(all_features, selected_features_mask) if selected]\n",
        "elif isinstance(best_feature_selector, RFE):\n",
        "    selected_features_mask = best_feature_selector.support_\n",
        "    all_features = numeric_features + grid_search.best_estimator_.named_steps['preprocessor'].named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_features).tolist()\n",
        "    selected_features = [feature for feature, selected in zip(all_features, selected_features_mask) if selected]\n",
        "elif isinstance(best_feature_selector, SelectFromModel):\n",
        "    selected_features_mask = best_feature_selector.get_support()\n",
        "    all_features = numeric_features + grid_search.best_estimator_.named_steps['preprocessor'].named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_features).tolist()\n",
        "    selected_features = [feature for feature, selected in zip(all_features, selected_features_mask) if selected]\n",
        "\n",
        "print(\"Selected features: \", selected_features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rU-Y-n1O-A9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPPYMaITZIJDWBsvZzvIXqi"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}