{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOCKIQEC85PE0sKGevmZKuV"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Why Use Pipelines for Hyperparameter Tuning?\n",
        "\n",
        "Using pipelines for hyperparameter tuning offers several advantages:\n",
        "\n",
        "1. **Consistency and Reproducibility**:\n",
        "   - Pipelines ensure that the same preprocessing steps are consistently applied to all data splits during cross-validation, preventing data leakage and ensuring reproducibility.\n",
        "\n",
        "2. **Simplified Workflow**:\n",
        "   - Pipelines encapsulate the entire machine learning workflow (preprocessing, feature selection, modeling) into a single object. This simplifies the code and makes it easier to manage complex workflows.\n",
        "\n",
        "3. **Hyperparameter Optimization**:\n",
        "   - With pipelines, you can optimize hyperparameters for both preprocessing steps and the model simultaneously. For example, you can tune the parameters for scaling, encoding, and the classifier in one go.\n",
        "\n",
        "4. **Avoiding Data Leakage**:\n",
        "   - Pipelines prevent data leakage by ensuring that transformations (e.g., scaling, encoding) are applied to the training data only during cross-validation. The same transformations are then applied to the validation set, but they are not influenced by it.\n",
        "\n",
        "5. **Modularity and Flexibility**:\n",
        "   - Pipelines are modular, meaning you can easily swap out or modify steps (e.g., changing the model or adding new preprocessing steps) without altering the rest of the workflow.\n",
        "\n",
        "### Example Scenario: Without Pipelines\n",
        "\n",
        "When not using pipelines, you would have to manually apply preprocessing steps to your training and validation sets separately. This can lead to inconsistencies and data leakage. Additionally, you would have to manually manage and tune hyperparameters for each step, which is cumbersome and error-prone.\n",
        "\n",
        "### Example Scenario: With Pipelines\n",
        "\n",
        "With pipelines, you define your preprocessing and modeling steps once. During hyperparameter tuning, all steps are applied consistently, and you can easily optimize the hyperparameters for the entire pipeline.\n"
      ],
      "metadata": {
        "id": "a5VU2ntcNXAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "# Load the Adult Census Income dataset from OpenML\n",
        "adult = fetch_openml(data_id=1590, as_frame=True, parser='auto')\n",
        "df = adult.frame\n",
        "\n",
        "# Take a sample of the dataset to reduce run time\n",
        "df = df.sample(frac=0.3, random_state=42)\n",
        "\n",
        "# Rename columns to lower case and replace hyphens with underscores\n",
        "df.columns = [col.lower().replace('-', '_') for col in df.columns]\n",
        "\n",
        "# Select features and target\n",
        "target = 'class'\n",
        "X = df.drop(columns=[target])\n",
        "y = df[target].apply(lambda x: 1 if x == '>50K' else 0)  # Convert target to binary\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Identify column types\n",
        "numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "# Define preprocessing for numeric columns\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())])\n",
        "\n",
        "# Define preprocessing for categorical columns\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "# Combine preprocessing steps\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)])\n",
        "\n",
        "# Create the full pipeline with a classifier\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', RandomForestClassifier())])\n",
        "\n",
        "# Train the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = pipeline.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Perform cross-validation to check for overfitting\n",
        "cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5)\n",
        "print(\"Cross-validation scores: \", cv_scores)\n",
        "print(\"Mean cross-validation score: \", cv_scores.mean())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IF9oQJoHM8FU",
        "outputId": "fbbb543b-e6c5-41b6-a562-3ac39fc7e714"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.94      0.91      2236\n",
            "           1       0.76      0.61      0.67       695\n",
            "\n",
            "    accuracy                           0.86      2931\n",
            "   macro avg       0.82      0.77      0.79      2931\n",
            "weighted avg       0.86      0.86      0.86      2931\n",
            "\n",
            "Cross-validation scores:  [0.85884861 0.86268657 0.8587884  0.85622867 0.8587884 ]\n",
            "Mean cross-validation score:  0.8590681283975055\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Search\n",
        "\n",
        "**Description**:\n",
        "- Random search samples a fixed number of hyperparameter combinations from the specified distributions.\n",
        "\n",
        "**Strengths**:\n",
        "1. **Efficient**: Typically requires less computational time compared to grid search as it evaluates fewer combinations.\n",
        "2. **Good Performance**: Can find good hyperparameter combinations faster, especially when the hyperparameter space is large.\n",
        "3. **Flexible**: Can easily be extended to more iterations if needed.\n",
        "\n",
        "**Weaknesses**:\n",
        "1. **Stochastic Nature**: Results can vary between runs because it samples randomly.\n",
        "2. **Not Exhaustive**: May miss the best combination as it does not evaluate all possible combinations.\n",
        "\n",
        "**When to Choose**:\n",
        "- When the hyperparameter space is large or when computational resources are limited.\n",
        "- When you want a quick and reasonably good set of hyperparameters rather than the absolute best.\n"
      ],
      "metadata": {
        "id": "S8OtYpxpO2MH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.datasets import fetch_openml\n",
        "from scipy.stats import randint, uniform\n",
        "\n",
        "# Load the Adult Census Income dataset from OpenML\n",
        "adult = fetch_openml(data_id=1590, as_frame=True, parser='auto')\n",
        "df = adult.frame\n",
        "\n",
        "# Take a sample of the dataset to reduce run time\n",
        "df = df.sample(frac=0.3, random_state=42)\n",
        "\n",
        "# Rename columns to lower case and replace hyphens with underscores\n",
        "df.columns = [col.lower().replace('-', '_') for col in df.columns]\n",
        "\n",
        "# Select features and target\n",
        "target = 'class'\n",
        "X = df.drop(columns=[target])\n",
        "y = df[target].apply(lambda x: 1 if x == '>50K' else 0)  # Convert target to binary\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Identify column types\n",
        "numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "# Define preprocessing for numeric columns\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())])\n",
        "\n",
        "# Define preprocessing for categorical columns\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "# Combine preprocessing steps\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)])\n",
        "\n",
        "# Create the pipeline with RandomForestClassifier\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', RandomForestClassifier(random_state=42))])\n",
        "\n",
        "# Define the parameter distributions for RandomizedSearchCV\n",
        "param_distributions = {\n",
        "    'classifier__n_estimators': randint(50, 200),\n",
        "    'classifier__max_depth': [10, 20, None],\n",
        "    'classifier__min_samples_split': randint(2, 11),\n",
        "    'classifier__min_samples_leaf': randint(1, 5),\n",
        "    'classifier__bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "# Perform randomized search\n",
        "random_search = RandomizedSearchCV(pipeline, param_distributions, n_iter=50, cv=5, n_jobs=-1, verbose=2, random_state=42)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and best estimator\n",
        "print(\"Best parameters found: \", random_search.best_params_)\n",
        "print(\"Best estimator found: \", random_search.best_estimator_)\n",
        "\n",
        "# Predict and evaluate using the best estimator\n",
        "y_pred = random_search.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Perform cross-validation to check for overfitting\n",
        "cv_scores = cross_val_score(random_search.best_estimator_, X_train, y_train, cv=5)\n",
        "print(\"Cross-validation scores: \", cv_scores)\n",
        "print(\"Mean cross-validation score: \", cv_scores.mean())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIiWEopEO556",
        "outputId": "8026dec6-e338-4f1c-8eb6-6d5b9a9dfc49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
            "Best parameters found:  {'classifier__bootstrap': True, 'classifier__max_depth': None, 'classifier__min_samples_leaf': 2, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 178}\n",
            "Best estimator found:  Pipeline(steps=[('preprocessor',\n",
            "                 ColumnTransformer(transformers=[('num',\n",
            "                                                  Pipeline(steps=[('imputer',\n",
            "                                                                   SimpleImputer(strategy='median')),\n",
            "                                                                  ('scaler',\n",
            "                                                                   StandardScaler())]),\n",
            "                                                  ['age', 'fnlwgt',\n",
            "                                                   'education_num',\n",
            "                                                   'capital_gain',\n",
            "                                                   'capital_loss',\n",
            "                                                   'hours_per_week']),\n",
            "                                                 ('cat',\n",
            "                                                  Pipeline(steps=[('imputer',\n",
            "                                                                   SimpleImputer(fill_value='missing',\n",
            "                                                                                 strategy='constant')),\n",
            "                                                                  ('onehot',\n",
            "                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
            "                                                  ['workclass', 'education',\n",
            "                                                   'marital_status',\n",
            "                                                   'occupation', 'relationship',\n",
            "                                                   'race', 'sex',\n",
            "                                                   'native_country'])])),\n",
            "                ('classifier',\n",
            "                 RandomForestClassifier(min_samples_leaf=2, n_estimators=178,\n",
            "                                        random_state=42))])\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.95      0.92      2236\n",
            "           1       0.79      0.60      0.68       695\n",
            "\n",
            "    accuracy                           0.87      2931\n",
            "   macro avg       0.84      0.77      0.80      2931\n",
            "weighted avg       0.86      0.87      0.86      2931\n",
            "\n",
            "Cross-validation scores:  [0.86908316 0.87206823 0.87329352 0.86220137 0.86646758]\n",
            "Mean cross-validation score:  0.8686227686530777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grid Search\n",
        "\n",
        "**Description**:\n",
        "- Grid search is an exhaustive search method where all possible combinations of the specified hyperparameters are evaluated.\n",
        "\n",
        "**Strengths**:\n",
        "1. **Comprehensive**: Evaluates every possible combination of hyperparameters within the specified ranges, ensuring the best possible parameters within the grid are found.\n",
        "2. **Deterministic**: Given the same hyperparameters and dataset, it will always produce the same results.\n",
        "\n",
        "**Weaknesses**:\n",
        "1. **Computationally Expensive**: Can be very slow and resource-intensive, especially with a large number of hyperparameters or a wide range of values.\n",
        "2. **Inefficient**: Often evaluates many combinations that do not improve the model performance.\n",
        "\n",
        "**When to Choose**:\n",
        "- When the hyperparameter space is small and computational resources are not a constraint.\n",
        "- When you need to ensure that the absolute best combination within the specified range is found.\n",
        "\n"
      ],
      "metadata": {
        "id": "eTZva5NdPadP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "# Load the Adult Census Income dataset from OpenML\n",
        "adult = fetch_openml(data_id=1590, as_frame=True, parser='auto')\n",
        "df = adult.frame\n",
        "\n",
        "# Take a sample of the dataset\n",
        "df = df.sample(frac=0.1, random_state=42)  # Adjust frac to 0.1 (10%) for a smaller sample\n",
        "\n",
        "# Rename columns to lower case and replace hyphens with underscores\n",
        "df.columns = [col.lower().replace('-', '_') for col in df.columns]\n",
        "\n",
        "# Select features and target\n",
        "target = 'class'\n",
        "X = df.drop(columns=[target])\n",
        "y = df[target].apply(lambda x: 1 if x == '>50K' else 0)  # Convert target to binary\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Identify column types\n",
        "numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "# Define preprocessing for numeric columns\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())])\n",
        "\n",
        "# Define preprocessing for categorical columns\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "# Combine preprocessing steps\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)])\n",
        "\n",
        "# Create the pipeline with RandomForestClassifier\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', RandomForestClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "# Define the parameter grid for hyperparameter tuning\n",
        "param_grid = {\n",
        "    'classifier__n_estimators': [50, 100, 200],\n",
        "    'classifier__max_depth': [10, 20, None],\n",
        "    'classifier__min_samples_split': [2, 5, 10],\n",
        "    'classifier__min_samples_leaf': [1, 2, 4],\n",
        "    'classifier__bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "# Perform grid search\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1, verbose=2)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and best estimator\n",
        "print(\"Best parameters found: \", grid_search.best_params_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZkDVV3bN0lI",
        "outputId": "99a3d642-865d-4b87-de89-9fcd171d132a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n",
            "Best parameters found:  {'classifier__bootstrap': True, 'classifier__max_depth': None, 'classifier__min_samples_leaf': 2, 'classifier__min_samples_split': 10, 'classifier__n_estimators': 50}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Choosing Between Grid Search and Random Search\n",
        "\n",
        "**Grid Search**:\n",
        "- Use when you have a relatively small hyperparameter space and you want to ensure that the optimal parameters are found within that space.\n",
        "- Suitable for cases where computational resources and time are not major constraints.\n",
        "\n",
        "**Random Search**:\n",
        "- Use when the hyperparameter space is large, making grid search computationally impractical.\n",
        "- Suitable for cases where you need a quicker solution and can afford to explore a large hyperparameter space without exhaustive evaluation.\n",
        "- Often used as a first step to identify promising regions of the hyperparameter space, which can then be fine-tuned using more focused searches.\n",
        "\n",
        "### Practical Example\n",
        "\n",
        "In practice, you might start with random search to identify a promising set of hyperparameters and then use grid search around that region for fine-tuning. This combined approach can provide a good balance between efficiency and thoroughness.\n",
        "\n",
        "### Summary\n",
        "\n",
        "- **Grid Search**: Comprehensive but computationally expensive; best for small hyperparameter spaces.\n",
        "- **Random Search**: More efficient and suitable for large hyperparameter spaces; good for quick results and exploring large spaces."
      ],
      "metadata": {
        "id": "AObkA_krP4Li"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Model with Best Params"
      ],
      "metadata": {
        "id": "Z6-mRzcWbVHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the best parameters\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "# Create a new pipeline with the best parameters\n",
        "final_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', RandomForestClassifier(\n",
        "        random_state=42,\n",
        "        n_estimators=best_params['classifier__n_estimators'],\n",
        "        max_depth=best_params['classifier__max_depth'],\n",
        "        min_samples_split=best_params['classifier__min_samples_split'],\n",
        "        min_samples_leaf=best_params['classifier__min_samples_leaf'],\n",
        "        bootstrap=best_params['classifier__bootstrap']\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Train the final model on the entire training dataset\n",
        "final_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate using the final model\n",
        "y_pred_final = final_pipeline.predict(X_test)\n",
        "print(\"Final Model Performance\")\n",
        "print(classification_report(y_test, y_pred_final))\n",
        "\n",
        "# Optionally, you can save the final model\n",
        "import joblib\n",
        "joblib.dump(final_pipeline, '/content/sample_data/final_model.pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eMmHQMibYrK",
        "outputId": "64920f35-c2bf-43fc-ae72-6e26e847f03c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Model Performance\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.96      0.92       747\n",
            "           1       0.81      0.60      0.69       230\n",
            "\n",
            "    accuracy                           0.87       977\n",
            "   macro avg       0.85      0.78      0.81       977\n",
            "weighted avg       0.87      0.87      0.87       977\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/sample_data/final_model.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Saving the Pipeline:\n",
        "1. **`joblib.dump`**:\n",
        "   - `joblib.dump` is used to serialize the pipeline object and save it to a file.\n",
        "   - The method takes two arguments:\n",
        "     - The first argument is the object to be saved (in this case, `final_pipeline`).\n",
        "     - The second argument is the file path where the object should be saved (`'final_model.pkl'`).\n",
        "\n",
        "#### Loading the Pipeline:\n",
        "1. **`joblib.load`**:\n",
        "   - To load the saved pipeline, you use `joblib.load`.\n",
        "   - This method reads the serialized object from the file and deserializes it back into a usable Python object.\n",
        "\n",
        "\n",
        "### Explanation:\n",
        "\n",
        "1. **Saving the Pipeline**:\n",
        "   - `joblib.dump(final_pipeline, 'final_model.pkl')`: Saves the entire pipeline (including preprocessing steps and the trained model) to the file `final_model.pkl`.\n",
        "\n",
        "2. **Loading the Pipeline**:\n",
        "   - `loaded_pipeline = joblib.load('final_model.pkl')`: Loads the saved pipeline from the file `final_model.pkl`.\n",
        "   - The loaded pipeline can be used exactly like the original pipeline, allowing you to make predictions or further train the model.\n",
        "\n",
        "### Benefits of Saving the Pipeline:\n",
        "\n",
        "1. **Consistency**:\n",
        "   - Ensures that the exact same preprocessing steps and model configuration are used when the model is deployed or reused, maintaining consistency across different stages of the machine learning workflow.\n",
        "\n",
        "2. **Reusability**:\n",
        "   - Allows you to reuse the trained model and preprocessing steps without retraining, saving time and computational resources.\n",
        "\n",
        "3. **Portability**:\n",
        "   - Makes it easy to share the trained model and preprocessing steps with others, enabling collaboration and reproducibility.\n",
        "\n",
        "4. **Deployment**:\n",
        "   - Simplifies the deployment process by providing a single object that includes all necessary steps for making predictions on new data.\n",
        "\n",
        "By saving the entire pipeline, you encapsulate the entire data preprocessing and modeling workflow, ensuring that it can be reliably and efficiently reused in the future."
      ],
      "metadata": {
        "id": "nTgQ3y-waHVu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Loading the Pipeline and Making Predictions"
      ],
      "metadata": {
        "id": "VVPxzu-kcagL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the saved pipeline\n",
        "loaded_pipeline = joblib.load('/content/sample_data/final_model.pkl')\n",
        "\n",
        "# Use the loaded pipeline to make predictions on the test set\n",
        "y_pred_loaded = loaded_pipeline.predict(X_test)\n",
        "\n",
        "# Evaluate the loaded model\n",
        "print(\"Loaded Model Performance\")\n",
        "print(classification_report(y_test, y_pred_loaded))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KvnMQCGaNjk",
        "outputId": "7355d9c6-44df-4c22-ec3e-95ebddb3c0ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded Model Performance\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.96      0.92       747\n",
            "           1       0.81      0.60      0.69       230\n",
            "\n",
            "    accuracy                           0.87       977\n",
            "   macro avg       0.85      0.78      0.81       977\n",
            "weighted avg       0.87      0.87      0.87       977\n",
            "\n"
          ]
        }
      ]
    }
  ]
}