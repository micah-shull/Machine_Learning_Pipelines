{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOgHVrrrjO+5i7HWkLn3lLE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/pipelines/blob/main/pipelines_04_pytorch_sklearn_pipeline_wrapper_1_explained.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 1: Load and Preprocess the Dataset"
      ],
      "metadata": {
        "id": "sIJa-I-0O9Qa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "# Load the dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\"\n",
        "df = pd.read_excel(url, header=1)\n",
        "\n",
        "# Rename columns to lower case and replace spaces with underscores\n",
        "df.columns = [col.lower().replace(' ', '_') for col in df.columns]\n",
        "\n",
        "# Select features and target\n",
        "target = 'default_payment_next_month'\n",
        "X = df.drop(columns=[target])\n",
        "y = df[target]\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Identify column types\n",
        "numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "# Define preprocessing for numeric columns\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())])\n",
        "\n",
        "# Define preprocessing for categorical columns\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "# Combine preprocessing steps\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)])\n",
        "\n",
        "# Create the preprocessing pipeline\n",
        "preprocessing_pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
        "\n",
        "# Fit and transform the data\n",
        "X_train_processed = preprocessing_pipeline.fit_transform(X_train)\n",
        "X_test_processed = preprocessing_pipeline.transform(X_test)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "import torch\n",
        "X_train_tensor = torch.tensor(X_train_processed, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)\n",
        "X_test_tensor = torch.tensor(X_test_processed, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1)"
      ],
      "metadata": {
        "id": "jejoJGDmPBij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 2: Define a Simple PyTorch Neural Network Model"
      ],
      "metadata": {
        "id": "X8_LCSTMOx9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.sigmoid(self.fc1(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "0gYZJqjEPJlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 3: Define the sklearn Wrapper\n",
        "\n"
      ],
      "metadata": {
        "id": "4BTrsBTyOsPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "import torch.optim as optim\n",
        "\n",
        "class SklearnNN(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, input_dim, learning_rate=0.001):\n",
        "        self.input_dim = input_dim\n",
        "        self.learning_rate = learning_rate\n",
        "        self.model = SimpleNN(self.input_dim)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        criterion = nn.BCELoss()\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "        train_dataset = torch.utils.data.TensorDataset(torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32).unsqueeze(1))\n",
        "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "        for epoch in range(50):\n",
        "            self.model.train()\n",
        "            for inputs, targets in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                outputs = self.model(inputs)\n",
        "                loss = criterion(outputs, targets.view(-1, 1))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(torch.tensor(X, dtype=torch.float32))\n",
        "            predictions = (outputs > 0.5).float()\n",
        "        return predictions.numpy().squeeze()"
      ],
      "metadata": {
        "id": "TIh_nWPVPOMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 4: Train and Evaluate the Model\n",
        "\n"
      ],
      "metadata": {
        "id": "U2T1wujrOocw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of SklearnNN\n",
        "input_dim = X_train_tensor.shape[1]\n",
        "nn_estimator = SklearnNN(input_dim=input_dim)\n",
        "\n",
        "# Fit the model\n",
        "nn_estimator.fit(X_train_tensor.numpy(), y_train_tensor.numpy())\n",
        "\n",
        "# Predict on the test set\n",
        "test_predictions = nn_estimator.predict(X_test_tensor.numpy())\n",
        "\n",
        "# Evaluate the model\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test_tensor.numpy(), test_predictions))"
      ],
      "metadata": {
        "id": "T9JBFIDyPR-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary\n",
        "\n",
        "- **Step 1**: Load and preprocess the dataset.\n",
        "- **Step 2**: Define a simple PyTorch neural network model.\n",
        "- **Step 3**: Create a sklearn wrapper for the PyTorch model.\n",
        "- **Step 4**: Train and evaluate the model using the sklearn wrapper.\n",
        "\n",
        "This provides a simple yet complete example of integrating a PyTorch neural network with scikit-learn for training and evaluation."
      ],
      "metadata": {
        "id": "sQhkgIlrOlpU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Step-by-Step Process\n",
        "\n",
        "#### 1. Simple PyTorch Neural Network Model\n",
        "\n",
        "We'll start with the simplest possible neural network model and then enhance it incrementally.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6VXTsGucQ5pG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.sigmoid(self.fc1(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "WpBIWPezRwCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Define the sklearn Wrapper\n",
        "\n",
        "We'll create the sklearn wrapper and start by adding a basic `fit` method.\n",
        "\n"
      ],
      "metadata": {
        "id": "e6CNGbq7Rs_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "import torch.optim as optim\n",
        "\n",
        "class SklearnNN(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, input_dim, learning_rate=0.001):\n",
        "        self.input_dim = input_dim\n",
        "        self.learning_rate = learning_rate\n",
        "        self.model = SimpleNN(self.input_dim)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        criterion = nn.BCELoss()\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "        train_dataset = torch.utils.data.TensorDataset(torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32).unsqueeze(1))\n",
        "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "        for epoch in range(50):\n",
        "            self.model.train()\n",
        "            for inputs, targets in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                outputs = self.model(inputs)\n",
        "                loss = criterion(outputs, targets.view(-1, 1))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(torch.tensor(X, dtype=torch.float32))\n",
        "            predictions = (outputs > 0.5).float()\n",
        "        return predictions.numpy().squeeze()"
      ],
      "metadata": {
        "id": "OCoz2EqaRi6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example Workflow\n",
        "\n",
        "Let's use the same dataset and preprocess it, then train and evaluate our simple model.\n",
        "\n",
        "#### Load and Preprocess the Dataset"
      ],
      "metadata": {
        "id": "T62AoLg7RjiW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "# Load the dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\"\n",
        "df = pd.read_excel(url, header=1)\n",
        "\n",
        "# Rename columns to lower case and replace spaces with underscores\n",
        "df.columns = [col.lower().replace(' ', '_') for col in df.columns]\n",
        "\n",
        "# Select features and target\n",
        "target = 'default_payment_next_month'\n",
        "X = df.drop(columns=[target])\n",
        "y = df[target]\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Identify column types\n",
        "numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "# Define preprocessing for numeric columns\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())])\n",
        "\n",
        "# Define preprocessing for categorical columns\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "# Combine preprocessing steps\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)])\n",
        "\n",
        "# Create the preprocessing pipeline\n",
        "preprocessing_pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
        "\n",
        "# Fit and transform the data\n",
        "X_train_processed = preprocessing_pipeline.fit_transform(X_train)\n",
        "X_test_processed = preprocessing_pipeline.transform(X_test)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train_processed, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)\n",
        "X_test_tensor = torch.tensor(X_test_processed, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1)"
      ],
      "metadata": {
        "id": "Ha2xcHscRd-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train and Evaluate the Model"
      ],
      "metadata": {
        "id": "Re0kwFALRXUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of SklearnNN\n",
        "input_dim = X_train_tensor.shape[1]\n",
        "nn_estimator = SklearnNN(input_dim=input_dim)\n",
        "\n",
        "# Fit the model\n",
        "nn_estimator.fit(X_train_tensor.numpy(), y_train_tensor.numpy())\n",
        "\n",
        "# Predict on the test set\n",
        "test_predictions = nn_estimator.predict(X_test_tensor.numpy())\n",
        "\n",
        "# Evaluate the model\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test_tensor.numpy(), test_predictions))"
      ],
      "metadata": {
        "id": "WF03TMBcRVTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Incrementally Adding Features\n",
        "\n",
        "Let's enhance our model by adding more layers, dropout, and other hyperparameters."
      ],
      "metadata": {
        "id": "7LEavl8qRR_b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Enhanced PyTorch Neural Network Model"
      ],
      "metadata": {
        "id": "oLwMb5cyRPBU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EnhancedNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden1_dim=64, hidden2_dim=32, dropout_rate=0.5):\n",
        "        super(EnhancedNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden1_dim)\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "        self.fc2 = nn.Linear(hidden1_dim, hidden2_dim)\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "        self.fc3 = nn.Linear(hidden2_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = torch.sigmoid(self.fc3(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "SK2kSveDRL_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Enhanced sklearn Wrapper"
      ],
      "metadata": {
        "id": "pzaNax_BRJEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SklearnEnhancedNN(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, input_dim, hidden1_dim=64, hidden2_dim=32, dropout_rate=0.5, learning_rate=0.001):\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden1_dim = hidden1_dim\n",
        "        self.hidden2_dim = hidden2_dim\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.learning_rate = learning_rate\n",
        "        self.model = EnhancedNN(self.input_dim, self.hidden1_dim, self.hidden2_dim, self.dropout_rate)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        criterion = nn.BCELoss()\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "        train_dataset = torch.utils.data.TensorDataset(torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32).unsqueeze(1))\n",
        "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "        for epoch in range(50):\n",
        "            self.model.train()\n",
        "            for inputs, targets in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                outputs = self.model(inputs)\n",
        "                loss = criterion(outputs, targets.view(-1, 1))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(torch.tensor(X, dtype=torch.float32))\n",
        "            predictions = (outputs > 0.5).float()\n",
        "        return predictions.numpy().squeeze()"
      ],
      "metadata": {
        "id": "v0ZXY_CjRGx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train and Evaluate the Enhanced Model"
      ],
      "metadata": {
        "id": "7PAUfab1Q-3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of SklearnEnhancedNN\n",
        "input_dim = X_train_tensor.shape[1]\n",
        "nn_estimator = SklearnEnhancedNN(input_dim=input_dim)\n",
        "\n",
        "# Fit the model\n",
        "nn_estimator.fit(X_train_tensor.numpy(), y_train_tensor.numpy())\n",
        "\n",
        "# Predict on the test set\n",
        "test_predictions = nn_estimator.predict(X_test_tensor.numpy())\n",
        "\n",
        "# Evaluate the model\n",
        "print(classification_report(y_test_tensor.numpy(), test_predictions))"
      ],
      "metadata": {
        "id": "JgDYmOuZRCDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Summary\n",
        "\n",
        "1. **Define a Simple PyTorch Neural Network**: Start with a simple neural network model.\n",
        "2. **Create a sklearn Wrapper**: Define a wrapper that initializes the neural network model and adds additional parameters like learning rate.\n",
        "3. **Incrementally Enhance the Model**: Add more layers, dropout, and other hyperparameters to the PyTorch model and update the wrapper accordingly.\n",
        "4. **Train and Evaluate the Model**: Use the sklearn wrapper to fit the model, make predictions, and evaluate its performance.\n",
        "\n",
        "This approach allows you to leverage the strengths of both scikit-learn and PyTorch, creating a powerful and flexible workflow for building and tuning neural network models."
      ],
      "metadata": {
        "id": "bYqQh2oZQ78_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code inside the `SklearnNN` wrapper looks like PyTorch code because it essentially is PyTorch code. The purpose of the wrapper is to integrate the PyTorch neural network model into the scikit-learn framework, allowing you to use scikit-learn's tools for model training, evaluation, and hyperparameter tuning.\n",
        "\n",
        "#### Why Use PyTorch Code Inside the Wrapper?\n",
        "\n",
        "1. **Model Training**:\n",
        "   - The `fit` method in the wrapper uses PyTorch's training loop, including forward passes, loss calculation, backpropagation, and optimizer steps.\n",
        "   - This is necessary because the model itself is a PyTorch model, and the training process involves operations specific to PyTorch.\n",
        "\n",
        "2. **Prediction**:\n",
        "   - The `predict` method also uses PyTorch for making predictions. It puts the model in evaluation mode and performs forward passes to generate predictions.\n",
        "\n",
        "3. **Compatibility**:\n",
        "   - By writing the training and prediction code in PyTorch within the scikit-learn wrapper, you ensure that the neural network model is compatible with the PyTorch framework while also being usable within the scikit-learn framework.\n",
        "\n",
        "### Detailed Breakdown\n",
        "\n",
        "Let's break down the key parts of the wrapper:\n",
        "\n",
        "#### Initialization\n",
        "\n",
        "```python\n",
        "class SklearnNN(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, input_dim, learning_rate=0.001):\n",
        "        self.input_dim = input_dim\n",
        "        self.learning_rate = learning_rate\n",
        "        self.model = SimpleNN(self.input_dim)\n",
        "```\n",
        "\n",
        "- **Initialization (`__init__`)**:\n",
        "  - Parameters like `input_dim` and `learning_rate` are defined and stored.\n",
        "  - An instance of the `SimpleNN` model is created using the provided `input_dim`.\n",
        "\n",
        "#### Training the Model\n",
        "\n",
        "```python\n",
        "    def fit(self, X, y):\n",
        "        criterion = nn.BCELoss()\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "        train_dataset = torch.utils.data.TensorDataset(torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32).unsqueeze(1))\n",
        "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "        for epoch in range(50):\n",
        "            self.model.train()\n",
        "            for inputs, targets in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                outputs = self.model(inputs)\n",
        "                loss = criterion(outputs, targets.view(-1, 1))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        return self\n",
        "```\n",
        "\n",
        "- **Loss Function (`criterion`)**:\n",
        "  - Uses binary cross-entropy loss, which is common for binary classification tasks.\n",
        "\n",
        "- **Optimizer**:\n",
        "  - Uses the Adam optimizer to update model parameters.\n",
        "\n",
        "- **DataLoader**:\n",
        "  - Converts the input data to PyTorch tensors and creates a DataLoader to handle mini-batches.\n",
        "\n",
        "- **Training Loop**:\n",
        "  - For each epoch, the model is set to training mode.\n",
        "  - Iterates over mini-batches, performing forward passes, calculating loss, backpropagating gradients, and updating parameters.\n",
        "\n",
        "#### Making Predictions\n",
        "\n",
        "```python\n",
        "    def predict(self, X):\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(torch.tensor(X, dtype=torch.float32))\n",
        "            predictions = (outputs > 0.5).float()\n",
        "        return predictions.numpy().squeeze()\n",
        "```\n",
        "\n",
        "- **Evaluation Mode**:\n",
        "  - The model is set to evaluation mode to disable dropout and other training-specific layers.\n",
        "\n",
        "- **Forward Pass**:\n",
        "  - Performs a forward pass through the model to generate predictions.\n",
        "  - Converts the output to binary predictions (0 or 1).\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "The `SklearnNN` wrapper allows you to integrate a PyTorch neural network model into the scikit-learn framework. Inside the wrapper, you use PyTorch code to handle model training and prediction because the model itself is a PyTorch model. This approach leverages the strengths of both frameworks: the flexibility and power of PyTorch for neural network modeling and the ease of use and integration capabilities of scikit-learn for preprocessing, model selection, and evaluation."
      ],
      "metadata": {
        "id": "1wL948AlUCUU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you were to use a TensorFlow neural network model, you would need to use TensorFlow code within the wrapper to handle the training and prediction processes. The idea is to wrap the TensorFlow model in a scikit-learn compatible interface so that you can leverage scikit-learn's utilities while using TensorFlow for the neural network operations.\n",
        "\n",
        "#### TensorFlow Neural Network with scikit-learn Wrapper\n",
        "\n",
        "Let's go through an example of how to integrate a TensorFlow neural network with scikit-learn.\n",
        "\n",
        "#### Step 1: Define a Simple TensorFlow Neural Network\n",
        "\n",
        "We'll start with a simple neural network model using TensorFlow and Keras.\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "def create_simple_nn(input_dim):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(1, input_dim=input_dim, activation='sigmoid'))\n",
        "    return model\n",
        "```\n",
        "\n",
        "#### Step 2: Define the sklearn Wrapper\n",
        "\n",
        "Next, we create the sklearn wrapper for the TensorFlow model. This involves defining a class that inherits from `BaseEstimator` and `ClassifierMixin`, similar to the PyTorch example.\n",
        "\n",
        "```python\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "\n",
        "class SklearnTFNN(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, input_dim, learning_rate=0.001, epochs=50, batch_size=64):\n",
        "        self.input_dim = input_dim\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.model = create_simple_nn(self.input_dim)\n",
        "        self.model.compile(optimizer=tf.keras.optimizers.Adam(lr=self.learning_rate),\n",
        "                           loss='binary_crossentropy',\n",
        "                           metrics=['accuracy'])\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.model.fit(X, y, epochs=self.epochs, batch_size=self.batch_size, verbose=0)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = self.model.predict(X)\n",
        "        return (predictions > 0.5).astype(\"int32\")\n",
        "```\n",
        "\n",
        "### Example Workflow\n",
        "\n",
        "Let's use the same dataset and preprocess it, then train and evaluate our TensorFlow model using the sklearn wrapper.\n",
        "\n",
        "#### Load and Preprocess the Dataset\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "# Load the dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\"\n",
        "df = pd.read_excel(url, header=1)\n",
        "\n",
        "# Rename columns to lower case and replace spaces with underscores\n",
        "df.columns = [col.lower().replace(' ', '_') for col in df.columns]\n",
        "\n",
        "# Select features and target\n",
        "target = 'default_payment_next_month'\n",
        "X = df.drop(columns=[target])\n",
        "y = df[target]\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Identify column types\n",
        "numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "# Define preprocessing for numeric columns\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())])\n",
        "\n",
        "# Define preprocessing for categorical columns\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "# Combine preprocessing steps\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)])\n",
        "\n",
        "# Create the preprocessing pipeline\n",
        "preprocessing_pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
        "\n",
        "# Fit and transform the data\n",
        "X_train_processed = preprocessing_pipeline.fit_transform(X_train)\n",
        "X_test_processed = preprocessing_pipeline.transform(X_test)\n",
        "\n",
        "# Convert to NumPy arrays\n",
        "X_train_array = X_train_processed.toarray() if hasattr(X_train_processed, 'toarray') else X_train_processed\n",
        "X_test_array = X_test_processed.toarray() if hasattr(X_test_processed, 'toarray') else X_test_processed\n",
        "```\n",
        "\n",
        "### Train and Evaluate the TensorFlow Model\n",
        "\n",
        "```python\n",
        "# Create an instance of SklearnTFNN\n",
        "input_dim = X_train_array.shape[1]\n",
        "nn_estimator = SklearnTFNN(input_dim=input_dim)\n",
        "\n",
        "# Fit the model\n",
        "nn_estimator.fit(X_train_array, y_train.values)\n",
        "\n",
        "# Predict on the test set\n",
        "test_predictions = nn_estimator.predict(X_test_array)\n",
        "\n",
        "# Evaluate the model\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, test_predictions))\n",
        "```\n",
        "\n",
        "### Explanation\n",
        "\n",
        "1. **TensorFlow Neural Network**:\n",
        "   - The `create_simple_nn` function defines a simple neural network with a single layer.\n",
        "   - The model is compiled with an Adam optimizer, binary cross-entropy loss, and accuracy metric.\n",
        "\n",
        "2. **sklearn Wrapper**:\n",
        "   - The `SklearnTFNN` class initializes the TensorFlow model with the specified parameters.\n",
        "   - The `fit` method trains the TensorFlow model on the provided data.\n",
        "   - The `predict` method generates predictions using the trained TensorFlow model.\n",
        "\n",
        "3. **Preprocessing**:\n",
        "   - Data preprocessing is handled using scikit-learn pipelines, ensuring consistency between training and testing datasets.\n",
        "\n",
        "4. **Training and Evaluation**:\n",
        "   - The model is trained using the `fit` method and evaluated using the `predict` method.\n",
        "   - The classification report provides an evaluation of the model's performance.\n",
        "\n",
        "By using TensorFlow code within the wrapper, you ensure that the TensorFlow neural network model is properly trained and evaluated within the scikit-learn framework. This approach allows you to combine the strengths of both TensorFlow and scikit-learn in a seamless and efficient manner."
      ],
      "metadata": {
        "id": "nP8PoHdWUu5u"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "czo7YDJROmLQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}