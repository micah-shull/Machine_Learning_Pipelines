{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPcx0PIfCk2RZKPZoOEft1i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/pipelines/blob/main/pipelines_04_pytorch_sklearn_pipeline_wrapper_2_adding_complexity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Slowly adding complexity to a model to improve performance is a well-established approach in machine learning and deep learning. This iterative and incremental approach is beneficial for several reasons:\n",
        "\n",
        "## Reasons for Incrementally Adding Complexity\n",
        "\n",
        "1. **Understanding the Model**:\n",
        "   - **Starting Simple**: Beginning with a simple model allows you to establish a baseline performance and understand the basic behavior of the model.\n",
        "   - **Diagnosing Issues**: Simple models are easier to debug. If there are any issues, they are more straightforward to identify and fix.\n",
        "\n",
        "2. **Preventing Overfitting**:\n",
        "   - **Controlled Complexity**: Gradually increasing complexity helps in controlling overfitting. You can monitor how the model's performance on the training and validation data changes as you add more layers, neurons, or other complexities.\n",
        "   - **Regularization**: Adding complexity slowly allows you to implement and fine-tune regularization techniques such as dropout, L2 regularization, or early stopping.\n",
        "\n",
        "3. **Efficient Resource Use**:\n",
        "   - **Resource Management**: Simple models require less computational power and memory, making them faster to train and easier to iterate upon. This is especially important in environments with limited resources.\n",
        "   - **Scalability**: You can incrementally scale up your model as needed, optimizing resource usage and training time.\n",
        "\n",
        "4. **Improved Model Performance**:\n",
        "   - **Layer-Wise Optimization**: By incrementally adding layers or neurons, you can optimize each part of the model. This helps in identifying the most effective architecture.\n",
        "   - **Hyperparameter Tuning**: Incremental complexity allows for systematic hyperparameter tuning, ensuring each added complexity contributes positively to the model's performance.\n",
        "\n",
        "5. **Building Intuition**:\n",
        "   - **Learning Process**: This approach helps build intuition about how different architectural changes impact model performance. It’s a valuable learning process for understanding deep learning principles.\n",
        "   - **Domain Knowledge**: Incorporating domain knowledge gradually into the model architecture can lead to better and more interpretable models.\n",
        "\n",
        "### Practical Steps for Incremental Complexity\n",
        "\n",
        "1. **Start with a Simple Model**:\n",
        "   - Begin with a straightforward model, such as a single-layer neural network.\n",
        "   - Establish a baseline performance metric.\n",
        "\n",
        "2. **Monitor Performance Metrics**:\n",
        "   - Evaluate the model using relevant performance metrics (accuracy, F1-score, etc.).\n",
        "   - Use cross-validation to ensure the model generalizes well.\n",
        "\n",
        "3. **Gradually Increase Complexity**:\n",
        "   - Add more layers or neurons.\n",
        "   - Introduce dropout layers for regularization.\n",
        "   - Experiment with different activation functions.\n",
        "\n",
        "4. **Tune Hyperparameters**:\n",
        "   - Adjust learning rates, batch sizes, and the number of epochs.\n",
        "   - Use techniques like grid search or random search for systematic hyperparameter tuning.\n",
        "\n",
        "5. **Evaluate and Iterate**:\n",
        "   - After each change, re-evaluate the model’s performance.\n",
        "   - Compare against the baseline and previous iterations to ensure improvements.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZeDM5402fynT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Step 1: Load and Preprocess the Dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "fLTqeTnsX24o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "# Load the dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\"\n",
        "df = pd.read_excel(url, header=1)\n",
        "\n",
        "# Rename columns to lower case and replace spaces with underscores\n",
        "df.columns = [col.lower().replace(' ', '_') for col in df.columns]\n",
        "\n",
        "# Select features and target\n",
        "target = 'default_payment_next_month'\n",
        "X = df.drop(columns=[target]+['id'])\n",
        "y = df[target]\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Identify column types\n",
        "numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "# Define preprocessing for numeric columns\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())])\n",
        "\n",
        "# Define preprocessing for categorical columns\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "# Combine preprocessing steps\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)])\n",
        "\n",
        "# Create the preprocessing pipeline\n",
        "preprocessing_pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
        "\n",
        "# Fit and transform the data\n",
        "X_train_processed = preprocessing_pipeline.fit_transform(X_train)\n",
        "X_test_processed = preprocessing_pipeline.transform(X_test)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "import torch\n",
        "X_train_tensor = torch.tensor(X_train_processed, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)\n",
        "X_test_tensor = torch.tensor(X_test_processed, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1)"
      ],
      "metadata": {
        "id": "gNhf5xnta2Xa"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Define a Simple PyTorch Neural Network Model\n",
        "\n",
        "Define the simplest possible neural network model."
      ],
      "metadata": {
        "id": "iS04-wSBaxcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.sigmoid(self.fc1(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "IRMRGnzEauYE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Define the sklearn Wrapper\n",
        "\n",
        "Create the sklearn wrapper for the simple PyTorch model."
      ],
      "metadata": {
        "id": "-TkE7yINaq5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "import torch.optim as optim\n",
        "\n",
        "class SklearnNN(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, input_dim, learning_rate=0.001, epochs=50, batch_size=64):\n",
        "        self.input_dim = input_dim\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.model = SimpleNN(self.input_dim)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        criterion = nn.BCELoss()\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "        train_dataset = torch.utils.data.TensorDataset(torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32).unsqueeze(1))\n",
        "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            self.model.train()\n",
        "            for inputs, targets in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                outputs = self.model(inputs)\n",
        "                loss = criterion(outputs, targets.view(-1, 1))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(torch.tensor(X, dtype=torch.float32))\n",
        "            predictions = (outputs > 0.5).float()\n",
        "        return predictions.numpy().squeeze()"
      ],
      "metadata": {
        "id": "mHQ-AoMjaox4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Train and Evaluate the Simple Model\n",
        "\n",
        "Train and evaluate the simple PyTorch neural network model.\n"
      ],
      "metadata": {
        "id": "UDsxquZOakTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of SklearnNN\n",
        "input_dim = X_train_tensor.shape[1]\n",
        "nn_estimator = SklearnNN(input_dim=input_dim)\n",
        "\n",
        "# Fit the model\n",
        "nn_estimator.fit(X_train_tensor.numpy(), y_train_tensor.numpy())\n",
        "\n",
        "# Predict on the test set\n",
        "test_predictions = nn_estimator.predict(X_test_tensor.numpy())\n",
        "\n",
        "# Evaluate the model\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test_tensor.numpy(), test_predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEBSMdJzahcu",
        "outputId": "cdf91107-19e1-41ed-b494-7903e0f8c66c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.82      0.97      0.89      4687\n",
            "         1.0       0.69      0.24      0.35      1313\n",
            "\n",
            "    accuracy                           0.81      6000\n",
            "   macro avg       0.76      0.60      0.62      6000\n",
            "weighted avg       0.79      0.81      0.77      6000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding Layers of Complexity\n",
        "\n",
        "Let's gradually add more layers and complexity to the model and evaluate the performance.\n",
        "\n",
        "#### Enhanced PyTorch Neural Network Model\n",
        "\n",
        "Add more layers and dropout.\n"
      ],
      "metadata": {
        "id": "mnBe8rwQaXgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EnhancedNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden1_dim=64, hidden2_dim=32, dropout_rate=0.5):\n",
        "        super(EnhancedNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden1_dim)\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "        self.fc2 = nn.Linear(hidden1_dim, hidden2_dim)\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "        self.fc3 = nn.Linear(hidden2_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = torch.sigmoid(self.fc3(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "RpPwAzIvadwg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Enhanced sklearn Wrapper\n",
        "\n",
        "Update the wrapper to use the enhanced model."
      ],
      "metadata": {
        "id": "ZjSYAPw4aUHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SklearnEnhancedNN(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, input_dim, hidden1_dim=64, hidden2_dim=32, dropout_rate=0.5, learning_rate=0.001, epochs=50, batch_size=64):\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden1_dim = hidden1_dim\n",
        "        self.hidden2_dim = hidden2_dim\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.model = EnhancedNN(self.input_dim, self.hidden1_dim, self.hidden2_dim, self.dropout_rate)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        criterion = nn.BCELoss()\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "        train_dataset = torch.utils.data.TensorDataset(torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32).unsqueeze(1))\n",
        "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            self.model.train()\n",
        "            for inputs, targets in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                outputs = self.model(inputs)\n",
        "                loss = criterion(outputs, targets.view(-1, 1))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(torch.tensor(X, dtype=torch.float32))\n",
        "            predictions = (outputs > 0.5).float()\n",
        "        return predictions.numpy().squeeze()\n"
      ],
      "metadata": {
        "id": "US0YTXtJaOYr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train and Evaluate the Enhanced Model\n",
        "\n",
        "By following these steps, you can start with a simple PyTorch neural network model, train and evaluate it, and then gradually add more layers and complexity to see if it improves performance. This approach allows you to systematically explore the impact of different model architectures on the performance of your neural network."
      ],
      "metadata": {
        "id": "dZRgVdFYaC2r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDcGxBzFXwIk",
        "outputId": "ee4c170a-bd3a-4ba4-a00d-f04cc3dc5efd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.83      0.96      0.89      4687\n",
            "         1.0       0.69      0.32      0.43      1313\n",
            "\n",
            "    accuracy                           0.82      6000\n",
            "   macro avg       0.76      0.64      0.66      6000\n",
            "weighted avg       0.80      0.82      0.79      6000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Create an instance of SklearnEnhancedNN\n",
        "nn_estimator = SklearnEnhancedNN(input_dim=input_dim)\n",
        "\n",
        "# Fit the model\n",
        "nn_estimator.fit(X_train_tensor.numpy(), y_train_tensor.numpy())\n",
        "\n",
        "# Predict on the test set\n",
        "test_predictions = nn_estimator.predict(X_test_tensor.numpy())\n",
        "\n",
        "# Evaluate the model\n",
        "print(classification_report(y_test_tensor.numpy(), test_predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding More Complexity"
      ],
      "metadata": {
        "id": "TNgwXfudgpbq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's add more complexity to the neural network model by adding another hidden layer and experimenting with different activation functions and dropout rates. We can also tweak other hyperparameters to see if the performance improves further.\n",
        "\n",
        "### Step 1: Define a More Complex PyTorch Neural Network Model\n",
        "\n",
        "We'll add another hidden layer and experiment with different activation functions.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bv_hcc1befCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MoreComplexNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden1_dim=128, hidden2_dim=64, hidden3_dim=32, dropout_rate=0.5):\n",
        "        super(MoreComplexNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden1_dim)\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "        self.fc2 = nn.Linear(hidden1_dim, hidden2_dim)\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "        self.fc3 = nn.Linear(hidden2_dim, hidden3_dim)\n",
        "        self.dropout3 = nn.Dropout(dropout_rate)\n",
        "        self.fc4 = nn.Linear(hidden3_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        x = self.dropout3(x)\n",
        "        x = torch.sigmoid(self.fc4(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "OxIaupzxe2S2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Update the sklearn Wrapper\n",
        "\n",
        "Update the wrapper to use the more complex model."
      ],
      "metadata": {
        "id": "YUmgOh-Oex0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SklearnMoreComplexNN(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, input_dim, hidden1_dim=128, hidden2_dim=64, hidden3_dim=32, dropout_rate=0.5, learning_rate=0.001, epochs=50, batch_size=64):\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden1_dim = hidden1_dim\n",
        "        self.hidden2_dim = hidden2_dim\n",
        "        self.hidden3_dim = hidden3_dim\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.model = MoreComplexNN(self.input_dim, self.hidden1_dim, self.hidden2_dim, self.hidden3_dim, self.dropout_rate)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        criterion = nn.BCELoss()\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "        train_dataset = torch.utils.data.TensorDataset(torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32).unsqueeze(1))\n",
        "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            self.model.train()\n",
        "            for inputs, targets in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                outputs = self.model(inputs)\n",
        "                loss = criterion(outputs, targets.view(-1, 1))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(torch.tensor(X, dtype=torch.float32))\n",
        "            predictions = (outputs > 0.5).float()\n",
        "        return predictions.numpy().squeeze()"
      ],
      "metadata": {
        "id": "XQU194HkeuN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Train and Evaluate the More Complex Model"
      ],
      "metadata": {
        "id": "4RwDlmFQeoOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of SklearnMoreComplexNN\n",
        "nn_estimator = SklearnMoreComplexNN(input_dim=input_dim)\n",
        "\n",
        "# Fit the model\n",
        "nn_estimator.fit(X_train_tensor.numpy(), y_train_tensor.numpy())\n",
        "\n",
        "# Predict on the test set\n",
        "test_predictions = nn_estimator.predict(X_test_tensor.numpy())\n",
        "\n",
        "# Evaluate the model\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test_tensor.numpy(), test_predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7fl3aYJek3a",
        "outputId": "a8ca9d24-25d9-48b5-89d6-9ce6a1b1572a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.83      0.96      0.89      4687\n",
            "         1.0       0.69      0.32      0.43      1313\n",
            "\n",
            "    accuracy                           0.82      6000\n",
            "   macro avg       0.76      0.64      0.66      6000\n",
            "weighted avg       0.80      0.82      0.79      6000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classification Report Analysis\n",
        "\n",
        "\n",
        "### Key Metrics to Focus On\n",
        "\n",
        "1. **Precision**:\n",
        "   - **Class 0 (No Default)**: 0.83\n",
        "   - **Class 1 (Default)**: 0.69\n",
        "   - Precision is relatively high for both classes, but there is a noticeable drop for class 1.\n",
        "\n",
        "2. **Recall**:\n",
        "   - **Class 0 (No Default)**: 0.96\n",
        "   - **Class 1 (Default)**: 0.32\n",
        "   - Recall for class 0 is very high, but recall for class 1 is significantly low, indicating that many instances of class 1 are not being identified correctly.\n",
        "\n",
        "3. **F1-score**:\n",
        "   - **Class 0 (No Default)**: 0.89\n",
        "   - **Class 1 (Default)**: 0.43\n",
        "   - The F1-score for class 1 is much lower than for class 0, reflecting poor performance in correctly predicting defaults.\n",
        "\n",
        "4. **Support**:\n",
        "   - **Class 0 (No Default)**: 4687 instances\n",
        "   - **Class 1 (Default)**: 1313 instances\n",
        "   - There is a significant imbalance between the two classes, with class 0 having more than three times the number of instances as class 1.\n",
        "\n",
        "### Observations\n",
        "\n",
        "1. **Imbalance in Class Distribution**:\n",
        "   - The support values indicate a clear imbalance in the dataset, with many more instances of class 0 (no default) compared to class 1 (default).\n",
        "\n",
        "2. **High Precision, Low Recall for Class 1**:\n",
        "   - The precision for class 1 is not too bad, but the recall is very low. This means that while the model is reasonably good at predicting defaults when it does predict them, it misses a large number of actual defaults.\n",
        "   - This low recall for class 1 suggests that the model is biased towards predicting class 0, which is a common issue when dealing with imbalanced datasets.\n",
        "\n",
        "3. **Overall Performance**:\n",
        "   - The overall accuracy is relatively high at 0.82, but this is mainly driven by the high number of correct predictions for class 0.\n",
        "   - The macro and weighted averages of precision, recall, and F1-score indicate a discrepancy in performance between the classes.\n",
        "\n",
        "### Why Addressing Class Imbalance is Important\n",
        "\n",
        "- **Improving Recall for Class 1**:\n",
        "  - Addressing class imbalance can help improve the recall for class 1, ensuring that more instances of actual defaults are correctly identified by the model.\n",
        "\n",
        "- **Balanced Model Performance**:\n",
        "  - Balancing the classes can help the model to learn to differentiate better between the two classes, leading to more balanced precision, recall, and F1-scores across both classes.\n",
        "\n",
        "- **Preventing Model Bias**:\n",
        "  - An imbalanced dataset can lead to a model that is biased towards the majority class, as the model might learn to predict the majority class more often simply because it is overrepresented in the training data.\n",
        "\n",
        "### Recommended Approach\n",
        "\n",
        "- **Using SMOTE (Synthetic Minority Over-sampling Technique)**:\n",
        "  - SMOTE is a popular technique for addressing class imbalance. It works by generating synthetic examples for the minority class by interpolating between existing examples.\n",
        "  - This helps in creating a more balanced dataset, which can lead to better model performance, especially in terms of recall for the minority class.\n"
      ],
      "metadata": {
        "id": "hdRPeegTehXg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imabalanced Data Handling\n",
        "\n",
        "Let's implement SMOTE (Synthetic Minority Over-sampling Technique) to address the class imbalance and then retrain the model on the balanced dataset. Here's the step-by-step implementation:\n",
        "\n",
        "### Step 1: Install imbalanced-learn Library\n",
        "\n",
        "First, ensure you have the `imbalanced-learn` library installed. You can install it using pip if you haven't already:\n",
        "\n",
        "```bash\n",
        "pip install imbalanced-learn\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7FHJrQBTleYV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Apply SMOTE to Balance the Dataset\n",
        "\n",
        "We'll use SMOTE to oversample the minority class in the training data."
      ],
      "metadata": {
        "id": "bqDxMreCmAl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "import torch\n",
        "\n",
        "# Apply SMOTE to balance the dataset\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_processed, y_train)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train_resampled, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train_resampled.values, dtype=torch.float32).unsqueeze(1)\n",
        "X_test_tensor = torch.tensor(X_test_processed, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1)"
      ],
      "metadata": {
        "id": "hbKwxRnzl6Cp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train and Evaluate the Model\n",
        "\n",
        "Now, let's train and evaluate the model using the balanced dataset:\n",
        "\n",
        "\n",
        "### Summary\n",
        "\n",
        "By applying SMOTE to balance the dataset and retraining the model, we can address the class imbalance and improve the model's performance, especially in terms of recall for the minority class (defaults). This should lead to more balanced precision, recall, and F1-scores across both classes. Let's see how the performance metrics change after addressing the class imbalance."
      ],
      "metadata": {
        "id": "iDA0lejplmUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of SklearnMoreComplexNN\n",
        "input_dim = X_train_tensor.shape[1]\n",
        "nn_estimator = SklearnMoreComplexNN(input_dim=input_dim)\n",
        "\n",
        "# Fit the model\n",
        "nn_estimator.fit(X_train_tensor.numpy(), y_train_tensor.numpy())\n",
        "\n",
        "# Predict on the test set\n",
        "test_predictions = nn_estimator.predict(X_test_tensor.numpy())\n",
        "\n",
        "# Evaluate the model\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test_tensor.numpy(), test_predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRQQlqgIk3pT",
        "outputId": "6d4f3a67-e84e-4b44-f358-6745def11e04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.88      0.76      0.82      4687\n",
            "         1.0       0.43      0.64      0.51      1313\n",
            "\n",
            "    accuracy                           0.73      6000\n",
            "   macro avg       0.66      0.70      0.67      6000\n",
            "weighted avg       0.78      0.73      0.75      6000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's analyze the results after applying SMOTE to balance the dataset and retraining the model with the `MoreComplexNN` architecture:\n",
        "\n",
        "### Classification Report Analysis\n",
        "\n",
        "### Key Metrics to Focus On\n",
        "\n",
        "1. **Precision**:\n",
        "   - **Class 0 (No Default)**: 0.88\n",
        "   - **Class 1 (Default)**: 0.43\n",
        "   - Precision for class 0 is high, indicating that when the model predicts no default, it is usually correct. However, precision for class 1 is lower, meaning there are more false positives (incorrectly predicting defaults).\n",
        "\n",
        "2. **Recall**:\n",
        "   - **Class 0 (No Default)**: 0.76\n",
        "   - **Class 1 (Default)**: 0.64\n",
        "   - Recall for class 0 has decreased compared to before applying SMOTE, but recall for class 1 has significantly improved. This means the model is now better at identifying actual defaults.\n",
        "\n",
        "3. **F1-score**:\n",
        "   - **Class 0 (No Default)**: 0.82\n",
        "   - **Class 1 (Default)**: 0.51\n",
        "   - The F1-score for class 1 has improved, reflecting better overall performance in predicting defaults.\n",
        "\n",
        "4. **Support**:\n",
        "   - **Class 0 (No Default)**: 4687 instances\n",
        "   - **Class 1 (Default)**: 1313 instances\n",
        "   - The number of instances for each class remains the same as expected since support values are based on the original test set.\n",
        "\n",
        "5. **Overall Accuracy**:\n",
        "   - The overall accuracy is 0.73, which is lower than the initial accuracy before applying SMOTE. However, this is expected because the model is now also focusing on identifying the minority class.\n",
        "\n",
        "### Interpretation\n",
        "\n",
        "1. **Improved Recall for Class 1**:\n",
        "   - The recall for class 1 (defaults) has improved from 0.32 to 0.64. This is a significant improvement and indicates that the model is now correctly identifying more default cases.\n",
        "\n",
        "2. **Balanced Performance**:\n",
        "   - There is a trade-off between precision and recall for class 1. Precision has decreased, but recall has increased, leading to a more balanced F1-score for class 1.\n",
        "   - The overall macro and weighted averages for precision, recall, and F1-score indicate a more balanced performance between the two classes compared to the previous model.\n",
        "\n",
        "3. **Accuracy vs. Recall Trade-off**:\n",
        "   - The overall accuracy has decreased from 0.82 to 0.73, but this is not necessarily a bad thing. Accuracy is not always the best metric for imbalanced datasets. The improvement in recall for class 1 is more important in this context, as it means the model is better at identifying defaults, which could be critical in real-world applications.\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "To further improve the model's performance, consider the following steps:\n",
        "\n",
        "1. **Tune Hyperparameters**:\n",
        "   - Perform hyperparameter tuning to find the optimal settings for the model. This can include experimenting with different learning rates, batch sizes, number of epochs, and architectures.\n",
        "\n",
        "2. **Experiment with Different Architectures**:\n",
        "   - Try different neural network architectures, including deeper networks or different activation functions, to see if they provide better performance.\n",
        "\n",
        "3. **Regularization Techniques**:\n",
        "   - Implement regularization techniques like L2 regularization (weight decay) to prevent overfitting and improve generalization.\n",
        "\n",
        "4. **Adjust Decision Threshold**:\n",
        "   - Experiment with different decision thresholds for the classification to see if adjusting the threshold can further improve the recall for class 1.\n",
        "\n"
      ],
      "metadata": {
        "id": "bBfMYRypoYq0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HFz5vLycoiqL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}