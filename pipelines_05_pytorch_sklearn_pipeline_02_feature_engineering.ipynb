{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOi3cBVvgOb/u0bQKEsaip5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/pipelines/blob/main/pipelines_05_pytorch_sklearn_pipeline_02_feature_engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baseline Model Pipeline with Sklearn Wrapper"
      ],
      "metadata": {
        "id": "eTC5nDBP08iH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.metrics import classification_report, precision_recall_curve, f1_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "\n",
        "# Load the dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\"\n",
        "df = pd.read_excel(url, header=1)\n",
        "\n",
        "# Rename columns to lower case and replace spaces with underscores\n",
        "df.columns = [col.lower().replace(' ', '_') for col in df.columns]\n",
        "\n",
        "# Select features and target\n",
        "target = 'default_payment_next_month'\n",
        "X = df.drop(columns=[target])\n",
        "y = df[target]\n",
        "\n",
        "# Perform stratified train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Identify column types\n",
        "numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "# Define preprocessing for numeric columns\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())])\n",
        "\n",
        "# Define preprocessing for categorical columns\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "# Combine preprocessing steps\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)])\n",
        "\n",
        "# Fit and transform the data\n",
        "preprocessing_pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
        "X_train_processed = preprocessing_pipeline.fit_transform(X_train)\n",
        "X_test_processed = preprocessing_pipeline.transform(X_test)\n",
        "\n",
        "# Apply SMOTE to balance the training dataset\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_processed, y_train)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train_resampled, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train_resampled.values, dtype=torch.float32).unsqueeze(1)\n",
        "X_test_tensor = torch.tensor(X_test_processed, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "optimal_threshold = 0.8000141978263855\n",
        "\n",
        "# Define a simple neural network model\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 32)\n",
        "        self.fc2 = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Define the sklearn wrapper for the neural network model\n",
        "class SklearnSimpleNN(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, input_dim, learning_rate=0.001, epochs=50, batch_size=64, pos_weight=1.0):\n",
        "        self.input_dim = input_dim\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.pos_weight = pos_weight  # Accept as float\n",
        "        self.model = SimpleNN(self.input_dim)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Convert pos_weight to tensor here\n",
        "        criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(self.pos_weight, dtype=torch.float32))\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "        train_dataset = torch.utils.data.TensorDataset(torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32).unsqueeze(1))\n",
        "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            self.model.train()\n",
        "            for inputs, targets in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                outputs = self.model(inputs)\n",
        "                loss = criterion(outputs, targets.view(-1, 1))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(torch.tensor(X, dtype=torch.float32))\n",
        "            probabilities = torch.sigmoid(outputs)\n",
        "            predictions = (probabilities > optimal_threshold).float()  # Use the manually adjusted threshold here\n",
        "        return predictions.numpy().squeeze()\n",
        "\n",
        "# Calculate the class weights\n",
        "class_weights = len(y_train) / (2 * np.bincount(y_train))\n",
        "\n",
        "# Create an instance of SklearnSimpleNN with the adjusted weight\n",
        "input_dim = X_train_tensor.shape[1]\n",
        "nn_estimator = SklearnSimpleNN(input_dim=input_dim, pos_weight=class_weights[1])\n",
        "\n",
        "# Fit the model\n",
        "nn_estimator.fit(X_train_tensor.numpy(), y_train_tensor.numpy())\n",
        "\n",
        "# Predict on the test set with the optimal threshold\n",
        "test_predictions_optimal_threshold = nn_estimator.predict(X_test_tensor.numpy())\n",
        "\n",
        "# Evaluate the model with the optimal threshold\n",
        "print(classification_report(y_test_tensor.numpy(), test_predictions_optimal_threshold))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXHZDvPeueTe",
        "outputId": "26a53acf-896b-4975-cb72-7110ebc63d71"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.86      0.89      0.87      4673\n",
            "         1.0       0.54      0.48      0.51      1327\n",
            "\n",
            "    accuracy                           0.80      6000\n",
            "   macro avg       0.70      0.68      0.69      6000\n",
            "weighted avg       0.79      0.80      0.79      6000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rato Feature\n",
        "\n",
        "The `RatioFeatures` transformer is a custom transformer class used in a data preprocessing pipeline. Its main purpose is to create new features by calculating the ratio between pairs of existing features. Specifically, in this context, it generates ratio features between the payment amounts (`pay_amt`) and the corresponding billing amounts (`bill_amt`) for different months."
      ],
      "metadata": {
        "id": "7W-NDVkZRBZb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What the `RatioFeatures` Transformer Does:\n",
        "\n",
        "1. **Initialization**: The transformer is initialized without any parameters, meaning it doesn't require any specific setup when instantiated.\n",
        "\n",
        "2. **Fit Method**: The `fit` method is a standard requirement for transformers in scikit-learn's API. In this transformer, it doesn't perform any action and simply returns `self`. This is because the transformation logic doesn't depend on the training data (it doesn't learn any parameters from the data).\n",
        "\n",
        "3. **Transform Method**: The core functionality of the transformer is implemented in the `transform` method:\n",
        "   - **Copying Data**: It starts by making a copy of the input DataFrame to avoid modifying the original data.\n",
        "   - **Creating Ratios**: For each month (from 1 to 6), it computes the ratio of `pay_amt` to `bill_amt` and creates a new column for each ratio. For instance, `pay_amt1` is divided by `bill_amt1`, and the result is stored in a new column `pay_to_bill_ratio_1`. This process is repeated for all six months.\n",
        "   - **Avoiding Division by Zero**: A small constant (`1e-9`) is added to the billing amounts in the denominator to prevent division by zero, which would otherwise result in `NaN` values.\n",
        "\n",
        "### Purpose of Ratio Features:\n",
        "\n",
        "The ratio features created by the `RatioFeatures` transformer can provide meaningful insights into the customer's payment behavior. For example:\n",
        "- A high ratio might indicate that a customer is paying a large portion or all of their bill, which could be a positive indicator.\n",
        "- A low ratio might indicate that a customer is paying only a small portion of their bill, which could be a negative indicator.\n",
        "\n",
        "These new features can help improve the performance of machine learning models by providing additional information that might be more predictive of the target variable (in this case, whether the customer will default on their credit card payment next month).\n",
        "\n",
        "### Example of the `RatioFeatures` Transformer Code:\n",
        "\n",
        "```python\n",
        "class RatioFeatures(TransformerMixin, BaseEstimator):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        # Create ratios for bill_amt and pay_amt columns\n",
        "        for i in range(1, 7):\n",
        "            bill_col = f'bill_amt{i}'\n",
        "            pay_col = f'pay_amt{i}'\n",
        "            ratio_col = f'pay_to_bill_ratio_{i}'\n",
        "            X[ratio_col] = X[pay_col] / (X[bill_col] + 1e-9)  # Add a small constant to avoid division by zero\n",
        "        return X\n",
        "```\n",
        "\n",
        "By including this transformer in a data preprocessing pipeline, we ensure that the ratio features are automatically created and included in the model training process, potentially enhancing the model's predictive power."
      ],
      "metadata": {
        "id": "qjcg7dk8Prs7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.metrics import classification_report, precision_recall_curve, f1_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Load the dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\"\n",
        "df = pd.read_excel(url, header=1)\n",
        "\n",
        "# Rename columns to lower case and replace spaces with underscores\n",
        "df.columns = [col.lower().replace(' ', '_') for col in df.columns]\n",
        "\n",
        "# Select features and target\n",
        "target = 'default_payment_next_month'\n",
        "X = df.drop(columns=[target])\n",
        "y = df[target]\n",
        "\n",
        "# Perform stratified train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Identify column types\n",
        "numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "# Custom transformer for ratio features\n",
        "class RatioFeatures(TransformerMixin, BaseEstimator):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        # Create ratios for bill_amt and pay_amt columns\n",
        "        for i in range(1, 7):\n",
        "            bill_col = f'bill_amt{i}'\n",
        "            pay_col = f'pay_amt{i}'\n",
        "            ratio_col = f'pay_to_bill_ratio_{i}'\n",
        "            X[ratio_col] = X[pay_col] / (X[bill_col] + 1e-9)  # Add a small constant to avoid division by zero\n",
        "        return X\n",
        "\n",
        "# Define preprocessing for numeric columns\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())])\n",
        "\n",
        "# Define preprocessing for categorical columns\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "# Combine preprocessing steps and feature engineering\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features),\n",
        "        ('ratio', RatioFeatures(), numeric_features)\n",
        "    ])\n",
        "\n",
        "# Define feature engineering pipeline\n",
        "feature_engineering_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor)\n",
        "])\n",
        "\n",
        "# Fit and transform the training data\n",
        "X_train_fe = feature_engineering_pipeline.fit_transform(X_train)\n",
        "X_test_fe = feature_engineering_pipeline.transform(X_test)\n",
        "\n",
        "# Check for any remaining NaNs and impute them\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "X_train_fe = imputer.fit_transform(X_train_fe)\n",
        "X_test_fe = imputer.transform(X_test_fe)\n",
        "\n",
        "# Apply SMOTE to balance the training dataset\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled_fe, y_train_resampled_fe = smote.fit_resample(X_train_fe, y_train)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor_fe = torch.tensor(X_train_resampled_fe, dtype=torch.float32)  # Convert to dense\n",
        "y_train_tensor_fe = torch.tensor(y_train_resampled_fe.values, dtype=torch.float32).unsqueeze(1)\n",
        "X_test_tensor_fe = torch.tensor(X_test_fe, dtype=torch.float32)  # Convert to dense\n",
        "y_test_tensor_fe = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "optimal_threshold = 0.8000141978263855\n",
        "\n",
        "# Define a simple neural network model\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 32)\n",
        "        self.fc2 = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Define the sklearn wrapper for the neural network model\n",
        "class SklearnSimpleNN(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, input_dim, learning_rate=0.001, epochs=50, batch_size=64, pos_weight=1.0):\n",
        "        self.input_dim = input_dim\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.pos_weight = pos_weight  # Accept as float\n",
        "        self.model = SimpleNN(self.input_dim)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Convert pos_weight to tensor here\n",
        "        criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(self.pos_weight, dtype=torch.float32))\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "        train_dataset = torch.utils.data.TensorDataset(torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32).unsqueeze(1))\n",
        "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            self.model.train()\n",
        "            for inputs, targets in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                outputs = self.model(inputs)\n",
        "                loss = criterion(outputs, targets.view(-1, 1))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(torch.tensor(X, dtype=torch.float32))\n",
        "            probabilities = torch.sigmoid(outputs)\n",
        "            predictions = (probabilities > optimal_threshold).float()  # Use the manually adjusted threshold here\n",
        "        return predictions.numpy().squeeze()\n",
        "\n",
        "# Calculate the class weights\n",
        "class_weights = len(y_train) / (2 * np.bincount(y_train))\n",
        "\n",
        "# Create an instance of SklearnSimpleNN with the adjusted weight\n",
        "input_dim = X_train_tensor_fe.shape[1]\n",
        "nn_estimator_fe = SklearnSimpleNN(input_dim=input_dim, pos_weight=class_weights[1])\n",
        "\n",
        "# Fit the model with feature engineering\n",
        "nn_estimator_fe.fit(X_train_tensor_fe.numpy(), y_train_tensor_fe.numpy())\n",
        "\n",
        "# Predict on the test set with the optimal threshold\n",
        "test_predictions_fe = nn_estimator_fe.predict(X_test_tensor_fe.numpy())\n",
        "\n",
        "# Evaluate the model with the optimal threshold\n",
        "print(classification_report(y_test_tensor_fe.numpy(), test_predictions_fe))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7nvxrhSMwya",
        "outputId": "e3890daf-3216-445b-e40f-2a54a638f724"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.89      0.84      4673\n",
            "         1.0       0.37      0.22      0.28      1327\n",
            "\n",
            "    accuracy                           0.74      6000\n",
            "   macro avg       0.59      0.56      0.56      6000\n",
            "weighted avg       0.71      0.74      0.72      6000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Analysis of the Classification Report\n",
        "\n",
        "**Class 0 (No Default):**\n",
        "- **Precision**: 0.80, meaning 80% of instances predicted as no default are actually no default.\n",
        "- **Recall**: 0.89, meaning 89% of actual no default instances are correctly identified.\n",
        "- **F1-score**: 0.84, which is the harmonic mean of precision and recall.\n",
        "- **Support**: 4673 instances.\n",
        "\n",
        "**Class 1 (Default):**\n",
        "- **Precision**: 0.37, meaning 37% of instances predicted as default are actually default.\n",
        "- **Recall**: 0.22, meaning only 22% of actual default instances are correctly identified.\n",
        "- **F1-score**: 0.28, which is the harmonic mean of precision and recall.\n",
        "- **Support**: 1327 instances.\n",
        "\n",
        "**Overall Metrics:**\n",
        "- **Accuracy**: 0.74, meaning 74% of all instances are correctly classified.\n",
        "- **Macro Avg**: Precision: 0.59, Recall: 0.56, F1-score: 0.56.\n",
        "- **Weighted Avg**: Precision: 0.71, Recall: 0.74, F1-score: 0.72.\n",
        "\n",
        "### Interpretation\n",
        "\n",
        "- The model performs well for Class 0 (no default) with high precision and recall.\n",
        "- The performance for Class 1 (default) is relatively poor, with low precision and recall, resulting in a low F1-score.\n",
        "- The overall accuracy is driven by the model's performance on the majority class (no default).\n",
        "\n",
        "### Recommendations for Improvement\n",
        "\n",
        "1. **Class Imbalance Handling**:\n",
        "   - Despite using SMOTE, the model still struggles with class imbalance. Fine-tuning the oversampling ratio or experimenting with other techniques like ADASYN, or ensemble methods such as BalancedBaggingClassifier, might help.\n",
        "   \n",
        "2. **Feature Engineering**:\n",
        "   - Explore additional feature engineering, such as creating more interaction features, or using domain knowledge to create features that might capture important aspects of the data.\n",
        "   \n",
        "3. **Threshold Tuning**:\n",
        "   - Fine-tune the decision threshold specifically for Class 1 to improve precision and recall for the minority class.\n",
        "   \n",
        "4. **Model Complexity**:\n",
        "   - Consider using a more complex model or an ensemble of models to capture more patterns in the data.\n",
        "   \n",
        "5. **Hyperparameter Tuning**:\n",
        "   - Perform a thorough hyperparameter tuning using techniques like GridSearchCV or RandomizedSearchCV to find the optimal set of parameters for the model.\n",
        "\n",
        "### Next Steps: Further Feature Engineering\n",
        "\n",
        "1. **Interaction Features**:\n",
        "   - Create interaction features between various columns to capture more complex relationships.\n",
        "   \n",
        "2. **Statistical Features**:\n",
        "   - Calculate statistical features such as mean, median, variance, etc., for bill amounts and payment amounts.\n",
        "   \n",
        "3. **Behavioral Features**:\n",
        "   - Features capturing behavioral patterns, such as the frequency of full payments or the number of months with no payment, could be insightful.\n",
        "\n",
        "Let's implement additional feature engineering steps to capture more patterns in the data.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "i4OeorXhQldK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interaction Feature Engineering\n",
        "\n",
        "### Summary of Changes\n",
        "\n",
        "1. **Interaction Features**: Added a new transformer to create interaction features between bill amounts and payment amounts.\n",
        "2. **Updated Preprocessing Pipeline**: Included the new interaction features transformer in the preprocessing pipeline.\n",
        "\n",
        "These changes aim to capture more complex relationships in the data and potentially improve the model's performance on the minority class."
      ],
      "metadata": {
        "id": "nmjxBBp4Ro29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.metrics import classification_report, precision_recall_curve, f1_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Load the dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\"\n",
        "df = pd.read_excel(url, header=1)\n",
        "\n",
        "# Rename columns to lower case and replace spaces with underscores\n",
        "df.columns = [col.lower().replace(' ', '_') for col in df.columns]\n",
        "\n",
        "# Select features and target\n",
        "target = 'default_payment_next_month'\n",
        "X = df.drop(columns=[target])\n",
        "y = df[target]\n",
        "\n",
        "# Perform stratified train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Identify column types\n",
        "numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "# Custom transformer for ratio features\n",
        "class RatioFeatures(TransformerMixin, BaseEstimator):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        # Create ratios for bill_amt and pay_amt columns\n",
        "        for i in range(1, 7):\n",
        "            bill_col = f'bill_amt{i}'\n",
        "            pay_col = f'pay_amt{i}'\n",
        "            ratio_col = f'pay_to_bill_ratio_{i}'\n",
        "            X[ratio_col] = X[pay_col] / (X[bill_col] + 1e-9)  # Add a small constant to avoid division by zero\n",
        "        return X\n",
        "\n",
        "# Custom transformer for interaction features\n",
        "class InteractionFeatures(TransformerMixin, BaseEstimator):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        # Create interaction features between bill_amt and pay_amt columns\n",
        "        for i in range(1, 7):\n",
        "            bill_col = f'bill_amt{i}'\n",
        "            pay_col = f'pay_amt{i}'\n",
        "            interaction_col = f'bill_pay_interaction_{i}'\n",
        "            X[interaction_col] = X[bill_col] * X[pay_col]\n",
        "        return X\n",
        "\n",
        "# Define preprocessing for numeric columns\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())])\n",
        "\n",
        "# Define preprocessing for categorical columns\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "# Combine preprocessing steps and feature engineering\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features),\n",
        "        ('ratio', RatioFeatures(), numeric_features),\n",
        "        ('interaction', InteractionFeatures(), numeric_features)\n",
        "    ])\n",
        "\n",
        "# Define feature engineering pipeline\n",
        "feature_engineering_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor)\n",
        "])\n",
        "\n",
        "# Fit and transform the training data\n",
        "X_train_fe = feature_engineering_pipeline.fit_transform(X_train)\n",
        "X_test_fe = feature_engineering_pipeline.transform(X_test)\n",
        "\n",
        "# Check for any remaining NaNs and impute them\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "X_train_fe = imputer.fit_transform(X_train_fe)\n",
        "X_test_fe = imputer.transform(X_test_fe)\n",
        "\n",
        "# Apply SMOTE to balance the training dataset\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled_fe, y_train_resampled_fe = smote.fit_resample(X_train_fe, y_train)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor_fe = torch.tensor(X_train_resampled_fe, dtype=torch.float32)  # Convert to dense\n",
        "y_train_tensor_fe = torch.tensor(y_train_resampled_fe.values, dtype=torch.float32).unsqueeze(1)\n",
        "X_test_tensor_fe = torch.tensor(X_test_fe, dtype=torch.float32)  # Convert to dense\n",
        "y_test_tensor_fe = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "optimal_threshold = 0.8000141978263855\n",
        "\n",
        "# Define a simple neural network model\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 32)\n",
        "        self.fc2 = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Define the sklearn wrapper for the neural network model\n",
        "class SklearnSimpleNN(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, input_dim, learning_rate=0.001, epochs=50, batch_size=64, pos_weight=1.0):\n",
        "        self.input_dim = input_dim\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.pos_weight = pos_weight  # Accept as float\n",
        "        self.model = SimpleNN(self.input_dim)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Convert pos_weight to tensor here\n",
        "        criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(self.pos_weight, dtype=torch.float32))\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "        train_dataset = torch.utils.data.TensorDataset(torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32).unsqueeze(1))\n",
        "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            self.model.train()\n",
        "            for inputs, targets in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                outputs = self.model(inputs)\n",
        "                loss = criterion(outputs, targets.view(-1, 1))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(torch.tensor(X, dtype=torch.float32))\n",
        "            probabilities = torch.sigmoid(outputs)\n",
        "            predictions = (probabilities > optimal_threshold).float()  # Use the manually adjusted threshold here\n",
        "        return predictions.numpy().squeeze()\n",
        "\n",
        "# Calculate the class weights\n",
        "class_weights = len(y_train) / (2 * np.bincount(y_train))\n",
        "\n",
        "# Create an instance of SklearnSimpleNN with the adjusted weight\n",
        "input_dim = X_train_tensor_fe.shape[1]\n",
        "nn_estimator_fe = SklearnSimpleNN(input_dim=input_dim, pos_weight=class_weights[1])\n",
        "\n",
        "# Fit the model with feature engineering\n",
        "nn_estimator_fe.fit(X_train_tensor_fe.numpy(), y_train_tensor_fe.numpy())\n",
        "\n",
        "# Predict on the test set with the optimal threshold\n",
        "test_predictions_fe = nn_estimator_fe.predict(X_test_tensor_fe.numpy())\n",
        "\n",
        "# Evaluate the model with the optimal threshold\n",
        "print(classification_report(y_test_tensor_fe.numpy(), test_predictions_fe))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6SKtr-mMwpk",
        "outputId": "6a01b829-e084-49f5-ce5e-77b1c72af0d7"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.78      0.99      0.87      4673\n",
            "         1.0       0.26      0.01      0.02      1327\n",
            "\n",
            "    accuracy                           0.77      6000\n",
            "   macro avg       0.52      0.50      0.45      6000\n",
            "weighted avg       0.67      0.77      0.68      6000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Analysis of the Classification Report\n",
        "\n",
        "The updated classification report indicates that the model's performance for the minority class (default) has worsened, with very low precision and recall for Class 1 (default). This suggests that the additional features have not improved the model's ability to identify defaults and may have even led to overfitting to the majority class.\n",
        "\n",
        "### Interpretation\n",
        "\n",
        "- The model performs very well for Class 0 (no default) with high recall, but this comes at the cost of poor performance for Class 1 (default).\n",
        "- The precision and recall for Class 1 are both very low, indicating that the model is not identifying default instances effectively.\n",
        "\n",
        "### Recommendations for Further Improvement\n",
        "\n",
        "1. **Further Tuning of SMOTE**:\n",
        "   - Experiment with different SMOTE ratios to see if this can help balance the classes more effectively.\n",
        "   \n",
        "2. **Alternative Sampling Methods**:\n",
        "   - Try other resampling techniques such as ADASYN, RandomUnderSampler, or a combination of oversampling and undersampling.\n",
        "\n",
        "3. **Ensemble Methods**:\n",
        "   - Consider using ensemble methods such as Random Forest, Gradient Boosting, or XGBoost, which can often handle imbalanced data better.\n",
        "\n",
        "4. **Model Complexity and Hyperparameter Tuning**:\n",
        "   - Perform hyperparameter tuning for the neural network model, such as adjusting the learning rate, number of epochs, batch size, and architecture of the network.\n",
        "\n",
        "5. **Feature Selection**:\n",
        "   - Use feature selection techniques such as Recursive Feature Elimination (RFE) or feature importance scores from tree-based models to select the most relevant features.\n",
        "\n",
        "### Implementation of Further Tuning of SMOTE and Alternative Sampling Methods\n",
        "\n",
        "Let's first experiment with different SMOTE ratios to see if this can improve the model's performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "qyIClnFvSWd7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.metrics import classification_report, precision_recall_curve, f1_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Load the dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\"\n",
        "df = pd.read_excel(url, header=1)\n",
        "\n",
        "# Rename columns to lower case and replace spaces with underscores\n",
        "df.columns = [col.lower().replace(' ', '_') for col in df.columns]\n",
        "\n",
        "# Select features and target\n",
        "target = 'default_payment_next_month'\n",
        "X = df.drop(columns=[target])\n",
        "y = df[target]\n",
        "\n",
        "# Perform stratified train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Identify column types\n",
        "numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "# Custom transformer for ratio features\n",
        "class RatioFeatures(TransformerMixin, BaseEstimator):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        # Create ratios for bill_amt and pay_amt columns\n",
        "        for i in range(1, 7):\n",
        "            bill_col = f'bill_amt{i}'\n",
        "            pay_col = f'pay_amt{i}'\n",
        "            ratio_col = f'pay_to_bill_ratio_{i}'\n",
        "            X[ratio_col] = X[pay_col] / (X[bill_col] + 1e-9)  # Add a small constant to avoid division by zero\n",
        "        return X\n",
        "\n",
        "# Custom transformer for interaction features\n",
        "class InteractionFeatures(TransformerMixin, BaseEstimator):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        # Create interaction features between bill_amt and pay_amt columns\n",
        "        for i in range(1, 7):\n",
        "            bill_col = f'bill_amt{i}'\n",
        "            pay_col = f'pay_amt{i}'\n",
        "            interaction_col = f'bill_pay_interaction_{i}'\n",
        "            X[interaction_col] = X[bill_col] * X[pay_col]\n",
        "        return X\n",
        "\n",
        "# Define preprocessing for numeric columns\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())])\n",
        "\n",
        "# Define preprocessing for categorical columns\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "# Combine preprocessing steps and feature engineering\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features),\n",
        "        ('ratio', RatioFeatures(), numeric_features),\n",
        "        ('interaction', InteractionFeatures(), numeric_features)\n",
        "    ])\n",
        "\n",
        "# Define feature engineering pipeline\n",
        "feature_engineering_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor)\n",
        "])\n",
        "\n",
        "# Fit and transform the training data\n",
        "X_train_fe = feature_engineering_pipeline.fit_transform(X_train)\n",
        "X_test_fe = feature_engineering_pipeline.transform(X_test)\n",
        "\n",
        "# Check for any remaining NaNs and impute them\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "X_train_fe = imputer.fit_transform(X_train_fe)\n",
        "X_test_fe = imputer.transform(X_test_fe)\n",
        "\n",
        "# Experiment with different SMOTE ratios\n",
        "smote_ratios = [0.5, 0.75, 1.0]\n",
        "best_model = None\n",
        "best_f1_score = 0\n",
        "best_ratio = None\n",
        "\n",
        "for ratio in smote_ratios:\n",
        "    smote = SMOTE(sampling_strategy=ratio, random_state=42)\n",
        "    X_train_resampled_fe, y_train_resampled_fe = smote.fit_resample(X_train_fe, y_train)\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    X_train_tensor_fe = torch.tensor(X_train_resampled_fe, dtype=torch.float32)  # Convert to dense\n",
        "    y_train_tensor_fe = torch.tensor(y_train_resampled_fe.values, dtype=torch.float32).unsqueeze(1)\n",
        "    X_test_tensor_fe = torch.tensor(X_test_fe, dtype=torch.float32)  # Convert to dense\n",
        "    y_test_tensor_fe = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "    # Define a simple neural network model\n",
        "    class SimpleNN(nn.Module):\n",
        "        def __init__(self, input_dim):\n",
        "            super(SimpleNN, self).__init__()\n",
        "            self.fc1 = nn.Linear(input_dim, 32)\n",
        "            self.fc2 = nn.Linear(32, 1)\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = torch.relu(self.fc1(x))\n",
        "            x = self.fc2(x)\n",
        "            return x\n",
        "\n",
        "    # Define the sklearn wrapper for the neural network model\n",
        "    class SklearnSimpleNN(BaseEstimator, ClassifierMixin):\n",
        "        def __init__(self, input_dim, learning_rate=0.001, epochs=50, batch_size=64, pos_weight=1.0):\n",
        "            self.input_dim = input_dim\n",
        "            self.learning_rate = learning_rate\n",
        "            self.epochs = epochs\n",
        "            self.batch_size = batch_size\n",
        "            self.pos_weight = pos_weight  # Accept as float\n",
        "            self.model = SimpleNN(self.input_dim)\n",
        "\n",
        "        def fit(self, X, y):\n",
        "            # Convert pos_weight to tensor here\n",
        "            criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(self.pos_weight, dtype=torch.float32))\n",
        "            optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "            train_dataset = torch.utils.data.TensorDataset(torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32).unsqueeze(1))\n",
        "            train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "            for epoch in range(self.epochs):\n",
        "                self.model.train()\n",
        "                for inputs, targets in train_loader:\n",
        "                    optimizer.zero_grad()\n",
        "                    outputs = self.model(inputs)\n",
        "                    loss = criterion(outputs, targets.view(-1, 1))\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "            return self\n",
        "\n",
        "        def predict(self, X):\n",
        "            self.model.eval()\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(torch.tensor(X, dtype=torch.float32))\n",
        "                probabilities = torch.sigmoid(outputs)\n",
        "                predictions = (probabilities > optimal_threshold).float()  # Use the manually adjusted threshold here\n",
        "            return predictions.numpy().squeeze()\n",
        "\n",
        "    # Calculate the class weights\n",
        "    class_weights = len(y_train) / (2 * np.bincount(y_train))\n",
        "\n",
        "    # Create an instance of SklearnSimpleNN with the adjusted weight\n",
        "    input_dim = X_train_tensor_fe.shape[1]\n",
        "    nn_estimator_fe = SklearnSimpleNN(input_dim=input_dim, pos_weight=class_weights[1])\n",
        "\n",
        "    # Fit the model with feature engineering\n",
        "    nn_estimator_fe.fit(X_train_tensor_fe.numpy(), y_train_tensor_fe.numpy())\n",
        "\n",
        "    # Predict on the test set with the optimal threshold\n",
        "    test_predictions_fe = nn_estimator_fe.predict(X_test_tensor_fe.numpy())\n",
        "\n",
        "    # Evaluate the model with the optimal threshold\n",
        "    report = classification_report(y_test_tensor_fe.numpy(), test_predictions_fe, output_dict=True)\n",
        "    f1 = report['1']['f1-score']\n",
        "\n",
        "    if f1 > best_f1_score:\n",
        "        best_f1_score = f1\n",
        "        best_model = nn_estimator_fe\n",
        "        best_ratio = ratio\n",
        "\n",
        "# Print the best ratio and the classification report of the best model\n",
        "print(f'Best SMOTE Ratio: {best_ratio}')\n",
        "print(classification_report(y_test_tensor_fe.numpy(), best_model.predict(X_test_tensor_fe.numpy())))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "eK7_hJ2CMF4D",
        "outputId": "61b328ac-136f-461b-91f5-9746b47ce8ab"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'1'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-e181f7f8b151>\u001b[0m in \u001b[0;36m<cell line: 110>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;31m# Evaluate the model with the optimal threshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_tensor_fe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_predictions_fe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m     \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreport\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'f1-score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mf1\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_f1_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: '1'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Caf25i13SqGk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}