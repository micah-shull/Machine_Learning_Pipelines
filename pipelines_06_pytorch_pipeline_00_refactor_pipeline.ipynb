{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPCHJcwm7hyDTOO4IynwDb3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/pipelines/blob/main/pipelines_06_pytorch_pipeline_00_refactor_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Pipeline"
      ],
      "metadata": {
        "id": "iYGBroz8piwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import classification_report\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Load the dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\"\n",
        "df = pd.read_excel(url, header=1)\n",
        "\n",
        "# Rename columns to lower case and replace spaces with underscores\n",
        "df.columns = [col.lower().replace(' ', '_') for col in df.columns]\n",
        "\n",
        "# Convert specific numeric columns to categorical\n",
        "categorical_columns = ['sex', 'education', 'marriage']\n",
        "df[categorical_columns] = df[categorical_columns].astype('category')\n",
        "\n",
        "# Select features and target\n",
        "target = 'default_payment_next_month'\n",
        "X = df.drop(columns=[target])\n",
        "y = df[target]\n",
        "\n",
        "# Perform stratified train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Identify column types\n",
        "numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "# Define preprocessing for numeric columns\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())])\n",
        "\n",
        "# Define preprocessing for categorical columns\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "# Combine preprocessing steps\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)])\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "def set_seed(seed):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# Custom preprocessor to ensure output is a DataFrame\n",
        "class DataFramePreprocessor(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, preprocessor):\n",
        "        self.preprocessor = preprocessor\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        self.preprocessor.fit(X, y)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_transformed = self.preprocessor.transform(X)\n",
        "        return pd.DataFrame(X_transformed)\n",
        "\n",
        "# Optimal number of principal components determined from PCA iteration\n",
        "optimal_n_components = 17\n",
        "\n",
        "# Create a new pipeline with preprocessing and PCA\n",
        "pca_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', DataFramePreprocessor(preprocessor)),\n",
        "    ('pca', PCA(n_components=optimal_n_components))\n",
        "])\n",
        "\n",
        "# Fit and transform the training data\n",
        "X_train_pca = pca_pipeline.fit_transform(X_train, y_train)\n",
        "X_test_pca = pca_pipeline.transform(X_test)\n",
        "\n",
        "# Apply RandomUnderSampler to balance the training dataset\n",
        "undersampler = RandomUnderSampler(sampling_strategy=0.75, random_state=42)\n",
        "X_train_resampled, y_train_resampled = undersampler.fit_resample(X_train_pca, y_train)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train_resampled, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train_resampled.to_numpy(), dtype=torch.float32).unsqueeze(1)\n",
        "X_test_tensor = torch.tensor(X_test_pca, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test.to_numpy(), dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "optimal_threshold = 0.8000141978263855\n",
        "\n",
        "# Define a simple neural network model\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 32)\n",
        "        self.fc2 = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Define the sklearn wrapper for the neural network model\n",
        "class SklearnSimpleNN(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, input_dim, learning_rate=0.001, epochs=50, batch_size=64, pos_weight=1.0):\n",
        "        self.input_dim = input_dim\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.pos_weight = pos_weight  # Accept as float\n",
        "        self.model = SimpleNN(self.input_dim)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Convert pos_weight to tensor here\n",
        "        criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(self.pos_weight, dtype=torch.float32))\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "        train_dataset = torch.utils.data.TensorDataset(torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32).unsqueeze(1))\n",
        "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            self.model.train()\n",
        "            for inputs, targets in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                outputs = self.model(inputs)\n",
        "                loss = criterion(outputs, targets.view(-1, 1))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(torch.tensor(X, dtype=torch.float32))\n",
        "            probabilities = torch.sigmoid(outputs)\n",
        "            predictions = (probabilities > optimal_threshold).float()  # Use the manually adjusted threshold here\n",
        "        return predictions.numpy().squeeze()\n",
        "\n",
        "# Calculate the class weights\n",
        "class_weights = len(y_train) / (2 * np.bincount(y_train))\n",
        "\n",
        "# Create an instance of SklearnSimpleNN with the adjusted weight\n",
        "input_dim = X_train_tensor.shape[1]\n",
        "nn_estimator = SklearnSimpleNN(input_dim=input_dim, pos_weight=class_weights[1])\n",
        "\n",
        "# Fit the model\n",
        "nn_estimator.fit(X_train_tensor.numpy(), y_train_tensor.numpy())\n",
        "\n",
        "# Predict on the test set with the optimal threshold\n",
        "y_pred_pca = nn_estimator.predict(X_test_tensor.numpy())\n",
        "\n",
        "# Evaluate the model with the optimal threshold\n",
        "print(classification_report(y_test_tensor.numpy(), y_pred_pca))\n"
      ],
      "metadata": {
        "id": "ah-kVDA-eeEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Starting with small, manageable pieces and gradually building up your pipeline is a smart approach. This way, you can ensure that each part works correctly before moving on to the next step. We'll begin with the data preprocessing part and then incrementally add more functionality.\n",
        "\n",
        "### Step 1: Create Basic Data Preprocessing Functions\n",
        "\n",
        "We'll start by defining the basic data preprocessing functions in `model_pipeline.py` and testing them in the notebook.\n",
        "\n",
        "#### 1.1 Define Basic Functions in `model_pipeline.py`\n",
        "\n",
        "First, create the basic functions for loading data, cleaning columns, and preprocessing data.\n"
      ],
      "metadata": {
        "id": "59zwJnvlgCfp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "import logging\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Load the dataset from a URL\n",
        "def load_data_from_url(url):\n",
        "    df = pd.read_excel(url, header=1)\n",
        "    return df\n",
        "\n",
        "# Clean column names\n",
        "def clean_column_names(df):\n",
        "    df.columns = [col.lower().replace(' ', '_') for col in df.columns]\n",
        "    return df\n",
        "\n",
        "# Remove the 'id' column\n",
        "def remove_id_column(df):\n",
        "    if 'id' in df.columns:\n",
        "        df = df.drop(columns=['id'])\n",
        "    return df\n",
        "\n",
        "# Convert specified columns to categorical type\n",
        "def convert_categorical(df, categorical_columns):\n",
        "    df[categorical_columns] = df[categorical_columns].astype('category')\n",
        "    return df\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "def split_data(df, target):\n",
        "    X = df.drop(columns=[target])\n",
        "    y = df[target]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "# Define the preprocessor\n",
        "def define_preprocessor(X_train):\n",
        "    numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "    categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "    numeric_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())])\n",
        "\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numeric_transformer, numeric_features),\n",
        "            ('cat', categorical_transformer, categorical_features)])\n",
        "\n",
        "    return preprocessor\n",
        "\n",
        "# Preprocess the data\n",
        "def preprocess_data(preprocessor, X_train, X_test):\n",
        "    X_train_processed = preprocessor.fit_transform(X_train)\n",
        "    X_test_processed = preprocessor.transform(X_test)\n",
        "    return X_train_processed, X_test_processed\n",
        "\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 32)\n",
        "        self.fc2 = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class SklearnSimpleNN(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, input_dim, learning_rate=0.001, epochs=50, batch_size=64, pos_weight=1.0):\n",
        "        self.input_dim = input_dim\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.pos_weight = pos_weight\n",
        "        self.model = SimpleNN(self.input_dim)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(self.pos_weight, dtype=torch.float32))\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "        train_dataset = torch.utils.data.TensorDataset(torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32).unsqueeze(1))\n",
        "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            self.model.train()\n",
        "            for inputs, targets in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                outputs = self.model(inputs)\n",
        "                loss = criterion(outputs, targets.view(-1, 1))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(torch.tensor(X, dtype=torch.float32))\n",
        "            probabilities = torch.sigmoid(outputs)\n",
        "            predictions = (probabilities > 0.5).float()\n",
        "        return predictions.numpy().squeeze()\n",
        "\n",
        "# Train the Model\n",
        "def train_model(nn_estimator, X_train_tensor, y_train_tensor):\n",
        "    nn_estimator.fit(X_train_tensor.numpy(), y_train_tensor.numpy())\n",
        "    return nn_estimator\n",
        "\n",
        "# Evaluate the Model\n",
        "def evaluate_model(nn_estimator, X_test_tensor, y_test_tensor):\n",
        "    y_pred = nn_estimator.predict(X_test_tensor.numpy())\n",
        "    print(classification_report(y_test_tensor.numpy(), y_pred))\n"
      ],
      "metadata": {
        "id": "ObnD0m4dmgiM"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Testing Basic Data Preprocessing Functions"
      ],
      "metadata": {
        "id": "yE6lIWXxmkZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define dataset-specific parameters\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\"\n",
        "categorical_columns = ['sex', 'education', 'marriage']\n",
        "target = 'default_payment_next_month'\n",
        "\n",
        "# Load and preprocess data\n",
        "data = load_data_from_url(url)\n",
        "data = clean_column_names(data)\n",
        "data = remove_id_column(data)\n",
        "data = convert_categorical(data, categorical_columns=categorical_columns)\n",
        "X_train, X_test, y_train, y_test = split_data(data, target=target)\n",
        "preprocessor = define_preprocessor(X_train)\n",
        "X_train_processed, X_test_processed = preprocess_data(preprocessor, X_train, X_test)\n",
        "\n",
        "# Calculate class weights for imbalanced datasets\n",
        "class_weights = len(y_train) / (2 * np.bincount(y_train))\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train_processed, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)\n",
        "X_test_tensor = torch.tensor(X_test_processed, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "# Define the neural network estimator\n",
        "nn_estimator = SklearnSimpleNN(input_dim=X_train_tensor.shape[1], pos_weight=class_weights[1])\n",
        "\n",
        "# Train and evaluate the model\n",
        "nn_estimator = train_model(nn_estimator, X_train_tensor, y_train_tensor)\n",
        "evaluate_model(nn_estimator, X_test_tensor, y_test_tensor)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zF214ZqmnkK",
        "outputId": "297c67bc-3992-426f-fa38-88a8f253d4c3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.86      0.88      0.87      4673\n",
            "         1.0       0.55      0.51      0.53      1327\n",
            "\n",
            "    accuracy                           0.80      6000\n",
            "   macro avg       0.71      0.70      0.70      6000\n",
            "weighted avg       0.79      0.80      0.80      6000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Create the model_pipeline.py script"
      ],
      "metadata": {
        "id": "vYhMf9L3nFQf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the model_pipeline.py script with the complete content\n",
        "script_content = \"\"\"\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "import logging\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Load the dataset from a URL\n",
        "def load_data_from_url(url):\n",
        "    df = pd.read_excel(url, header=1)\n",
        "    return df\n",
        "\n",
        "# Clean column names\n",
        "def clean_column_names(df):\n",
        "    df.columns = [col.lower().replace(' ', '_') for col in df.columns]\n",
        "    return df\n",
        "\n",
        "# Remove the 'id' column\n",
        "def remove_id_column(df):\n",
        "    if 'id' in df.columns:\n",
        "        df = df.drop(columns=['id'])\n",
        "    return df\n",
        "\n",
        "# Convert specified columns to categorical type\n",
        "def convert_categorical(df, categorical_columns):\n",
        "    df[categorical_columns] = df[categorical_columns].astype('category')\n",
        "    return df\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "def split_data(df, target):\n",
        "    X = df.drop(columns=[target])\n",
        "    y = df[target]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "# Define the preprocessor\n",
        "def define_preprocessor(X_train):\n",
        "    numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "    categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "    numeric_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())])\n",
        "\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numeric_transformer, numeric_features),\n",
        "            ('cat', categorical_transformer, categorical_features)])\n",
        "\n",
        "    return preprocessor\n",
        "\n",
        "# Preprocess the data\n",
        "def preprocess_data(preprocessor, X_train, X_test):\n",
        "    X_train_processed = preprocessor.fit_transform(X_train)\n",
        "    X_test_processed = preprocessor.transform(X_test)\n",
        "    return X_train_processed, X_test_processed\n",
        "\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 32)\n",
        "        self.fc2 = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class SklearnSimpleNN(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, input_dim, learning_rate=0.001, epochs=50, batch_size=64, pos_weight=1.0):\n",
        "        self.input_dim = input_dim\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.pos_weight = pos_weight\n",
        "        self.model = SimpleNN(self.input_dim)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(self.pos_weight, dtype=torch.float32))\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "        train_dataset = torch.utils.data.TensorDataset(torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32).unsqueeze(1))\n",
        "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            self.model.train()\n",
        "            for inputs, targets in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                outputs = self.model(inputs)\n",
        "                loss = criterion(outputs, targets.view(-1, 1))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(torch.tensor(X, dtype=torch.float32))\n",
        "            probabilities = torch.sigmoid(outputs)\n",
        "            predictions = (probabilities > 0.5).float()\n",
        "        return predictions.numpy().squeeze()\n",
        "\n",
        "# Train the Model\n",
        "def train_model(nn_estimator, X_train_tensor, y_train_tensor):\n",
        "    nn_estimator.fit(X_train_tensor.numpy(), y_train_tensor.numpy())\n",
        "    return nn_estimator\n",
        "\n",
        "# Evaluate the Model\n",
        "def evaluate_model(nn_estimator, X_test_tensor, y_test_tensor):\n",
        "    y_pred = nn_estimator.predict(X_test_tensor.numpy())\n",
        "    print(classification_report(y_test_tensor.numpy(), y_pred))\n",
        "\"\"\"\n",
        "with open(\"model_pipeline.py\", \"w\") as file:\n",
        "    file.write(script_content)\n"
      ],
      "metadata": {
        "id": "CU5AfW6amnhm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Test the model_pipeline.py script"
      ],
      "metadata": {
        "id": "0M-nwmjZveHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from model_pipeline import load_data_from_url, clean_column_names, remove_id_column, convert_categorical, split_data, define_preprocessor, preprocess_data, SklearnSimpleNN, train_model, evaluate_model\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Define dataset-specific parameters\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\"\n",
        "categorical_columns = ['sex', 'education', 'marriage']\n",
        "target = 'default_payment_next_month'\n",
        "\n",
        "# Load and preprocess data\n",
        "data = load_data_from_url(url)\n",
        "data = clean_column_names(data)\n",
        "data = remove_id_column(data)\n",
        "data = convert_categorical(data, categorical_columns=categorical_columns)\n",
        "X_train, X_test, y_train, y_test = split_data(data, target=target)\n",
        "preprocessor = define_preprocessor(X_train)\n",
        "X_train_processed, X_test_processed = preprocess_data(preprocessor, X_train, X_test)\n",
        "\n",
        "# Calculate class weights for imbalanced datasets\n",
        "class_weights = len(y_train) / (2 * np.bincount(y_train))\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train_processed, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)\n",
        "X_test_tensor = torch.tensor(X_test_processed, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "# Define the neural network estimator\n",
        "nn_estimator = SklearnSimpleNN(input_dim=X_train_tensor.shape[1], pos_weight=class_weights[1])\n",
        "\n",
        "# Train and evaluate the model\n",
        "nn_estimator = train_model(nn_estimator, X_train_tensor, y_train_tensor)\n",
        "evaluate_model(nn_estimator, X_test_tensor, y_test_tensor)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jp7b2_xpmnZB",
        "outputId": "4dcc1bb6-577c-4a94-8a52-a526852e059a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.86      0.88      0.87      4673\n",
            "         1.0       0.54      0.52      0.53      1327\n",
            "\n",
            "    accuracy                           0.80      6000\n",
            "   macro avg       0.70      0.70      0.70      6000\n",
            "weighted avg       0.79      0.80      0.80      6000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Refactor the Remaining Pipeline Steps"
      ],
      "metadata": {
        "id": "Cudo2U-jxDmV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the model_pipeline.py script with the complete content\n",
        "script_content = \"\"\"\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "import logging\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Load the dataset from a URL\n",
        "def load_data_from_url(url):\n",
        "    df = pd.read_excel(url, header=1)\n",
        "    return df\n",
        "\n",
        "# Clean column names\n",
        "def clean_column_names(df):\n",
        "    df.columns = [col.lower().replace(' ', '_') for col in df.columns]\n",
        "    return df\n",
        "\n",
        "# Remove the 'id' column\n",
        "def remove_id_column(df):\n",
        "    if 'id' in df.columns:\n",
        "        df = df.drop(columns=['id'])\n",
        "    return df\n",
        "\n",
        "# Convert specified columns to categorical type\n",
        "def convert_categorical(df, categorical_columns):\n",
        "    df[categorical_columns] = df[categorical_columns].astype('category')\n",
        "    return df\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "def split_data(df, target):\n",
        "    X = df.drop(columns=[target])\n",
        "    y = df[target]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "# Define the preprocessor\n",
        "def define_preprocessor(X_train):\n",
        "    numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "    categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "    numeric_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())])\n",
        "\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numeric_transformer, numeric_features),\n",
        "            ('cat', categorical_transformer, categorical_features)])\n",
        "\n",
        "    return preprocessor\n",
        "\n",
        "# Preprocess the data\n",
        "def preprocess_data(preprocessor, X_train, X_test):\n",
        "    X_train_processed = preprocessor.fit_transform(X_train)\n",
        "    X_test_processed = preprocessor.transform(X_test)\n",
        "    return X_train_processed, X_test_processed\n",
        "\n",
        "# Calculate class weights for imbalanced datasets\n",
        "def calculate_class_weights(y_train):\n",
        "    return len(y_train) / (2 * np.bincount(y_train))\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "def convert_to_tensors(X_train_processed, y_train, X_test_processed, y_test):\n",
        "    X_train_tensor = torch.tensor(X_train_processed, dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)\n",
        "    X_test_tensor = torch.tensor(X_test_processed, dtype=torch.float32)\n",
        "    y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1)\n",
        "    return X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor\n",
        "\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 32)\n",
        "        self.fc2 = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class SklearnSimpleNN(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, input_dim, learning_rate=0.001, epochs=50, batch_size=64, pos_weight=1.0):\n",
        "        self.input_dim = input_dim\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.pos_weight = pos_weight\n",
        "        self.model = SimpleNN(self.input_dim)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(self.pos_weight, dtype=torch.float32))\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "        train_dataset = torch.utils.data.TensorDataset(torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32).unsqueeze(1))\n",
        "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            self.model.train()\n",
        "            for inputs, targets in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                outputs = self.model(inputs)\n",
        "                loss = criterion(outputs, targets.view(-1, 1))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(torch.tensor(X, dtype=torch.float32))\n",
        "            probabilities = torch.sigmoid(outputs)\n",
        "            predictions = (probabilities > 0.5).float()\n",
        "        return predictions.numpy().squeeze()\n",
        "\n",
        "# Train the Model\n",
        "def train_model(nn_estimator, X_train_tensor, y_train_tensor):\n",
        "    nn_estimator.fit(X_train_tensor.numpy(), y_train_tensor.numpy())\n",
        "    return nn_estimator\n",
        "\n",
        "# Evaluate the Model\n",
        "def evaluate_model(nn_estimator, X_test_tensor, y_test_tensor):\n",
        "    y_pred = nn_estimator.predict(X_test_tensor.numpy())\n",
        "    print(classification_report(y_test_tensor.numpy(), y_pred))\n",
        "\n",
        "# Function to run the full pipeline\n",
        "def run_full_pipeline(url, categorical_columns, target):\n",
        "    data = load_data_from_url(url)\n",
        "    data = clean_column_names(data)\n",
        "    data = remove_id_column(data)\n",
        "    data = convert_categorical(data, categorical_columns=categorical_columns)\n",
        "    X_train, X_test, y_train, y_test = split_data(data, target=target)\n",
        "    preprocessor = define_preprocessor(X_train)\n",
        "    X_train_processed, X_test_processed = preprocess_data(preprocessor, X_train, X_test)\n",
        "\n",
        "    class_weights = calculate_class_weights(y_train)\n",
        "\n",
        "    X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor = convert_to_tensors(\n",
        "        X_train_processed, y_train, X_test_processed, y_test)\n",
        "\n",
        "    nn_estimator = SklearnSimpleNN(input_dim=X_train_tensor.shape[1], pos_weight=class_weights[1])\n",
        "    nn_estimator = train_model(nn_estimator, X_train_tensor, y_train_tensor)\n",
        "    evaluate_model(nn_estimator, X_test_tensor, y_test_tensor)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser(description=\"Run full pipeline\")\n",
        "    parser.add_argument(\"url\", type=str, help=\"URL of the dataset\")\n",
        "    parser.add_argument(\"categorical_columns\", type=str, nargs=\"+\", help=\"List of categorical columns\")\n",
        "    parser.add_argument(\"target\", type=str, help=\"Target column\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    run_full_pipeline(args.url, args.categorical_columns, args.target)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "with open(\"model_pipeline.py\", \"w\") as file:\n",
        "    file.write(script_content)\n"
      ],
      "metadata": {
        "id": "pCTO3MfHyyxg"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from model_pipeline import run_full_pipeline\n",
        "\n",
        "# Define dataset-specific parameters\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\"\n",
        "categorical_columns = ['sex', 'education', 'marriage']\n",
        "target = 'default_payment_next_month'\n",
        "\n",
        "# Execute the pipeline\n",
        "run_full_pipeline(url, categorical_columns, target)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HASUyR641IKB",
        "outputId": "da78c193-d8fd-4cc8-d915-52cd295b348e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.87      0.87      0.87      4673\n",
            "         1.0       0.53      0.53      0.53      1327\n",
            "\n",
            "    accuracy                           0.79      6000\n",
            "   macro avg       0.70      0.70      0.70      6000\n",
            "weighted avg       0.79      0.79      0.79      6000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bWfug1Bo1IHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uTTeAfh21IEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jbmDy0RL1H6-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}