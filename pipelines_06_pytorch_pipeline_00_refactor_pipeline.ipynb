{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJeAH6iGpmaQ8QTcZUtFy3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/pipelines/blob/main/pipelines_06_pytorch_pipeline_00_refactor_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import classification_report\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Load the dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\"\n",
        "df = pd.read_excel(url, header=1)\n",
        "\n",
        "# Rename columns to lower case and replace spaces with underscores\n",
        "df.columns = [col.lower().replace(' ', '_') for col in df.columns]\n",
        "\n",
        "# Convert specific numeric columns to categorical\n",
        "categorical_columns = ['sex', 'education', 'marriage']\n",
        "df[categorical_columns] = df[categorical_columns].astype('category')\n",
        "\n",
        "# Select features and target\n",
        "target = 'default_payment_next_month'\n",
        "X = df.drop(columns=[target])\n",
        "y = df[target]\n",
        "\n",
        "# Perform stratified train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Identify column types\n",
        "numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "# Define preprocessing for numeric columns\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())])\n",
        "\n",
        "# Define preprocessing for categorical columns\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "# Combine preprocessing steps\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)])\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "def set_seed(seed):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# Custom preprocessor to ensure output is a DataFrame\n",
        "class DataFramePreprocessor(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, preprocessor):\n",
        "        self.preprocessor = preprocessor\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        self.preprocessor.fit(X, y)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_transformed = self.preprocessor.transform(X)\n",
        "        return pd.DataFrame(X_transformed)\n",
        "\n",
        "# Optimal number of principal components determined from PCA iteration\n",
        "optimal_n_components = 17\n",
        "\n",
        "# Create a new pipeline with preprocessing and PCA\n",
        "pca_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', DataFramePreprocessor(preprocessor)),\n",
        "    ('pca', PCA(n_components=optimal_n_components))\n",
        "])\n",
        "\n",
        "# Fit and transform the training data\n",
        "X_train_pca = pca_pipeline.fit_transform(X_train, y_train)\n",
        "X_test_pca = pca_pipeline.transform(X_test)\n",
        "\n",
        "# Apply RandomUnderSampler to balance the training dataset\n",
        "undersampler = RandomUnderSampler(sampling_strategy=0.75, random_state=42)\n",
        "X_train_resampled, y_train_resampled = undersampler.fit_resample(X_train_pca, y_train)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train_resampled, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train_resampled.to_numpy(), dtype=torch.float32).unsqueeze(1)\n",
        "X_test_tensor = torch.tensor(X_test_pca, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test.to_numpy(), dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "optimal_threshold = 0.8000141978263855\n",
        "\n",
        "# Define a simple neural network model\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 32)\n",
        "        self.fc2 = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Define the sklearn wrapper for the neural network model\n",
        "class SklearnSimpleNN(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, input_dim, learning_rate=0.001, epochs=50, batch_size=64, pos_weight=1.0):\n",
        "        self.input_dim = input_dim\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.pos_weight = pos_weight  # Accept as float\n",
        "        self.model = SimpleNN(self.input_dim)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Convert pos_weight to tensor here\n",
        "        criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(self.pos_weight, dtype=torch.float32))\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "        train_dataset = torch.utils.data.TensorDataset(torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32).unsqueeze(1))\n",
        "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            self.model.train()\n",
        "            for inputs, targets in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                outputs = self.model(inputs)\n",
        "                loss = criterion(outputs, targets.view(-1, 1))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(torch.tensor(X, dtype=torch.float32))\n",
        "            probabilities = torch.sigmoid(outputs)\n",
        "            predictions = (probabilities > optimal_threshold).float()  # Use the manually adjusted threshold here\n",
        "        return predictions.numpy().squeeze()\n",
        "\n",
        "# Calculate the class weights\n",
        "class_weights = len(y_train) / (2 * np.bincount(y_train))\n",
        "\n",
        "# Create an instance of SklearnSimpleNN with the adjusted weight\n",
        "input_dim = X_train_tensor.shape[1]\n",
        "nn_estimator = SklearnSimpleNN(input_dim=input_dim, pos_weight=class_weights[1])\n",
        "\n",
        "# Fit the model\n",
        "nn_estimator.fit(X_train_tensor.numpy(), y_train_tensor.numpy())\n",
        "\n",
        "# Predict on the test set with the optimal threshold\n",
        "y_pred_pca = nn_estimator.predict(X_test_tensor.numpy())\n",
        "\n",
        "# Evaluate the model with the optimal threshold\n",
        "print(classification_report(y_test_tensor.numpy(), y_pred_pca))\n"
      ],
      "metadata": {
        "id": "ah-kVDA-eeEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Starting with small, manageable pieces and gradually building up your pipeline is a smart approach. This way, you can ensure that each part works correctly before moving on to the next step. We'll begin with the data preprocessing part and then incrementally add more functionality.\n",
        "\n",
        "### Step 1: Create Basic Data Preprocessing Functions\n",
        "\n",
        "We'll start by defining the basic data preprocessing functions in `model_pipeline.py` and testing them in the notebook.\n",
        "\n",
        "#### 1.1 Define Basic Functions in `model_pipeline.py`\n",
        "\n",
        "First, create the basic functions for loading data, cleaning columns, and preprocessing data.\n"
      ],
      "metadata": {
        "id": "59zwJnvlgCfp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "import logging\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Load the dataset from a URL\n",
        "def load_data_from_url(url):\n",
        "    df = pd.read_excel(url, header=1)\n",
        "    return df\n",
        "\n",
        "# Clean column names\n",
        "def clean_column_names(df):\n",
        "    df.columns = [col.lower().replace(' ', '_') for col in df.columns]\n",
        "    return df\n",
        "\n",
        "# Remove the 'id' column\n",
        "def remove_id_column(df):\n",
        "    if 'id' in df.columns:\n",
        "        df = df.drop(columns=['id'])\n",
        "    return df\n",
        "\n",
        "# Convert specified columns to categorical type\n",
        "def convert_categorical(df, categorical_columns):\n",
        "    df[categorical_columns] = df[categorical_columns].astype('category')\n",
        "    return df\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "def split_data(df, target):\n",
        "    X = df.drop(columns=[target])\n",
        "    y = df[target]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "# Define the preprocessor\n",
        "def define_preprocessor(X_train):\n",
        "    numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "    categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "    numeric_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())])\n",
        "\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numeric_transformer, numeric_features),\n",
        "            ('cat', categorical_transformer, categorical_features)])\n",
        "\n",
        "    return preprocessor\n",
        "\n",
        "# Preprocess the data\n",
        "def preprocess_data(preprocessor, X_train, X_test):\n",
        "    X_train_processed = preprocessor.fit_transform(X_train)\n",
        "    X_test_processed = preprocessor.transform(X_test)\n",
        "    return X_train_processed, X_test_processed\n",
        "\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 32)\n",
        "        self.fc2 = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class SklearnSimpleNN(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, input_dim, learning_rate=0.001, epochs=50, batch_size=64, pos_weight=1.0):\n",
        "        self.input_dim = input_dim\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.pos_weight = pos_weight\n",
        "        self.model = SimpleNN(self.input_dim)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(self.pos_weight, dtype=torch.float32))\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "        train_dataset = torch.utils.data.TensorDataset(torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32).unsqueeze(1))\n",
        "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            self.model.train()\n",
        "            for inputs, targets in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                outputs = self.model(inputs)\n",
        "                loss = criterion(outputs, targets.view(-1, 1))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(torch.tensor(X, dtype=torch.float32))\n",
        "            probabilities = torch.sigmoid(outputs)\n",
        "            predictions = (probabilities > 0.5).float()\n",
        "        return predictions.numpy().squeeze()\n",
        "\n",
        "# Train the Model\n",
        "def train_model(nn_estimator, X_train_tensor, y_train_tensor):\n",
        "    nn_estimator.fit(X_train_tensor.numpy(), y_train_tensor.numpy())\n",
        "    return nn_estimator\n",
        "\n",
        "# Evaluate the Model\n",
        "def evaluate_model(nn_estimator, X_test_tensor, y_test_tensor):\n",
        "    y_pred = nn_estimator.predict(X_test_tensor.numpy())\n",
        "    print(classification_report(y_test_tensor.numpy(), y_pred))\n"
      ],
      "metadata": {
        "id": "ObnD0m4dmgiM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the Pipeline"
      ],
      "metadata": {
        "id": "yE6lIWXxmkZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from model_pipeline import load_data_from_url, clean_column_names, remove_id_column, convert_categorical, split_data, define_preprocessor, preprocess_data, SklearnSimpleNN, train_model, evaluate_model\n",
        "# import torch\n",
        "# import numpy as np\n",
        "\n",
        "# Define dataset-specific parameters\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\"\n",
        "categorical_columns = ['sex', 'education', 'marriage']\n",
        "target = 'default_payment_next_month'\n",
        "\n",
        "# Load and preprocess data\n",
        "data = load_data_from_url(url)\n",
        "data = clean_column_names(data)\n",
        "data = remove_id_column(data)\n",
        "data = convert_categorical(data, categorical_columns=categorical_columns)\n",
        "X_train, X_test, y_train, y_test = split_data(data, target=target)\n",
        "preprocessor = define_preprocessor(X_train)\n",
        "X_train_processed, X_test_processed = preprocess_data(preprocessor, X_train, X_test)\n",
        "\n",
        "# Calculate class weights for imbalanced datasets\n",
        "class_weights = len(y_train) / (2 * np.bincount(y_train))\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train_processed, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)\n",
        "X_test_tensor = torch.tensor(X_test_processed, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "# Define the neural network estimator\n",
        "nn_estimator = SklearnSimpleNN(input_dim=X_train_tensor.shape[1], pos_weight=class_weights[1])\n",
        "\n",
        "# Train and evaluate the model\n",
        "nn_estimator = train_model(nn_estimator, X_train_tensor, y_train_tensor)\n",
        "evaluate_model(nn_estimator, X_test_tensor, y_test_tensor)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zF214ZqmnkK",
        "outputId": "c6187b1e-0239-4b2b-de40-d1eea423fbfa"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.86      0.88      0.87      4673\n",
            "         1.0       0.54      0.50      0.52      1327\n",
            "\n",
            "    accuracy                           0.80      6000\n",
            "   macro avg       0.70      0.69      0.70      6000\n",
            "weighted avg       0.79      0.80      0.79      6000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the model_pipeline.py script\n",
        "\n",
        "The code runs to completion so now we can create our script."
      ],
      "metadata": {
        "id": "vYhMf9L3nFQf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the model_pipeline.py script\n",
        "script_content = \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "with open(\"model_pipeline.py\", \"a\") as file:\n",
        "    file.write(script_content)"
      ],
      "metadata": {
        "id": "CU5AfW6amnhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jp7b2_xpmnZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the model_pipeline.py script\n",
        "script_content = \"\"\"\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "import logging\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Load the dataset from a URL\n",
        "def load_data_from_url(url):\n",
        "    df = pd.read_excel(url, header=1)\n",
        "    return df\n",
        "\n",
        "# Clean column names\n",
        "def clean_column_names(df):\n",
        "    df.columns = [col.lower().replace(' ', '_') for col in df.columns]\n",
        "    return df\n",
        "\n",
        "# Remove the 'id' column\n",
        "def remove_id_column(df):\n",
        "    if 'id' in df.columns:\n",
        "        df = df.drop(columns=['id'])\n",
        "    return df\n",
        "\n",
        "# Convert specified columns to categorical type\n",
        "def convert_categorical(df, categorical_columns):\n",
        "    df[categorical_columns] = df[categorical_columns].astype('category')\n",
        "    return df\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "def split_data(df, target):\n",
        "    X = df.drop(columns=[target])\n",
        "    y = df[target]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "# Define the preprocessor\n",
        "def define_preprocessor(X_train):\n",
        "    numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "    categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "    numeric_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())])\n",
        "\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numeric_transformer, numeric_features),\n",
        "            ('cat', categorical_transformer, categorical_features)])\n",
        "\n",
        "    return preprocessor\n",
        "\n",
        "# Preprocess the data\n",
        "def preprocess_data(preprocessor, X_train, X_test):\n",
        "    X_train_processed = preprocessor.fit_transform(X_train)\n",
        "    X_test_processed = preprocessor.transform(X_test)\n",
        "    return X_train_processed, X_test_processed\n",
        "\"\"\"\n",
        "with open(\"model_pipeline.py\", \"w\") as file:\n",
        "    file.write(script_content)\n"
      ],
      "metadata": {
        "id": "hm3afRh-i3oJ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2 Test Basic Functions in Notebook\n",
        "\n",
        "Next, import and test these functions in the notebook to ensure they work correctly."
      ],
      "metadata": {
        "id": "01lFSkqQgz_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the basic functions from the script\n",
        "from model_pipeline import load_data_from_url, clean_column_names, remove_id_column, convert_categorical, split_data, define_preprocessor, preprocess_data\n",
        "\n",
        "# Define dataset-specific parameters\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\"\n",
        "categorical_columns = ['sex', 'education', 'marriage']\n",
        "target = 'default_payment_next_month'\n",
        "\n",
        "# Load and preprocess data\n",
        "data = load_data_from_url(url)\n",
        "data = clean_column_names(data)\n",
        "data = remove_id_column(data)\n",
        "data = convert_categorical(data, categorical_columns=categorical_columns)\n",
        "X_train, X_test, y_train, y_test = split_data(data, target=target)\n",
        "preprocessor = define_preprocessor(X_train)\n",
        "X_train_processed, X_test_processed = preprocess_data(preprocessor, X_train, X_test)\n",
        "\n",
        "print(f\"X_train shape: {X_train_processed.shape}\")\n",
        "print(f\"X_test shape: {X_test_processed.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sp26RKH9gxkZ",
        "outputId": "fc9ac1dd-4e9f-42e3-c4ba-1bd4361199e5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (24000, 33)\n",
            "X_test shape: (6000, 33)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Add Model Training and Evaluation Functions"
      ],
      "metadata": {
        "id": "LwS6RmpLgeY-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.1 Define Model Training and Evaluation Functions in `model_pipeline.py`\n",
        "\n"
      ],
      "metadata": {
        "id": "qsMdH1kjgWfF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Append new functions to model_pipeline.py\n",
        "\n",
        "script_content = \"\"\"\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 32)\n",
        "        self.fc2 = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class SklearnSimpleNN(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, input_dim, learning_rate=0.001, epochs=50, batch_size=64, pos_weight=1.0):\n",
        "        self.input_dim = input_dim\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.pos_weight = pos_weight\n",
        "        self.model = SimpleNN(self.input_dim)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(self.pos_weight, dtype=torch.float32))\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "        train_dataset = torch.utils.data.TensorDataset(torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32).unsqueeze(1))\n",
        "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            self.model.train()\n",
        "            for inputs, targets in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                outputs = self.model(inputs)\n",
        "                loss = criterion(outputs, targets.view(-1, 1))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(torch.tensor(X, dtype=torch.float32))\n",
        "            probabilities = torch.sigmoid(outputs)\n",
        "            predictions = (probabilities > 0.5).float()\n",
        "        return predictions.numpy().squeeze()\n",
        "\n",
        "# Train the Model\n",
        "def train_model(nn_estimator, X_train_tensor, y_train_tensor):\n",
        "    nn_estimator.fit(X_train_tensor.numpy(), y_train_tensor.numpy())\n",
        "    return nn_estimator\n",
        "\n",
        "# Evaluate the Model\n",
        "def evaluate_model(nn_estimator, X_test_tensor, y_test_tensor):\n",
        "    y_pred = nn_estimator.predict(X_test_tensor.numpy())\n",
        "    print(classification_report(y_test_tensor.numpy(), y_pred))\n",
        "\"\"\"\n",
        "with open(\"model_pipeline.py\", \"a\") as file:\n",
        "    file.write(script_content)"
      ],
      "metadata": {
        "id": "s0oY1wh2gaFh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.2 Test Model Training and Evaluation Functions in Notebook"
      ],
      "metadata": {
        "id": "R1EZtZ39gOEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the model training and evaluation functions from the script\n",
        "from model_pipeline import train_model, evaluate_model, SklearnSimpleNN\n",
        "\n",
        "# Define the neural network estimator\n",
        "nn_estimator = SklearnSimpleNN(input_dim=X_train_tensor.shape[1], pos_weight=class_weights[1])\n",
        "\n",
        "# Train and evaluate the model\n",
        "nn_estimator = train_model(nn_estimator, X_train_tensor, y_train_tensor)\n",
        "evaluate_model(nn_estimator, X_test_tensor, y_test_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "WZP776owgKf5",
        "outputId": "f276b822-bd5b-4c8e-fef3-b0f939c69811"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'train_model' from 'model_pipeline' (/content/model_pipeline.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-cc1feff12d56>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Import the model training and evaluation functions from the script\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel_pipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSklearnSimpleNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Define the neural network estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnn_estimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSklearnSimpleNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'train_model' from 'model_pipeline' (/content/model_pipeline.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}