{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMzWzqEBv3HNDmVcSxk4HrF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/pipelines/blob/main/pipelines_06_pytorch_pipeline_02_data_prep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preparation Notebook Summary\n",
        "\n",
        "#### Purpose:\n",
        "The Data Preparation Notebook is dedicated to the initial steps of data handling in the machine learning workflow. It focuses on loading, cleaning, preprocessing, and splitting the dataset, ensuring that the data is ready for subsequent analysis and modeling in separate, specialized notebooks.\n",
        "\n",
        "#### What It Does:\n",
        "1. **Load the Dataset**: Reads the data from the specified source (e.g., an Excel file from a URL).\n",
        "2. **Rename Columns**: Standardizes column names by converting them to lowercase and replacing spaces with underscores for consistency.\n",
        "3. **Convert Data Types**: Converts specific columns to categorical types, which are more suitable for certain preprocessing steps.\n",
        "4. **Select Features and Target**: Identifies the target variable and selects the relevant features for analysis.\n",
        "5. **Train-Test Split**: Performs a stratified split of the dataset into training and testing sets to ensure that the class distribution is maintained in both subsets.\n",
        "6. **Preprocessing Pipelines**:\n",
        "   - **Numeric Features**: Imputes missing values and scales the features.\n",
        "   - **Categorical Features**: Imputes missing values and applies one-hot encoding.\n",
        "   - **Column Transformer**: Combines the preprocessing steps for numeric and categorical features.\n",
        "7. **Transform the Data**: Applies the preprocessing pipeline to the training and testing data.\n",
        "8. **Save Preprocessed Data**: Saves the transformed data to files for use in subsequent notebooks.\n",
        "\n",
        "#### Why a Modular Approach is Preferable:\n",
        "1. **Improved Readability**: Smaller notebooks focused on specific tasks are easier to read and understand. This clarity is especially beneficial for team members or collaborators who need to quickly grasp the purpose and functionality of the code.\n",
        "2. **Ease of Maintenance**: Modular notebooks simplify the process of updating, debugging, and maintaining code. Changes in one part of the workflow can be managed independently without affecting the entire project.\n",
        "3. **Enhanced Reusability**: Individual notebooks for data preparation, feature selection, and model training can be reused in different projects. This modularity saves time and effort when working on similar tasks in the future.\n",
        "4. **Collaboration**: A modular approach facilitates collaborative work by allowing team members to work on different aspects of the project simultaneously. Each member can focus on a specific notebook without interference.\n",
        "5. **Focused Analysis**: Each notebook serves a distinct purpose, allowing for a more focused and in-depth analysis of each step in the machine learning pipeline. This specialization leads to better-organized and more thorough documentation and analysis.\n",
        "6. **Scalability**: As the project grows, adding new methods or analyses becomes more manageable. New notebooks can be created for additional tasks without overcomplicating the existing workflow.\n",
        "7. **Professionalism**: Adopting a modular approach aligns with best practices in software development and data science. It demonstrates a methodical and organized approach to project management, enhancing the overall quality and professionalism of the work.\n",
        "\n",
        "By adopting a modular approach, the Data Preparation Notebook lays a solid foundation for a streamlined and efficient machine learning workflow, enabling more focused and effective subsequent analysis in specialized notebooks."
      ],
      "metadata": {
        "id": "zbhMmzF9B0cH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mm-9vJM9A-VB",
        "outputId": "80128de6-c826-4fc9-db6e-46f50fdaf725"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data preparation complete and saved.\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "# Load the dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\"\n",
        "df = pd.read_excel(url, header=1)\n",
        "\n",
        "# Rename columns to lower case and replace spaces with underscores\n",
        "df.columns = [col.lower().replace(' ', '_') for col in df.columns]\n",
        "\n",
        "# Convert specific numeric columns to categorical\n",
        "categorical_columns = ['sex', 'education', 'marriage']\n",
        "df[categorical_columns] = df[categorical_columns].astype('category')\n",
        "\n",
        "# Select features and target\n",
        "target = 'default_payment_next_month'\n",
        "X = df.drop(columns=[target])\n",
        "y = df[target]\n",
        "\n",
        "# Perform stratified train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Identify column types\n",
        "numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "# Define preprocessing for numeric columns\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())])\n",
        "\n",
        "# Define preprocessing for categorical columns\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "# Combine preprocessing steps\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)])\n",
        "\n",
        "# Fit and transform the data\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "# Save preprocessed data\n",
        "np.savez('preprocessed_data.npz', X_train_processed=X_train_processed, X_test_processed=X_test_processed, y_train=y_train, y_test=y_test)\n",
        "\n",
        "print(\"Data preparation complete and saved.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modular Code"
      ],
      "metadata": {
        "id": "ty18ZhB5kV_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "def load_data(url):\n",
        "    df = pd.read_excel(url, header=1)\n",
        "    df.columns = [col.lower().replace(' ', '_') for col in df.columns]\n",
        "    return df\n",
        "\n",
        "def convert_categorical(df, categorical_columns):\n",
        "    df[categorical_columns] = df[categorical_columns].astype('category')\n",
        "    return df\n",
        "\n",
        "def split_data(df, target):\n",
        "    X = df.drop(columns=[target])\n",
        "    y = df[target]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "def define_preprocessor(X_train):\n",
        "    numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "    categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "    numeric_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())])\n",
        "\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numeric_transformer, numeric_features),\n",
        "            ('cat', categorical_transformer, categorical_features)])\n",
        "\n",
        "    return preprocessor\n",
        "\n",
        "def preprocess_data(preprocessor, X_train, X_test):\n",
        "    X_train_processed = preprocessor.fit_transform(X_train)\n",
        "    X_test_processed = preprocessor.transform(X_test)\n",
        "    return X_train_processed, X_test_processed\n",
        "\n",
        "def save_data(X_train_processed, X_test_processed, y_train, y_test, filename='preprocessed_data.npz'):\n",
        "    np.savez(filename, X_train_processed=X_train_processed, X_test_processed=X_test_processed, y_train=y_train, y_test=y_test)\n",
        "    print(\"Data preparation complete and saved.\")\n"
      ],
      "metadata": {
        "id": "TPw3xkVPB3zg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from data_prep import load_data, convert_categorical, split_data, define_preprocessor, preprocess_data, save_data\n",
        "\n",
        "# Load the dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\"\n",
        "df = load_data(url)\n",
        "\n",
        "# Convert specific numeric columns to categorical\n",
        "categorical_columns = ['sex', 'education', 'marriage']\n",
        "df = convert_categorical(df, categorical_columns)\n",
        "\n",
        "# Select features and target\n",
        "target = 'default_payment_next_month'\n",
        "X_train, X_test, y_train, y_test = split_data(df, target)\n",
        "\n",
        "# Define preprocessor\n",
        "preprocessor = define_preprocessor(X_train)\n",
        "\n",
        "# Fit and transform the data\n",
        "X_train_processed, X_test_processed = preprocess_data(preprocessor, X_train, X_test)\n",
        "\n",
        "# Save preprocessed data\n",
        "save_data(X_train_processed, X_test_processed, y_train, y_test)\n",
        "\n",
        "print(\"Data preparation complete and saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6VecXYekqwl",
        "outputId": "ee60f801-a7ab-4ab0-c8a6-b33fffa182af"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data preparation complete and saved.\n",
            "Data preparation complete and saved.\n"
          ]
        }
      ]
    }
  ]
}