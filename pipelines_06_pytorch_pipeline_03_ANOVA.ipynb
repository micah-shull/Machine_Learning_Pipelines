{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNTCeggkSelAAy7op9ADQHf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/pipelines/blob/main/pipelines_06_pytorch_pipeline_03_ANOVA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modular Preprocessig Code"
      ],
      "metadata": {
        "id": "yVdReD6J45IQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin, TransformerMixin\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "import matplotlib.pyplot as plt\n",
        "import logging\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "def set_seed(seed):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def load_data(url=\"https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\"):\n",
        "    df = pd.read_excel(url, header=1)\n",
        "    df.columns = [col.lower().replace(' ', '_') for col in df.columns]\n",
        "    return df\n",
        "\n",
        "def convert_categorical(df, categorical_columns):\n",
        "    df[categorical_columns] = df[categorical_columns].astype('category')\n",
        "    return df\n",
        "\n",
        "def split_data(df, target):\n",
        "    X = df.drop(columns=[target])\n",
        "    y = df[target]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "def define_preprocessor(X_train):\n",
        "    numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "    categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "    numeric_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())])\n",
        "\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numeric_transformer, numeric_features),\n",
        "            ('cat', categorical_transformer, categorical_features)])\n",
        "\n",
        "    return preprocessor\n",
        "\n",
        "def preprocess_data(preprocessor, X_train, X_test):\n",
        "    X_train_processed = preprocessor.fit_transform(X_train)\n",
        "    X_test_processed = preprocessor.transform(X_test)\n",
        "    return X_train_processed, X_test_processed\n",
        "\n",
        "def calculate_class_weights(y_train):\n",
        "    return len(y_train) / (2 * np.bincount(y_train))\n",
        "\n",
        "def save_data(X_train_processed, X_test_processed, y_train, y_test, filename='preprocessed_data.npz'):\n",
        "    np.savez(filename, X_train_processed=X_train_processed, X_test_processed=X_test_processed, y_train=y_train, y_test=y_test)\n",
        "    print(\"Data preparation complete and saved.\")\n"
      ],
      "metadata": {
        "id": "1QY8k4YV54nT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANOVA Feature Selection Script"
      ],
      "metadata": {
        "id": "y8uNW4rm6X6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.pipeline import Pipeline as SklearnPipeline\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "import matplotlib.pyplot as plt\n",
        "import logging\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 32)\n",
        "        self.fc2 = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class SklearnSimpleNN(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, input_dim, learning_rate=0.001, epochs=50, batch_size=64, pos_weight=1.0):\n",
        "        self.input_dim = input_dim\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.pos_weight = pos_weight\n",
        "        self.model = SimpleNN(self.input_dim)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(self.pos_weight, dtype=torch.float32))\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "        train_dataset = torch.utils.data.TensorDataset(torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32).unsqueeze(1))\n",
        "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            self.model.train()\n",
        "            for inputs, targets in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                outputs = self.model(inputs)\n",
        "                loss = criterion(outputs, targets.view(-1, 1))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(torch.tensor(X, dtype=torch.float32))\n",
        "            probabilities = torch.sigmoid(outputs)\n",
        "            predictions = (probabilities > 0.5).float()\n",
        "        return predictions.numpy().squeeze()\n",
        "\n",
        "def evaluate_anova(X_train, y_train, X_test, y_test, preprocessor, class_weights, k_values):\n",
        "    f1_scores = []\n",
        "\n",
        "    for k in k_values:\n",
        "        logging.info(f\"Evaluating ANOVA with {k} features\")\n",
        "\n",
        "        anova_pipeline = SklearnPipeline(steps=[\n",
        "            ('preprocessor', preprocessor),\n",
        "            ('anova', SelectKBest(score_func=f_classif, k=k))\n",
        "        ])\n",
        "\n",
        "        X_train_anova = anova_pipeline.fit_transform(X_train, y_train)\n",
        "        X_test_anova = anova_pipeline.transform(X_test)\n",
        "\n",
        "        undersampler = RandomUnderSampler(sampling_strategy=0.75, random_state=42)\n",
        "        X_train_resampled, y_train_resampled = undersampler.fit_resample(X_train_anova, y_train)\n",
        "\n",
        "        X_train_tensor = torch.tensor(X_train_resampled, dtype=torch.float32)\n",
        "        y_train_tensor = torch.tensor(y_train_resampled, dtype=torch.float32).unsqueeze(1)\n",
        "        X_test_tensor = torch.tensor(X_test_anova, dtype=torch.float32)\n",
        "        y_test_tensor = torch.tensor(y_test.to_numpy(), dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "        nn_estimator = SklearnSimpleNN(input_dim=X_train_tensor.shape[1], pos_weight=class_weights[1])\n",
        "        nn_estimator.fit(X_train_tensor.numpy(), y_train_tensor.numpy())\n",
        "\n",
        "        y_pred = nn_estimator.predict(X_test_tensor.numpy())\n",
        "        f1 = f1_score(y_test_tensor.numpy(), y_pred)\n",
        "        f1_scores.append(f1)\n",
        "        logging.info(f\"Number of features: {k}, F1-score: {f1}\")\n",
        "\n",
        "    return k_values, f1_scores\n",
        "\n",
        "def plot_f1_scores(k_values, f1_scores):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(k_values, f1_scores, marker='o')\n",
        "    plt.title('F1-Score vs. Number of Features (ANOVA)')\n",
        "    plt.xlabel('Number of Features')\n",
        "    plt.ylabel('F1-Score')\n",
        "    plt.xticks(k_values)\n",
        "    plt.grid()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "u_Eyd1y06aWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from ml_utils import set_seed, load_data, preprocess_data, define_preprocessor, calculate_class_weights\n",
        "from anova_feature_selection import evaluate_anova, plot_f1_scores\n",
        "\n",
        "def main():\n",
        "    set_seed(42)\n",
        "    data = load_data()\n",
        "    data = convert_categorical(data, categorical_columns=['sex', 'education', 'marriage'])\n",
        "    X_train, X_test, y_train, y_test = preprocess_data(data)\n",
        "    preprocessor = define_preprocessor(X_train)\n",
        "    class_weights = calculate_class_weights(y_train)\n",
        "\n",
        "    # Define range of k values\n",
        "    k_values = range(1, X_train.shape[1] + 1)\n",
        "\n",
        "    # Evaluate ANOVA feature selection\n",
        "    k_values, f1_scores = evaluate_anova(X_train, y_train, X_test, y_test, preprocessor, class_weights, k_values)\n",
        "\n",
        "    # Plot F1-scores\n",
        "    plot_f1_scores(k_values, f1_scores)\n",
        "\n",
        "    # Find the optimal number of features\n",
        "    optimal_k = k_values[np.argmax(f1_scores)]\n",
        "    print(f\"Optimal number of features: {optimal_k}\")\n",
        "\n",
        "    # Save the optimal number of features\n",
        "    with open('optimal_anova_features.txt', 'w') as f:\n",
        "        f.write(str(optimal_k))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "EGPwXzug6b1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FzQwoJV46by2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BsDiFz7i6bv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2oCGUWa06bs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UcBSzKge6bpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aO9dUXw54Nmi"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "# from sklearn.pipeline import Pipeline as SklearnPipeline\n",
        "# from sklearn.feature_selection import SelectKBest, f_classif\n",
        "# from sklearn.metrics import f1_score, classification_report\n",
        "# from imblearn.under_sampling import RandomUnderSampler\n",
        "# import matplotlib.pyplot as plt\n",
        "# import logging\n",
        "\n",
        "# # Set up logging\n",
        "# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# def set_seed(seed):\n",
        "#     np.random.seed(seed)\n",
        "#     torch.manual_seed(seed)\n",
        "#     if torch.cuda.is_available():\n",
        "#         torch.cuda.manual_seed(seed)\n",
        "#         torch.cuda.manual_seed_all(seed)\n",
        "#     torch.backends.cudnn.deterministic = True\n",
        "#     torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# class SimpleNN(nn.Module):\n",
        "#     def __init__(self, input_dim):\n",
        "#         super(SimpleNN, self).__init__()\n",
        "#         self.fc1 = nn.Linear(input_dim, 32)\n",
        "#         self.fc2 = nn.Linear(32, 1)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = torch.relu(self.fc1(x))\n",
        "#         x = self.fc2(x)\n",
        "#         return x\n",
        "\n",
        "# class SklearnSimpleNN(BaseEstimator, ClassifierMixin):\n",
        "#     def __init__(self, input_dim, learning_rate=0.001, epochs=50, batch_size=64, pos_weight=1.0):\n",
        "#         self.input_dim = input_dim\n",
        "#         self.learning_rate = learning_rate\n",
        "#         self.epochs = epochs\n",
        "#         self.batch_size = batch_size\n",
        "#         self.pos_weight = pos_weight\n",
        "#         self.model = SimpleNN(self.input_dim)\n",
        "\n",
        "#     def fit(self, X, y):\n",
        "#         criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(self.pos_weight, dtype=torch.float32))\n",
        "#         optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "#         train_dataset = torch.utils.data.TensorDataset(torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32).unsqueeze(1))\n",
        "#         train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "#         for epoch in range(self.epochs):\n",
        "#             self.model.train()\n",
        "#             for inputs, targets in train_loader:\n",
        "#                 optimizer.zero_grad()\n",
        "#                 outputs = self.model(inputs)\n",
        "#                 loss = criterion(outputs, targets.view(-1, 1))\n",
        "#                 loss.backward()\n",
        "#                 optimizer.step()\n",
        "#         return self\n",
        "\n",
        "#     def predict(self, X):\n",
        "#         self.model.eval()\n",
        "#         with torch.no_grad():\n",
        "#             outputs = self.model(torch.tensor(X, dtype=torch.float32))\n",
        "#             probabilities = torch.sigmoid(outputs)\n",
        "#             predictions = (probabilities > 0.5).float()\n",
        "#         return predictions.numpy().squeeze()\n",
        "\n",
        "# def calculate_class_weights(y_train):\n",
        "#     return len(y_train) / (2 * np.bincount(y_train))\n",
        "\n",
        "# def evaluate_anova(X_train, y_train, X_test, y_test, preprocessor, class_weights, k_values):\n",
        "#     f1_scores = []\n",
        "\n",
        "#     for k in k_values:\n",
        "#         logging.info(f\"Evaluating ANOVA with {k} features\")\n",
        "\n",
        "#         anova_pipeline = SklearnPipeline(steps=[\n",
        "#             ('preprocessor', preprocessor),\n",
        "#             ('anova', SelectKBest(score_func=f_classif, k=k))\n",
        "#         ])\n",
        "\n",
        "#         X_train_anova = anova_pipeline.fit_transform(X_train, y_train)\n",
        "#         X_test_anova = anova_pipeline.transform(X_test)\n",
        "\n",
        "#         undersampler = RandomUnderSampler(sampling_strategy=0.75, random_state=42)\n",
        "#         X_train_resampled, y_train_resampled = undersampler.fit_resample(X_train_anova, y_train)\n",
        "\n",
        "#         X_train_tensor = torch.tensor(X_train_resampled, dtype=torch.float32)\n",
        "#         y_train_tensor = torch.tensor(y_train_resampled, dtype=torch.float32).unsqueeze(1)\n",
        "#         X_test_tensor = torch.tensor(X_test_anova, dtype=torch.float32)\n",
        "#         y_test_tensor = torch.tensor(y_test.to_numpy(), dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "#         nn_estimator = SklearnSimpleNN(input_dim=X_train_tensor.shape[1], pos_weight=class_weights[1])\n",
        "#         nn_estimator.fit(X_train_tensor.numpy(), y_train_tensor.numpy())\n",
        "\n",
        "#         y_pred = nn_estimator.predict(X_test_tensor.numpy())\n",
        "#         f1 = f1_score(y_test_tensor.numpy(), y_pred)\n",
        "#         f1_scores.append(f1)\n",
        "#         logging.info(f\"Number of features: {k}, F1-score: {f1}\")\n",
        "\n",
        "#     return k_values, f1_scores\n",
        "\n",
        "# def plot_f1_scores(k_values, f1_scores):\n",
        "#     plt.figure(figsize=(10, 6))\n",
        "#     plt.plot(k_values, f1_scores, marker='o')\n",
        "#     plt.title('F1-Score vs. Number of Features (ANOVA)')\n",
        "#     plt.xlabel('Number of Features')\n",
        "#     plt.ylabel('F1-Score')\n",
        "#     plt.xticks(k_values)\n",
        "#     plt.grid()\n",
        "#     plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Import necessary libraries\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# from sklearn.compose import ColumnTransformer\n",
        "# from sklearn.pipeline import Pipeline\n",
        "# from sklearn.impute import SimpleImputer\n",
        "# from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "# from anova_feature_selection import set_seed, calculate_class_weights, evaluate_anova, plot_f1_scores\n",
        "\n",
        "# # Set random seed\n",
        "# set_seed(42)\n",
        "\n",
        "# # Load the dataset\n",
        "# url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\"\n",
        "# df = pd.read_excel(url, header=1)\n",
        "# df.columns = [col.lower().replace(' ', '_') for col in df.columns]\n",
        "\n",
        "# # Convert specific numeric columns to categorical\n",
        "# categorical_columns = ['sex', 'education', 'marriage']\n",
        "# df[categorical_columns] = df[categorical_columns].astype('category')\n",
        "\n",
        "# # Select features and target\n",
        "# target = 'default_payment_next_month'\n",
        "# X = df.drop(columns=[target])\n",
        "# y = df[target]\n",
        "\n",
        "# # Perform stratified train-test split\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# # Identify column types\n",
        "# numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "# categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "# # Define preprocessing for numeric columns\n",
        "# numeric_transformer = Pipeline(steps=[\n",
        "#     ('imputer', SimpleImputer(strategy='median')),\n",
        "#     ('scaler', StandardScaler())])\n",
        "\n",
        "# # Define preprocessing for categorical columns\n",
        "# categorical_transformer = Pipeline(steps=[\n",
        "#     ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "#     ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "# # Combine preprocessing steps\n",
        "# preprocessor = ColumnTransformer(\n",
        "#     transformers=[\n",
        "#         ('num', numeric_transformer, numeric_features),\n",
        "#         ('cat', categorical_transformer, categorical_features)])\n",
        "\n",
        "# # Calculate class weights\n",
        "# class_weights = calculate_class_weights(y_train)\n",
        "\n",
        "# # Define range of k values\n",
        "# k_values = range(1, X_train.shape[1] + 1)\n",
        "\n",
        "# # Evaluate ANOVA feature selection\n",
        "# k_values, f1_scores = evaluate_anova(X_train, y_train, X_test, y_test, preprocessor, class_weights, k_values)\n",
        "\n",
        "# # Plot F1-scores\n",
        "# plot_f1_scores(k_values, f1_scores)\n",
        "\n",
        "# # Find the optimal number of features\n",
        "# optimal_k = k_values[np.argmax(f1_scores)]\n",
        "# print(f\"Optimal number of features: {optimal_k}\")\n",
        "\n",
        "# # Save the optimal number of features\n",
        "# with open('optimal_anova_features.txt', 'w') as f:\n",
        "#     f.write(str(optimal_k))\n"
      ],
      "metadata": {
        "id": "3oRL_KAP48KR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}