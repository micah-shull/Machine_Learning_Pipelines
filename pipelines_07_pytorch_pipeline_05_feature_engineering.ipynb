{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP6Er7RuSVPV6YONwSjWKOz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/pipelines/blob/main/pipelines_07_pytorch_pipeline_05_feature_engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Feature Engineering Order of Operations\n",
        "\n",
        "In feature engineering, certain transformations are better applied before splitting the data into training and testing sets to avoid data leakage, while others can be applied after splitting. Here's how you can determine which transformations to apply before or after the split:\n",
        "\n",
        "### Apply Before Train-Test Split:\n",
        "1. **Interaction Features**: Creating new features by combining existing ones.\n",
        "2. **Date Features**: Extracting features from date columns.\n",
        "3. **Target Encoding**: Encoding categorical variables using the target variable (with caution).\n",
        "4. **Binning**: Converting continuous variables into categorical variables.\n",
        "5. **Ratio Features**: Creating ratio features (e.g., payment-to-bill ratios).\n",
        "6. **Aggregations**: Creating features based on aggregations over certain periods (e.g., average bill amounts).\n",
        "\n",
        "### Apply After Train-Test Split:\n",
        "1. **Normalization/Standardization**: Scaling features to have a mean of zero and a standard deviation of one, or scaling to a specific range.\n",
        "2. **Imputation**: Filling missing values with a specific strategy (mean, median, etc.).\n",
        "3. **Encoding Categorical Variables**: One-hot encoding or label encoding.\n",
        "4. **Polynomial Features**: Generating polynomial and interaction features.\n",
        "5. **SMOTE, ADASYN, or Other Resampling Techniques**: Addressing class imbalance.\n",
        "\n",
        "### Why Apply Certain Transformations After the Train-Test Split?\n",
        "- **Prevent Data Leakage**: Ensuring that no information from the test set is used to influence the training process.\n",
        "- **Consistent Scaling**: Calculating the parameters (mean, standard deviation) of scaling on the training set and then applying them to both the training and test sets.\n",
        "- **Proper Imputation**: Filling missing values based on the distribution of the training set.\n",
        "\n",
        "### Summary\n",
        "\n",
        "- **Before Train-Test Split**: We applied feature engineering steps that create new features from existing ones, such as interaction features, target encoding, binning, ratio features, and aggregated features.\n",
        "- **After Train-Test Split**: We performed transformations such as imputation, scaling, and one-hot encoding to ensure that these transformations are based only on the training set and applied consistently to both training and test sets.\n",
        "\n",
        "This approach ensures that there is no data leakage and that the transformations are applied correctly. Let me know if you need any further adjustments or explanations!\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MFiTPfmm49g1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.metrics import classification_report\n",
        "from model_pipeline import load_data_from_url, clean_column_names, remove_id_column, convert_categorical, split_data, train_model, calculate_class_weights, convert_to_tensors, preprocess_data, define_preprocessor, SimpleNN, SklearnSimpleNN\n",
        "\n",
        "# Define dataset-specific parameters\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\"\n",
        "categorical_columns = ['sex', 'education', 'marriage']\n",
        "target = 'default_payment_next_month'\n",
        "\n",
        "# Load and preprocess data\n",
        "data = load_data_from_url(url)\n",
        "data = clean_column_names(data)\n",
        "data = remove_id_column(data)\n",
        "data = convert_categorical(data, categorical_columns=categorical_columns)\n",
        "\n",
        "# Rename columns\n",
        "def rename_columns(df):\n",
        "    rename_dict = {\n",
        "        'pay_0': 'pay_1'\n",
        "    }\n",
        "    df = df.rename(columns=rename_dict)\n",
        "    return df\n",
        "\n",
        "data = rename_columns(data)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = split_data(data, target=target)\n",
        "\n",
        "# Define preprocessor and preprocess the data\n",
        "preprocessor = define_preprocessor(X_train)\n",
        "X_train_processed, X_test_processed = preprocess_data(preprocessor, X_train, X_test)\n",
        "\n",
        "# Convert data to tensors\n",
        "X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor = convert_to_tensors(X_train_processed, y_train, X_test_processed, y_test)\n"
      ],
      "metadata": {
        "id": "3pTgMB3Od0JB"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Before"
      ],
      "metadata": {
        "id": "Aezp32nClik2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model without feature engineering\n",
        "nn_estimator_before = SklearnSimpleNN(input_dim=X_train_tensor.shape[1], pos_weight=calculate_class_weights(y_train)[1])\n",
        "nn_estimator_before = train_model(nn_estimator_before, X_train_tensor, y_train_tensor)\n",
        "\n",
        "# Evaluate the model\n",
        "# def evaluate_model(nn_estimator, X_test_tensor, y_test_tensor):\n",
        "#     y_pred = nn_estimator.predict(X_test_tensor.numpy())\n",
        "#     print(classification_report(y_test_tensor.numpy(), y_pred))\n",
        "\n",
        "# evaluate_model(nn_estimator_before, X_test_tensor, y_test_tensor)\n",
        "\n",
        "# Train the model without feature engineering\n",
        "nn_estimator_before = SklearnSimpleNN(input_dim=X_train_tensor.shape[1], pos_weight=calculate_class_weights(y_train)[1])\n",
        "nn_estimator_before = train_model(nn_estimator_before, X_train_tensor, y_train_tensor)\n",
        "\n",
        "# Evaluate the model and save the report before feature engineering\n",
        "def evaluate_model(nn_estimator, X_test_tensor, y_test_tensor, label):\n",
        "    y_pred = nn_estimator.predict(X_test_tensor.numpy())\n",
        "    report = classification_report(y_test_tensor.numpy(), y_pred, output_dict=True)\n",
        "    print(f\"Classification Report ({label}):\")\n",
        "    print(classification_report(y_test_tensor.numpy(), y_pred))\n",
        "    return report\n",
        "\n",
        "report_before = evaluate_model(nn_estimator_before, X_test_tensor, y_test_tensor, \"Before Feature Engineering\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g21ydv1qn8_R",
        "outputId": "7e0909d3-f4c1-423a-9ffe-c12ec8de1579"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report (Before Feature Engineering):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.87      0.87      0.87      4673\n",
            "         1.0       0.53      0.52      0.53      1327\n",
            "\n",
            "    accuracy                           0.79      6000\n",
            "   macro avg       0.70      0.70      0.70      6000\n",
            "weighted avg       0.79      0.79      0.79      6000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply feature engineering to the entire dataset\n",
        "def create_interaction_features(df):\n",
        "    df['limit_bal_age'] = df['limit_bal'] * df['age']\n",
        "    return df\n",
        "\n",
        "def target_encode(df, target, categorical_columns):\n",
        "    for col in categorical_columns:\n",
        "        mean_target = df.groupby(col)[target].mean()\n",
        "        df[col + '_target_enc'] = df[col].map(mean_target)\n",
        "    return df\n",
        "\n",
        "def bin_features(df, column, bins):\n",
        "    df[column + '_binned'] = pd.cut(df[column], bins=bins)\n",
        "    return df\n",
        "\n",
        "def create_payment_to_bill_ratios(df):\n",
        "    for i in range(1, 7):\n",
        "        df[f'pay_to_bill_ratio_{i}'] = df[f'pay_amt{i}'] / df[f'bill_amt{i}'].replace(0, np.nan)\n",
        "    return df\n",
        "\n",
        "def create_payment_to_limit_ratios(df):\n",
        "    for i in range(1, 7):\n",
        "        df[f'pay_to_limit_ratio_{i}'] = df[f'pay_amt{i}'] / df['limit_bal']\n",
        "    return df\n",
        "\n",
        "def create_bill_to_limit_ratios(df):\n",
        "    for i in range(1, 7):\n",
        "        df[f'bill_to_limit_ratio_{i}'] = df[f'bill_amt{i}'] / df['limit_bal']\n",
        "    return df\n",
        "\n",
        "def create_lagged_payment_differences(df):\n",
        "    for i in range(1, 6):\n",
        "        df[f'pay_amt_diff_{i}'] = df[f'pay_amt{i+1}'] - df[f'pay_amt{i}']\n",
        "    return df\n",
        "\n",
        "def create_debt_ratio_features(df):\n",
        "    for i in range(1, 7):\n",
        "        df[f'debt_ratio_{i}'] = df[f'bill_amt{i}'] / df['limit_bal']\n",
        "    return df\n",
        "\n",
        "def create_average_payment_and_bill(df):\n",
        "    df['avg_payment'] = df[[f'pay_amt{i}' for i in range(1, 7)]].mean(axis=1)\n",
        "    df['avg_bill'] = df[[f'bill_amt{i}' for i in range(1, 7)]].mean(axis=1)\n",
        "    return df\n",
        "\n",
        "def create_payment_timeliness_features(df):\n",
        "    for i in range(1, 7):\n",
        "        df[f'pay_on_time_{i}'] = (df[f'pay_{i}'] <= 0).astype(int)\n",
        "    return df\n",
        "\n",
        "def create_total_payment_and_bill(df):\n",
        "    df['total_payment'] = df[[f'pay_amt{i}' for i in range(1, 7)]].sum(axis=1)\n",
        "    df['total_bill'] = df[[f'bill_amt{i}' for i in range(1, 7)]].sum(axis=1)\n",
        "    return df\n",
        "\n",
        "def create_bill_difference_features(df):\n",
        "    for i in range(1, 6):\n",
        "        df[f'bill_diff_{i}'] = df[f'bill_amt{i+1}'] - df[f'bill_amt{i}']\n",
        "    return df\n",
        "\n",
        "data = create_interaction_features(data)\n",
        "data = target_encode(data, target, categorical_columns)\n",
        "data = bin_features(data, column='age', bins=5)\n",
        "data = create_payment_to_bill_ratios(data)\n",
        "data = create_payment_to_limit_ratios(data)\n",
        "data = create_bill_to_limit_ratios(data)\n",
        "data = create_lagged_payment_differences(data)\n",
        "data = create_debt_ratio_features(data)\n",
        "data = create_average_payment_and_bill(data)\n",
        "data = create_payment_timeliness_features(data)\n",
        "data = create_total_payment_and_bill(data)\n",
        "data = create_bill_difference_features(data)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = split_data(data, target=target)\n",
        "\n",
        "# Define preprocessor and preprocess the data\n",
        "preprocessor = define_preprocessor(X_train)\n",
        "X_train_processed, X_test_processed = preprocess_data(preprocessor, X_train, X_test)\n",
        "\n",
        "# Convert data to tensors\n",
        "X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor = convert_to_tensors(X_train_processed, y_train, X_test_processed, y_test)\n",
        "\n",
        "# Train the model after feature engineering\n",
        "nn_estimator_after = SklearnSimpleNN(input_dim=X_train_tensor.shape[1], pos_weight=calculate_class_weights(y_train)[1])\n",
        "nn_estimator_after = train_model(nn_estimator_after, X_train_tensor, y_train_tensor)\n",
        "\n",
        "# Evaluate the model\n",
        "# evaluate_model(nn_estimator_after, X_test_tensor, y_test_tensor)\n",
        "\n",
        "# Evaluate the model and save the report after feature engineering\n",
        "report_after = evaluate_model(nn_estimator_after, X_test_tensor, y_test_tensor, \"After Feature Engineering\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWQ-LU9pvKuc",
        "outputId": "41f37474-783e-4214-b383-9c8ec46b7117"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report (After Feature Engineering):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.86      0.87      0.87      4673\n",
            "         1.0       0.54      0.52      0.53      1327\n",
            "\n",
            "    accuracy                           0.79      6000\n",
            "   macro avg       0.70      0.69      0.70      6000\n",
            "weighted avg       0.79      0.79      0.79      6000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compare Report"
      ],
      "metadata": {
        "id": "2pQkhwZhwpQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def compare_classification_reports(report_before, report_after):\n",
        "    # Convert reports to DataFrame\n",
        "    report_before_df = pd.DataFrame(report_before).transpose()\n",
        "    report_after_df = pd.DataFrame(report_after).transpose()\n",
        "\n",
        "    # Merge reports\n",
        "    comparison_df = report_before_df.join(report_after_df, lsuffix='_before', rsuffix='_after')\n",
        "\n",
        "    # Calculate percentage change\n",
        "    comparison_df['precision_change'] = (comparison_df['precision_after'] - comparison_df['precision_before']) / comparison_df['precision_before'] * 100\n",
        "    comparison_df['recall_change'] = (comparison_df['recall_after'] - comparison_df['recall_before']) / comparison_df['recall_before'] * 100\n",
        "    comparison_df['f1-score_change'] = (comparison_df['f1-score_after'] - comparison_df['f1-score_before']) / comparison_df['f1-score_before'] * 100\n",
        "\n",
        "    print(\"Comparison of Classification Report Metrics:\")\n",
        "    print(comparison_df[['precision_before', 'precision_after', 'precision_change',\n",
        "                         'recall_before', 'recall_after', 'recall_change',\n",
        "                         'f1-score_before', 'f1-score_after', 'f1-score_change']])\n",
        "\n",
        "    return comparison_df\n",
        "\n",
        "# Compare the classification reports before and after feature engineering\n",
        "comparison_df = compare_classification_reports(report_before, report_after)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nu_paVr_3PAL",
        "outputId": "aadce762-56d2-4aea-bede-dda80e9af3c1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparison of Classification Report Metrics:\n",
            "              precision_before  precision_after  precision_change  \\\n",
            "0.0                   0.865446         0.864012         -0.165714   \n",
            "1.0                   0.533384         0.535575          0.410618   \n",
            "accuracy              0.793333         0.794000          0.084034   \n",
            "macro avg             0.699415         0.699793          0.054045   \n",
            "weighted avg          0.792005         0.791373         -0.079871   \n",
            "\n",
            "              recall_before  recall_after  recall_change  f1-score_before  \\\n",
            "0.0                0.869891      0.872887       0.344403         0.867663   \n",
            "1.0                0.523738      0.516202      -1.438849         0.528517   \n",
            "accuracy           0.793333      0.794000       0.084034         0.793333   \n",
            "macro avg          0.696814      0.694544      -0.325758         0.698090   \n",
            "weighted avg       0.793333      0.794000       0.084034         0.792655   \n",
            "\n",
            "              f1-score_after  f1-score_change  \n",
            "0.0                 0.868427         0.088041  \n",
            "1.0                 0.525710        -0.531148  \n",
            "accuracy            0.794000         0.084034  \n",
            "macro avg           0.697068        -0.146350  \n",
            "weighted avg        0.792629        -0.003269  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "report_before"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5elyv8H2Lqy",
        "outputId": "3a06e2c0-76c4-4ac6-c120-5f5592480b76"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'0.0': {'precision': 0.8654460293804556,\n",
              "  'recall': 0.8698908624010272,\n",
              "  'f1-score': 0.8676627534685166,\n",
              "  'support': 4673},\n",
              " '1.0': {'precision': 0.533384497313891,\n",
              "  'recall': 0.5237377543330821,\n",
              "  'f1-score': 0.5285171102661597,\n",
              "  'support': 1327},\n",
              " 'accuracy': 0.7933333333333333,\n",
              " 'macro avg': {'precision': 0.6994152633471733,\n",
              "  'recall': 0.6968143083670546,\n",
              "  'f1-score': 0.6980899318673381,\n",
              "  'support': 6000},\n",
              " 'weighted avg': {'precision': 0.7920050872050669,\n",
              "  'recall': 0.7933333333333333,\n",
              "  'f1-score': 0.7926550420469287,\n",
              "  'support': 6000}}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "report_after"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nd2jug82Lnq",
        "outputId": "5fa06bb7-2d61-4bf4-8746-53b22b503d1f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'0.0': {'precision': 0.8640118618936666,\n",
              "  'recall': 0.8728867964904772,\n",
              "  'f1-score': 0.8684266553119012,\n",
              "  'support': 4673},\n",
              " '1.0': {'precision': 0.5355746677091477,\n",
              "  'recall': 0.5162019593067069,\n",
              "  'f1-score': 0.5257099002302379,\n",
              "  'support': 1327},\n",
              " 'accuracy': 0.794,\n",
              " 'macro avg': {'precision': 0.6997932648014071,\n",
              "  'recall': 0.694544377898592,\n",
              "  'f1-score': 0.6970682777710695,\n",
              "  'support': 6000},\n",
              " 'weighted avg': {'precision': 0.7913725024465239,\n",
              "  'recall': 0.794,\n",
              "  'f1-score': 0.7926291329796733,\n",
              "  'support': 6000}}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKk973dB3x5r",
        "outputId": "a4d4314b-b282-4ea2-c3a4-f677bf938aff"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 30000 entries, 0 to 29999\n",
            "Data columns (total 73 columns):\n",
            " #   Column                      Non-Null Count  Dtype   \n",
            "---  ------                      --------------  -----   \n",
            " 0   limit_bal                   30000 non-null  int64   \n",
            " 1   sex                         30000 non-null  category\n",
            " 2   education                   30000 non-null  category\n",
            " 3   marriage                    30000 non-null  category\n",
            " 4   age                         30000 non-null  int64   \n",
            " 5   pay_1                       30000 non-null  int64   \n",
            " 6   pay_2                       30000 non-null  int64   \n",
            " 7   pay_3                       30000 non-null  int64   \n",
            " 8   pay_4                       30000 non-null  int64   \n",
            " 9   pay_5                       30000 non-null  int64   \n",
            " 10  pay_6                       30000 non-null  int64   \n",
            " 11  bill_amt1                   30000 non-null  int64   \n",
            " 12  bill_amt2                   30000 non-null  int64   \n",
            " 13  bill_amt3                   30000 non-null  int64   \n",
            " 14  bill_amt4                   30000 non-null  int64   \n",
            " 15  bill_amt5                   30000 non-null  int64   \n",
            " 16  bill_amt6                   30000 non-null  int64   \n",
            " 17  pay_amt1                    30000 non-null  int64   \n",
            " 18  pay_amt2                    30000 non-null  int64   \n",
            " 19  pay_amt3                    30000 non-null  int64   \n",
            " 20  pay_amt4                    30000 non-null  int64   \n",
            " 21  pay_amt5                    30000 non-null  int64   \n",
            " 22  pay_amt6                    30000 non-null  int64   \n",
            " 23  default_payment_next_month  30000 non-null  int64   \n",
            " 24  limit_bal_age               30000 non-null  int64   \n",
            " 25  sex_target_enc              30000 non-null  category\n",
            " 26  education_target_enc        30000 non-null  category\n",
            " 27  marriage_target_enc         30000 non-null  category\n",
            " 28  age_binned                  30000 non-null  category\n",
            " 29  pay_to_bill_ratio_1         27992 non-null  float64 \n",
            " 30  pay_to_bill_ratio_2         27494 non-null  float64 \n",
            " 31  pay_to_bill_ratio_3         27130 non-null  float64 \n",
            " 32  pay_to_bill_ratio_4         26805 non-null  float64 \n",
            " 33  pay_to_bill_ratio_5         26494 non-null  float64 \n",
            " 34  pay_to_bill_ratio_6         25980 non-null  float64 \n",
            " 35  pay_to_limit_ratio_1        30000 non-null  float64 \n",
            " 36  pay_to_limit_ratio_2        30000 non-null  float64 \n",
            " 37  pay_to_limit_ratio_3        30000 non-null  float64 \n",
            " 38  pay_to_limit_ratio_4        30000 non-null  float64 \n",
            " 39  pay_to_limit_ratio_5        30000 non-null  float64 \n",
            " 40  pay_to_limit_ratio_6        30000 non-null  float64 \n",
            " 41  bill_to_limit_ratio_1       30000 non-null  float64 \n",
            " 42  bill_to_limit_ratio_2       30000 non-null  float64 \n",
            " 43  bill_to_limit_ratio_3       30000 non-null  float64 \n",
            " 44  bill_to_limit_ratio_4       30000 non-null  float64 \n",
            " 45  bill_to_limit_ratio_5       30000 non-null  float64 \n",
            " 46  bill_to_limit_ratio_6       30000 non-null  float64 \n",
            " 47  pay_amt_diff_1              30000 non-null  int64   \n",
            " 48  pay_amt_diff_2              30000 non-null  int64   \n",
            " 49  pay_amt_diff_3              30000 non-null  int64   \n",
            " 50  pay_amt_diff_4              30000 non-null  int64   \n",
            " 51  pay_amt_diff_5              30000 non-null  int64   \n",
            " 52  debt_ratio_1                30000 non-null  float64 \n",
            " 53  debt_ratio_2                30000 non-null  float64 \n",
            " 54  debt_ratio_3                30000 non-null  float64 \n",
            " 55  debt_ratio_4                30000 non-null  float64 \n",
            " 56  debt_ratio_5                30000 non-null  float64 \n",
            " 57  debt_ratio_6                30000 non-null  float64 \n",
            " 58  avg_payment                 30000 non-null  float64 \n",
            " 59  avg_bill                    30000 non-null  float64 \n",
            " 60  pay_on_time_1               30000 non-null  int64   \n",
            " 61  pay_on_time_2               30000 non-null  int64   \n",
            " 62  pay_on_time_3               30000 non-null  int64   \n",
            " 63  pay_on_time_4               30000 non-null  int64   \n",
            " 64  pay_on_time_5               30000 non-null  int64   \n",
            " 65  pay_on_time_6               30000 non-null  int64   \n",
            " 66  total_payment               30000 non-null  int64   \n",
            " 67  total_bill                  30000 non-null  int64   \n",
            " 68  bill_diff_1                 30000 non-null  int64   \n",
            " 69  bill_diff_2                 30000 non-null  int64   \n",
            " 70  bill_diff_3                 30000 non-null  int64   \n",
            " 71  bill_diff_4                 30000 non-null  int64   \n",
            " 72  bill_diff_5                 30000 non-null  int64   \n",
            "dtypes: category(7), float64(26), int64(40)\n",
            "memory usage: 15.3 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6x3Rzfd52Lkn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `target_encode` function is a feature engineering technique that replaces the categories of a categorical variable with the mean (or other statistic) of the target variable for each category. This method can be particularly useful when dealing with high-cardinality categorical features, where one-hot encoding would create too many dummy variables.\n",
        "\n",
        "### Explanation of Target Encoding\n",
        "\n",
        "Let's break down what the `target_encode` function does step by step:\n",
        "\n",
        "1. **Group by Category and Compute Mean Target**:\n",
        "   - The function groups the dataframe by each unique value of the categorical feature.\n",
        "   - It then computes the mean of the target variable for each category.\n",
        "\n",
        "2. **Map the Mean Target to the Original Data**:\n",
        "   - The function maps the computed mean target values back to the original dataframe, replacing the categorical feature values with these mean target values.\n",
        "   \n",
        "### Benefits of Target Encoding\n",
        "\n",
        "- **Dimensionality Reduction**: Unlike one-hot encoding, which can significantly increase the number of features, target encoding results in only one new feature for each categorical variable.\n",
        "- **Capturing Impact on Target**: Target encoding captures the relationship between the categorical feature and the target variable, which can be beneficial for certain machine learning models.\n",
        "\n",
        "### Potential Issues\n",
        "\n",
        "- **Data Leakage**: If not applied correctly, target encoding can lead to data leakage, where information from the test set influences the training set. This is why it's important to perform target encoding based only on the training data and then apply the same encoding to the test data.\n",
        "- **Overfitting**: Target encoding can sometimes lead to overfitting, especially if there are categories with few examples. Regularization techniques can be applied to mitigate this.\n",
        "\n"
      ],
      "metadata": {
        "id": "WG3mMUq-2uSP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Step 3: Split the Data into Training and Testing Sets\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "q_0FJ69F9e3a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Define Preprocessing Pipelines and Apply to Training Data\n"
      ],
      "metadata": {
        "id": "TLF3ZN9Q9nnB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Convert Data to Tensors and Train the Model"
      ],
      "metadata": {
        "id": "zIdN8Jgx9puc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Determining which features improve the performance of a machine learning model is an iterative process that involves several steps. Here are the general steps you can follow to evaluate the impact of different features on your model's performance:\n",
        "\n",
        "### 1. **Baseline Model**:\n",
        "   - Start by building a baseline model with minimal preprocessing and feature engineering.\n",
        "   - Evaluate the baseline model's performance using appropriate metrics (e.g., accuracy, precision, recall, F1-score, AUC-ROC).\n",
        "\n",
        "### 2. **Feature Addition/Removal**:\n",
        "   - Incrementally add or remove features and observe changes in model performance.\n",
        "   - This can be done manually or using automated methods like recursive feature elimination.\n",
        "\n",
        "### 3. **Cross-Validation**:\n",
        "   - Use cross-validation to ensure that the performance improvements are consistent and not due to random chance.\n",
        "   - Cross-validation helps in providing a more robust estimate of the model's performance.\n",
        "\n",
        "### 4. **Feature Importance**:\n",
        "   - For models that provide feature importance (e.g., tree-based models like Random Forest, Gradient Boosting), examine the feature importance scores.\n",
        "   - Identify the most influential features according to the model.\n",
        "\n",
        "### 5. **Statistical Tests**:\n",
        "   - Perform statistical tests to determine the significance of individual features.\n",
        "   - Techniques like ANOVA, chi-square tests, and mutual information can help in understanding the relationship between features and the target variable.\n",
        "\n",
        "### 6. **Model Performance Metrics**:\n",
        "   - Monitor and compare key performance metrics before and after adding/removing features.\n",
        "   - Common metrics include accuracy, precision, recall, F1-score, and AUC-ROC.\n",
        "\n",
        "### 7. **Model Interpretation Tools**:\n",
        "   - Use tools like SHAP (SHapley Additive exPlanations) and LIME (Local Interpretable Model-agnostic Explanations) to understand the impact of features on model predictions.\n",
        "\n",
        "### Example: Feature Importance with Random Forest\n",
        "\n",
        "Hereâ€™s an example of how you might use a Random Forest model to assess feature importance:\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Fit a Random Forest model\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train_processed, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = rf.predict(X_test_processed)\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Get feature importances\n",
        "importances = rf.feature_importances_\n",
        "feature_names = X_train.columns\n",
        "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
        "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
        "print(feature_importance_df)\n",
        "\n",
        "# Cross-validation\n",
        "cv_scores = cross_val_score(rf, X_train_processed, y_train, cv=5)\n",
        "print(f'Cross-validation scores: {cv_scores}')\n",
        "print(f'Mean CV score: {np.mean(cv_scores)}')\n",
        "```\n",
        "\n",
        "### Example: Using SHAP for Model Interpretation\n",
        "\n",
        "SHAP values provide a way to understand the contribution of each feature to individual predictions:\n",
        "\n",
        "```python\n",
        "import shap\n",
        "\n",
        "# Initialize the SHAP explainer\n",
        "explainer = shap.TreeExplainer(rf)\n",
        "shap_values = explainer.shap_values(X_train_processed)\n",
        "\n",
        "# Plot SHAP summary\n",
        "shap.summary_plot(shap_values, X_train_processed, feature_names=feature_names)\n",
        "```\n",
        "\n",
        "### Summary Steps:\n",
        "1. **Build a baseline model and evaluate its performance.**\n",
        "2. **Incrementally add or remove features and observe changes in performance.**\n",
        "3. **Use cross-validation to ensure consistent performance improvements.**\n",
        "4. **Examine feature importance scores from models that support it.**\n",
        "5. **Perform statistical tests to assess the significance of features.**\n",
        "6. **Monitor key performance metrics to compare different feature sets.**\n",
        "7. **Use model interpretation tools like SHAP or LIME to understand feature contributions.**\n",
        "\n",
        "This iterative process helps in identifying the most impactful features and refining the model for better performance. If you have specific features or transformations you'd like to evaluate, I can help with implementing those as well."
      ],
      "metadata": {
        "id": "rVSXbs4lDOFM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zULXFgCUAgp5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Updated model_pipeline.py script"
      ],
      "metadata": {
        "id": "lpsIBvfghTUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "script_content = \"\"\"\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "import logging\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Load the dataset from a URL\n",
        "def load_data_from_url(url):\n",
        "    df = pd.read_excel(url, header=1)\n",
        "    return df\n",
        "\n",
        "# Clean column names\n",
        "def clean_column_names(df):\n",
        "    df.columns = [col.lower().replace(' ', '_') for col in df.columns]\n",
        "    return df\n",
        "\n",
        "# Remove the 'id' column\n",
        "def remove_id_column(df):\n",
        "    if 'id' in df.columns:\n",
        "        df = df.drop(columns=['id'])\n",
        "    return df\n",
        "\n",
        "# Convert specified columns to categorical type\n",
        "def convert_categorical(df, categorical_columns):\n",
        "    df[categorical_columns] = df[categorical_columns].astype('category')\n",
        "    return df\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "def split_data(df, target):\n",
        "    X = df.drop(columns=[target])\n",
        "    y = df[target]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "# Define the preprocessor\n",
        "def define_preprocessor(X_train):\n",
        "    numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "    categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "    numeric_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())])\n",
        "\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numeric_transformer, numeric_features),\n",
        "            ('cat', categorical_transformer, categorical_features)])\n",
        "\n",
        "    return preprocessor\n",
        "\n",
        "# Preprocess the data\n",
        "def preprocess_data(preprocessor, X_train, X_test):\n",
        "    X_train_processed = preprocessor.fit_transform(X_train)\n",
        "    X_test_processed = preprocessor.transform(X_test)\n",
        "    return X_train_processed, X_test_processed\n",
        "\n",
        "# Calculate class weights for imbalanced datasets\n",
        "def calculate_class_weights(y_train):\n",
        "    return len(y_train) / (2 * np.bincount(y_train))\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "def convert_to_tensors(X_train_processed, y_train, X_test_processed, y_test):\n",
        "    X_train_tensor = torch.tensor(X_train_processed, dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)\n",
        "    X_test_tensor = torch.tensor(X_test_processed, dtype=torch.float32)\n",
        "    y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1)\n",
        "    return X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor\n",
        "\n",
        "# Define the neural network model\n",
        "class SimpleNN(torch.nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(input_dim, 32)\n",
        "        self.fc2 = torch.nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Commented out the original SklearnSimpleNN class definition\n",
        "# class SklearnSimpleNN(BaseEstimator, ClassifierMixin):\n",
        "#     def __init__(self, input_dim, learning_rate=0.001, epochs=50, batch_size=64, pos_weight=1.0):\n",
        "#         self.input_dim = input_dim\n",
        "#         self.learning_rate = learning_rate\n",
        "#         self.epochs = epochs\n",
        "#         self.batch_size = batch_size\n",
        "#         self.pos_weight = pos_weight\n",
        "#         self.model = SimpleNN(self.input_dim)\n",
        "\n",
        "#     def fit(self, X, y):\n",
        "#         criterion = torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor(self.pos_weight, dtype=torch.float32))\n",
        "#         optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "#         train_dataset = torch.utils.data.TensorDataset(torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32).unsqueeze(1))\n",
        "#         train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "#         for epoch in range(self.epochs):\n",
        "#             self.model.train()\n",
        "#             for inputs, targets in train_loader:\n",
        "#                 optimizer.zero_grad()\n",
        "#                 outputs = self.model(inputs)\n",
        "#                 loss = criterion(outputs, targets.view(-1, 1))\n",
        "#                 loss.backward()\n",
        "#                 optimizer.step()\n",
        "#         return self\n",
        "\n",
        "#     def predict(self, X):\n",
        "#         self.model.eval()\n",
        "#         with torch.no_grad():\n",
        "#             if isinstance(X, np.ndarray):\n",
        "#                 X = torch.tensor(X, dtype=torch.float32)\n",
        "#             elif isinstance(X, pd.DataFrame):\n",
        "#                 X = torch.tensor(X.values, dtype=torch.float32)\n",
        "#             outputs = self.model(X)\n",
        "#             probabilities = torch.sigmoid(outputs)\n",
        "#             predictions = (probabilities > 0.5).float()\n",
        "#         return predictions.numpy().squeeze()\n",
        "\n",
        "# Updated SklearnSimpleNN class definition\n",
        "class SklearnSimpleNN(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, input_dim, learning_rate=0.001, epochs=50, batch_size=64, pos_weight=1.0):\n",
        "        self.input_dim = input_dim\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.pos_weight = pos_weight\n",
        "        self.model = SimpleNN(self.input_dim)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        criterion = torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor(self.pos_weight, dtype=torch.float32))\n",
        "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "        train_dataset = torch.utils.data.TensorDataset(torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32).unsqueeze(1))\n",
        "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            self.model.train()\n",
        "            for inputs, targets in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                outputs = self.model(inputs)\n",
        "                loss = criterion(outputs, targets.view(-1, 1))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            if isinstance(X, np.ndarray):\n",
        "                X = torch.tensor(X, dtype=torch.float32)\n",
        "            elif isinstance(X, pd.DataFrame):\n",
        "                X = torch.tensor(X.values, dtype=torch.float32)\n",
        "            outputs = self.model(X)\n",
        "            probabilities = torch.sigmoid(outputs)\n",
        "            predictions = (probabilities > 0.5).float()\n",
        "        return predictions.numpy().squeeze()\n",
        "\n",
        "def train_model(nn_estimator, X_train_tensor, y_train_tensor):\n",
        "    nn_estimator.fit(X_train_tensor.numpy(), y_train_tensor.numpy())\n",
        "    return nn_estimator\n",
        "\n",
        "def evaluate_model(nn_estimator, X_test_tensor, y_test_tensor):\n",
        "    y_pred = nn_estimator.predict(X_test_tensor.numpy())\n",
        "    print(classification_report(y_test_tensor.numpy(), y_pred))\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Append the functions to model_pipeline.py\n",
        "with open(\"model_pipeline.py\", \"w\") as file:\n",
        "    file.write(script_content)\n",
        "\n",
        "print(\"Functions written successfully to model_pipeline.py\")\n",
        "\n",
        "# reload script to make function available for use\n",
        "import importlib\n",
        "import model_pipeline\n",
        "importlib.reload(model_pipeline)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0D4GvO2yhII4",
        "outputId": "58644547-8607-4dc1-b852-6632307299be"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Functions written successfully to model_pipeline.py\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'model_pipeline' from '/content/model_pipeline.py'>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}