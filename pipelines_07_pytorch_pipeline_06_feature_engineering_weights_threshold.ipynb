{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPQGvn5033uHIeHieZAaEQ4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/pipelines/blob/main/pipelines_07_pytorch_pipeline_06_feature_engineering_weights_threshold.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "wTFVPgaHvwQ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Global Parameters\n",
        "best_class_weight = 3.0\n",
        "best_lower_threshold = 0.10\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from model_pipeline import (\n",
        "    load_data_from_url, clean_column_names, rename_columns, remove_id_column,\n",
        "    convert_categorical, split_data, define_preprocessor, preprocess_data,\n",
        "    calculate_class_weights, convert_to_tensors, SklearnSimpleNN, train_model, evaluate_model\n",
        ")\n",
        "from feature_engineering import (\n",
        "    create_interaction_features, create_payment_to_bill_ratios,\n",
        "    create_payment_to_limit_ratios, create_bill_to_limit_ratios,\n",
        "    create_lagged_payment_differences, create_debt_ratio_features,\n",
        "    create_average_payment_and_bill, create_payment_timeliness_features,\n",
        "    create_total_payment_and_bill, create_bill_difference_features,\n",
        "    add_features_incrementally\n",
        ")"
      ],
      "metadata": {
        "id": "laa9tgTMvtQh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Load and Preprocess Data"
      ],
      "metadata": {
        "id": "yv-4Hw1qoPmq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9xQ8ncHZUeXq"
      },
      "outputs": [],
      "source": [
        "# Define dataset-specific parameters\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\"\n",
        "categorical_columns = ['sex', 'education', 'marriage']\n",
        "target = 'default_payment_next_month'\n",
        "\n",
        "# Load and preprocess data\n",
        "data = load_data_from_url(url)\n",
        "data = clean_column_names(data)\n",
        "data = rename_columns(data)\n",
        "data = remove_id_column(data)\n",
        "categorical_columns = ['sex', 'education', 'marriage']  # Specify your categorical columns\n",
        "data = convert_categorical(data, categorical_columns=categorical_columns)\n",
        "\n",
        "# Split the data\n",
        "target = 'default_payment_next_month'  # Specify your target column\n",
        "X_train, X_test, y_train, y_test = split_data(data, target=target)\n",
        "\n",
        "# Define the preprocessor\n",
        "preprocessor = define_preprocessor(X_train)\n",
        "\n",
        "# Preprocess the data\n",
        "X_train_processed, X_test_processed = preprocess_data(preprocessor, X_train, X_test)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor = convert_to_tensors(\n",
        "    X_train_processed, y_train, X_test_processed, y_test\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from feature_engineering import (\n",
        "    create_interaction_features, create_payment_to_bill_ratios,\n",
        "    create_payment_to_limit_ratios, create_bill_to_limit_ratios,\n",
        "    create_lagged_payment_differences, create_debt_ratio_features,\n",
        "    create_average_payment_and_bill, create_payment_timeliness_features,\n",
        "    create_total_payment_and_bill, create_bill_difference_features,\n",
        "    add_features_incrementally\n",
        ")\n"
      ],
      "metadata": {
        "id": "b4a32a3nuQC_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train and Evaluate the Model with Best Weights and Threshold"
      ],
      "metadata": {
        "id": "U6x6gTNaoVPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define and train the model with the best class weight and threshold\n",
        "best_class_weight = 3.0\n",
        "best_lower_threshold = 0.10\n",
        "\n",
        "nn_estimator = SklearnSimpleNN(input_dim=X_train_tensor.shape[1], pos_weight=best_class_weight, threshold=best_lower_threshold)\n",
        "nn_estimator = train_model(nn_estimator, X_train_tensor, y_train_tensor)\n",
        "\n",
        "# Evaluate the model\n",
        "baseline_report = evaluate_model(nn_estimator, X_test_tensor, y_test_tensor, label=\"Baseline\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUtjisWAoA2E",
        "outputId": "f727c3af-501c-4afe-d215-e9e578797663"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report (Baseline):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.96      0.09      0.17      4673\n",
            "         1.0       0.24      0.99      0.38      1327\n",
            "\n",
            "    accuracy                           0.29      6000\n",
            "   macro avg       0.60      0.54      0.27      6000\n",
            "weighted avg       0.80      0.29      0.22      6000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Engineering (Incremental Steps)\n",
        "\n",
        "1. **Define the feature engineering functions.**\n",
        "2. **Apply the feature engineering functions incrementally and evaluate the model after each addition.**"
      ],
      "metadata": {
        "id": "0NqAjmh7sNY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features_to_add = [\n",
        "    {'name': 'interaction_features', 'func': create_interaction_features},\n",
        "    {'name': 'payment_to_bill_ratios', 'func': create_payment_to_bill_ratios},\n",
        "    {'name': 'payment_to_limit_ratios', 'func': create_payment_to_limit_ratios},\n",
        "    {'name': 'bill_to_limit_ratios', 'func': create_bill_to_limit_ratios},\n",
        "    {'name': 'lagged_payment_differences', 'func': create_lagged_payment_differences},\n",
        "    {'name': 'debt_ratio_features', 'func': create_debt_ratio_features},\n",
        "    {'name': 'average_payment_and_bill', 'func': create_average_payment_and_bill},\n",
        "    {'name': 'payment_timeliness_features', 'func': create_payment_timeliness_features},\n",
        "    {'name': 'total_payment_and_bill', 'func': create_total_payment_and_bill},\n",
        "    {'name': 'bill_difference_features', 'func': create_bill_difference_features},\n",
        "]\n",
        "\n",
        "# Add features incrementally and evaluate\n",
        "incremental_results = add_features_incrementally(data, features_to_add, target, best_class_weight, best_lower_threshold)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95CwM4-p3ua5",
        "outputId": "42841901-ef78-479e-9eaf-997dd3f2b162"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adding feature: interaction_features\n",
            "Adding feature: payment_to_bill_ratios\n",
            "Adding feature: payment_to_limit_ratios\n",
            "Adding feature: bill_to_limit_ratios\n",
            "Adding feature: lagged_payment_differences\n",
            "Adding feature: debt_ratio_features\n",
            "Adding feature: average_payment_and_bill\n",
            "Adding feature: payment_timeliness_features\n",
            "Adding feature: total_payment_and_bill\n",
            "Adding feature: bill_difference_features\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_metric_changes(all_comparisons_df, 'precision')\n",
        "plot_metric_changes(all_comparisons_df, 'recall')\n",
        "plot_metric_changes(all_comparisons_df, 'f1-score')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "wg4Xj-IC6IIv",
        "outputId": "bb9cb77c-6a55-478f-ba05-27fcf7407ef0"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'all_comparisons_df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-87cfb073fdf4>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_metric_changes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_comparisons_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'precision'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplot_metric_changes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_comparisons_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'recall'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplot_metric_changes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_comparisons_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'f1-score'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'all_comparisons_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compare and Visualize Results"
      ],
      "metadata": {
        "id": "IfxrKiaxsDOF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_incremental_reports(report_baseline, incremental_results):\n",
        "    # Convert the baseline report to DataFrame\n",
        "    report_baseline_df = pd.DataFrame(report_baseline).transpose()\n",
        "\n",
        "    # Initialize a list to store comparison results\n",
        "    all_comparisons = []\n",
        "\n",
        "    # Iterate through the incremental results\n",
        "    for result in incremental_results:\n",
        "        report_after = result['report']\n",
        "        feature_name = result['feature']\n",
        "\n",
        "        # Convert the current report to DataFrame\n",
        "        report_after_df = pd.DataFrame(report_after).transpose()\n",
        "\n",
        "        # Merge the baseline and current reports\n",
        "        comparison_df = report_baseline_df.join(report_after_df, lsuffix='_baseline', rsuffix=f'_{feature_name}')\n",
        "\n",
        "        # Calculate percentage change\n",
        "        comparison_df[f'precision_change_{feature_name}'] = (comparison_df[f'precision_{feature_name}'] - comparison_df['precision_baseline']) / comparison_df['precision_baseline'] * 100\n",
        "        comparison_df[f'recall_change_{feature_name}'] = (comparison_df[f'recall_{feature_name}'] - comparison_df['recall_baseline']) / comparison_df['recall_baseline'] * 100\n",
        "        comparison_df[f'f1-score_change_{feature_name}'] = (comparison_df[f'f1-score_{feature_name}'] - comparison_df['f1-score_baseline']) / comparison_df['f1-score_baseline'] * 100\n",
        "\n",
        "        # Add the comparison result to the list\n",
        "        all_comparisons.append(comparison_df)\n",
        "\n",
        "    # Concatenate all comparison DataFrames\n",
        "    all_comparisons_df = pd.concat(all_comparisons, axis=1)\n",
        "\n",
        "    return all_comparisons_df\n",
        "\n",
        "all_comparisons_df = compare_incremental_reports(baseline_report, incremental_results)\n",
        "\n",
        "def plot_metric_changes(all_comparisons_df, metric):\n",
        "    # Extract percentage change columns for the specified metric\n",
        "    change_cols = [col for col in all_comparisons_df.columns if col.startswith(f'{metric}_change')]\n",
        "\n",
        "    # Extract feature names from the column headers\n",
        "    feature_names = [col.split('_')[-1] for col in change_cols]\n",
        "\n",
        "    # Plot the percentage changes\n",
        "    changes = all_comparisons_df.loc[:, change_cols].iloc[0]  # assuming we want the changes for the first class (or overall if it's a macro/micro average)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.barh(feature_names, changes, color='skyblue')\n",
        "    plt.axvline(x=0, color='gray', linestyle='--')  # Add vertical line at 0\n",
        "    plt.xlabel('Percentage Change')\n",
        "    plt.title(f'Percentage Change in {metric.capitalize()} After Adding Each Feature')\n",
        "    plt.grid(axis='x')\n",
        "    plt.show()\n",
        "\n",
        "plot_metric_changes(all_comparisons_df, 'precision')\n",
        "plot_metric_changes(all_comparisons_df, 'recall')\n",
        "plot_metric_changes(all_comparisons_df, 'f1-score')"
      ],
      "metadata": {
        "id": "7FfyWSh4oAzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5D22KsfooAw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MmvE28uVoAtz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Write Model Pipeline Script"
      ],
      "metadata": {
        "id": "vQmHAxTBjnGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to write script\n",
        "script_content = \"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "import logging\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Load the dataset from a URL\n",
        "def load_data_from_url(url):\n",
        "    df = pd.read_excel(url, header=1)\n",
        "    return df\n",
        "\n",
        "# Clean column names\n",
        "def clean_column_names(df):\n",
        "    df.columns = [col.lower().replace(' ', '_') for col in df.columns]\n",
        "    return df\n",
        "\n",
        "# Remove the 'id' column\n",
        "def remove_id_column(df):\n",
        "    if 'id' in df.columns:\n",
        "        df = df.drop(columns=['id'])\n",
        "    return df\n",
        "\n",
        "# Rename columns (pay_0 not in dataset)\n",
        "def rename_columns(df):\n",
        "    rename_dict = {\n",
        "        'pay_0': 'pay_1'\n",
        "    }\n",
        "    df = df.rename(columns=rename_dict)\n",
        "    return df\n",
        "\n",
        "# Convert specified columns to categorical type\n",
        "def convert_categorical(df, categorical_columns):\n",
        "    df[categorical_columns] = df[categorical_columns].astype('category')\n",
        "    return df\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "def split_data(df, target):\n",
        "    X = df.drop(columns=[target])\n",
        "    y = df[target]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "# Define the preprocessor\n",
        "def define_preprocessor(X_train):\n",
        "    numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "    categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "    numeric_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())])\n",
        "\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numeric_transformer, numeric_features),\n",
        "            ('cat', categorical_transformer, categorical_features)])\n",
        "\n",
        "    return preprocessor\n",
        "\n",
        "# Preprocess the data\n",
        "def preprocess_data(preprocessor, X_train, X_test):\n",
        "    X_train_processed = preprocessor.fit_transform(X_train)\n",
        "    X_test_processed = preprocessor.transform(X_test)\n",
        "    return X_train_processed, X_test_processed\n",
        "\n",
        "# Calculate class weights for imbalanced datasets\n",
        "def calculate_class_weights(y_train):\n",
        "    return len(y_train) / (2 * np.bincount(y_train))\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "def convert_to_tensors(X_train_processed, y_train, X_test_processed, y_test):\n",
        "    X_train_tensor = torch.tensor(X_train_processed, dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)\n",
        "    X_test_tensor = torch.tensor(X_test_processed, dtype=torch.float32)\n",
        "    y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1)\n",
        "    return X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor\n",
        "\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 32)\n",
        "        self.fc2 = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class SklearnSimpleNN(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, input_dim, learning_rate=0.001, epochs=50, batch_size=64,\n",
        "                 pos_weight=1.0, threshold=0.5):\n",
        "        self.input_dim = input_dim\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.pos_weight = pos_weight\n",
        "        self.threshold = threshold\n",
        "        self.model = SimpleNN(self.input_dim)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        criterion = torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor(self.pos_weight, dtype=torch.float32))\n",
        "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "        train_dataset = torch.utils.data.TensorDataset(torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32).unsqueeze(1))\n",
        "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            self.model.train()\n",
        "            for inputs, targets in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                outputs = self.model(inputs)\n",
        "                loss = criterion(outputs, targets.view(-1, 1))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            if isinstance(X, np.ndarray):\n",
        "                X = torch.tensor(X, dtype=torch.float32)\n",
        "            elif isinstance(X, pd.DataFrame):\n",
        "                X = torch.tensor(X.values, dtype=torch.float32)\n",
        "            outputs = self.model(X)\n",
        "            probabilities = torch.sigmoid(outputs)\n",
        "            predictions = (probabilities > self.threshold).float()\n",
        "        return predictions.numpy().squeeze()\n",
        "\n",
        "# Train the Model\n",
        "def train_model(nn_estimator, X_train_tensor, y_train_tensor):\n",
        "    nn_estimator.fit(X_train_tensor.numpy(), y_train_tensor.numpy())\n",
        "    return nn_estimator\n",
        "\n",
        "# Evaluate the Model\n",
        "def evaluate_model(nn_estimator, X_test_tensor, y_test_tensor, label=\"\"):\n",
        "    y_pred = nn_estimator.predict(X_test_tensor.numpy())\n",
        "    report = classification_report(y_test_tensor.numpy(), y_pred, output_dict=True)\n",
        "    print(f\"Classification Report ({label}):\")\n",
        "    print(classification_report(y_test_tensor.numpy(), y_pred))\n",
        "    return report\n",
        "\n",
        "# # Evaluate the Model\n",
        "# def evaluate_model(nn_estimator, X_test_tensor, y_test_tensor):\n",
        "#     y_pred = nn_estimator.predict(X_test_tensor.numpy())\n",
        "#     print(classification_report(y_test_tensor.numpy(), y_pred))\n",
        "\n",
        "# Function to run the full pipeline with resampling\n",
        "def run_full_pipeline_with_resampling(url, categorical_columns, target, resampling_method=None):\n",
        "    data = load_data_from_url(url)\n",
        "    data = clean_column_names(data)\n",
        "    data = remove_id_column(data)\n",
        "    data = convert_categorical(data, categorical_columns=categorical_columns)\n",
        "    X_train, X_test, y_train, y_test = split_data(data, target=target)\n",
        "    preprocessor = define_preprocessor(X_train)\n",
        "\n",
        "    if resampling_method:\n",
        "        resampling_pipeline = ImbPipeline(steps=[\n",
        "            ('preprocessor', preprocessor),\n",
        "            ('resampler', resampling_method)\n",
        "        ])\n",
        "        X_train_processed, y_train = resampling_pipeline.fit_resample(X_train, y_train)\n",
        "        X_test_processed = preprocessor.transform(X_test)\n",
        "    else:\n",
        "        X_train_processed, X_test_processed = preprocess_data(preprocessor, X_train, X_test)\n",
        "\n",
        "    class_weights = calculate_class_weights(y_train)\n",
        "\n",
        "    X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor = convert_to_tensors(\n",
        "        X_train_processed, y_train, X_test_processed, y_test)\n",
        "\n",
        "    nn_estimator = SklearnSimpleNN(input_dim=X_train_tensor.shape[1], pos_weight=class_weights[1])\n",
        "    nn_estimator = train_model(nn_estimator, X_train_tensor, y_train_tensor)\n",
        "    evaluate_model(nn_estimator, X_test_tensor, y_test_tensor)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser(description=\"Run full pipeline with resampling\")\n",
        "    parser.add_argument(\"url\", type=str, help=\"URL of the dataset\")\n",
        "    parser.add_argument(\"categorical_columns\", type=str, nargs=\"+\", help=\"List of categorical columns\")\n",
        "    parser.add_argument(\"target\", type=str, help=\"Target column\")\n",
        "    parser.add_argument(\"--resampling\", type=str, choices=['smote', 'oversample', 'undersample'], help=\"Resampling method\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    if args.resampling == 'smote':\n",
        "        resampling_method = SMOTE(random_state=42)\n",
        "    elif args.resampling == 'oversample':\n",
        "        resampling_method = RandomOverSampler(random_state=42)\n",
        "    elif args.resampling == 'undersample':\n",
        "        resampling_method = RandomUnderSampler(random_state=42)\n",
        "    else:\n",
        "        resampling_method = None\n",
        "\n",
        "    run_full_pipeline_with_resampling(args.url, args.categorical_columns, args.target, resampling_method)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# write script\n",
        "with open(\"model_pipeline.py\", \"w\") as file:\n",
        "    file.write(script_content)\n",
        "\n",
        "print(\"Function appended successfully to model_pipeline.py\")\n",
        "\n",
        "# reload script to make function available for use\n",
        "import importlib\n",
        "import model_pipeline\n",
        "importlib.reload(model_pipeline)\n",
        "\n",
        "from model_pipeline import (\n",
        "    load_data_from_url, clean_column_names, rename_columns, remove_id_column,\n",
        "    convert_categorical, split_data, define_preprocessor, preprocess_data,\n",
        "    calculate_class_weights, convert_to_tensors, SklearnSimpleNN, train_model, evaluate_model\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHmIxFr3gGh0",
        "outputId": "e0ece7a2-c306-4ef0-e7b2-94962a75ffe7"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function appended successfully to model_pipeline.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Write Feature Engineering Script"
      ],
      "metadata": {
        "id": "9dq7I1OOl4Bw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to write script\n",
        "script_content = \"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.metrics import classification_report\n",
        "from model_pipeline import load_data_from_url, clean_column_names, remove_id_column, convert_categorical, split_data, train_model, calculate_class_weights, convert_to_tensors, preprocess_data, define_preprocessor, SimpleNN, SklearnSimpleNN\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Rename columns (pay_0 not in dataset)\n",
        "def rename_columns(df):\n",
        "    rename_dict = {\n",
        "        'pay_0': 'pay_1'\n",
        "    }\n",
        "    df = df.rename(columns=rename_dict)\n",
        "    return df\n",
        "\n",
        "# Apply feature engineering to the entire dataset\n",
        "def create_interaction_features(df):\n",
        "    df['limit_bal_age'] = df['limit_bal'] * df['age']\n",
        "    return df\n",
        "\n",
        "def target_encode(df, target, categorical_columns):\n",
        "    for col in categorical_columns:\n",
        "        mean_target = df.groupby(col)[target].mean()\n",
        "        df[col + '_target_enc'] = df[col].map(mean_target)\n",
        "    return df\n",
        "\n",
        "def bin_features(df, column, bins):\n",
        "    df[column + '_binned'] = pd.cut(df[column], bins=bins)\n",
        "    return df\n",
        "\n",
        "def create_payment_to_bill_ratios(df):\n",
        "    for i in range(1, 7):\n",
        "        df[f'pay_to_bill_ratio_{i}'] = df[f'pay_amt{i}'] / df[f'bill_amt{i}'].replace(0, np.nan)\n",
        "    return df\n",
        "\n",
        "def create_payment_to_limit_ratios(df):\n",
        "    for i in range(1, 7):\n",
        "        df[f'pay_to_limit_ratio_{i}'] = df[f'pay_amt{i}'] / df['limit_bal']\n",
        "    return df\n",
        "\n",
        "def create_bill_to_limit_ratios(df):\n",
        "    for i in range(1, 7):\n",
        "        df[f'bill_to_limit_ratio_{i}'] = df[f'bill_amt{i}'] / df['limit_bal']\n",
        "    return df\n",
        "\n",
        "def create_lagged_payment_differences(df):\n",
        "    for i in range(1, 6):\n",
        "        df[f'pay_amt_diff_{i}'] = df[f'pay_amt{i+1}'] - df[f'pay_amt{i}']\n",
        "    return df\n",
        "\n",
        "def create_debt_ratio_features(df):\n",
        "    for i in range(1, 7):\n",
        "        df[f'debt_ratio_{i}'] = df[f'bill_amt{i}'] / df['limit_bal']\n",
        "    return df\n",
        "\n",
        "def create_average_payment_and_bill(df):\n",
        "    df['avg_payment'] = df[[f'pay_amt{i}' for i in range(1, 7)]].mean(axis=1)\n",
        "    df['avg_bill'] = df[[f'bill_amt{i}' for i in range(1, 7)]].mean(axis=1)\n",
        "    return df\n",
        "\n",
        "def create_payment_timeliness_features(df):\n",
        "    for i in range(1, 7):\n",
        "        df[f'pay_on_time_{i}'] = (df[f'pay_{i}'] <= 0).astype(int)\n",
        "    return df\n",
        "\n",
        "def create_total_payment_and_bill(df):\n",
        "    df['total_payment'] = df[[f'pay_amt{i}' for i in range(1, 7)]].sum(axis=1)\n",
        "    df['total_bill'] = df[[f'bill_amt{i}' for i in range(1, 7)]].sum(axis=1)\n",
        "    return df\n",
        "\n",
        "def create_bill_difference_features(df):\n",
        "    for i in range(1, 6):\n",
        "        df[f'bill_diff_{i}'] = df[f'bill_amt{i+1}'] - df[f'bill_amt{i}']\n",
        "    return df\n",
        "\n",
        "# Evaluate the model and save the report before feature engineering\n",
        "def evaluate_model(nn_estimator, X_test_tensor, y_test_tensor, label):\n",
        "    y_pred = nn_estimator.predict(X_test_tensor.numpy())\n",
        "    report = classification_report(y_test_tensor.numpy(), y_pred, output_dict=True)\n",
        "    # print(f\"Classification Report ({label}):\")\n",
        "    # print(classification_report(y_test_tensor.numpy(), y_pred))\n",
        "    return report\n",
        "\n",
        "def compare_classification_reports(report_before, report_after):\n",
        "    # Convert reports to DataFrame\n",
        "    report_before_df = pd.DataFrame(report_before).transpose()\n",
        "    report_after_df = pd.DataFrame(report_after).transpose()\n",
        "\n",
        "    # Merge reports\n",
        "    comparison_df = report_before_df.join(report_after_df, lsuffix='_before', rsuffix='_after')\n",
        "\n",
        "    # Calculate percentage change\n",
        "    comparison_df['precision_change'] = (comparison_df['precision_after'] - comparison_df['precision_before']) / comparison_df['precision_before'] * 100\n",
        "    comparison_df['recall_change'] = (comparison_df['recall_after'] - comparison_df['recall_before']) / comparison_df['recall_before'] * 100\n",
        "    comparison_df['f1-score_change'] = (comparison_df['f1-score_after'] - comparison_df['f1-score_before']) / comparison_df['f1-score_before'] * 100\n",
        "\n",
        "    print(\"Comparison of Classification Report Metrics:\")\n",
        "    print(comparison_df[['precision_before', 'precision_after', 'precision_change',\n",
        "                         'recall_before', 'recall_after', 'recall_change',\n",
        "                         'f1-score_before', 'f1-score_after', 'f1-score_change']])\n",
        "\n",
        "    return comparison_df\n",
        "\n",
        "# Incrementally Add Features and Evaluate\n",
        "def add_features_incrementally(data, features_to_add, target, pos_weight, threshold):\n",
        "    results = []\n",
        "\n",
        "    for feature in features_to_add:\n",
        "        # Create a copy of the data to avoid modifying the original DataFrame\n",
        "        data_copy = data.copy()\n",
        "\n",
        "        # Apply the feature engineering function\n",
        "        print(f\"Adding feature: {feature['name']}\")\n",
        "        data_copy = feature['func'](data_copy)\n",
        "\n",
        "        # Split the data\n",
        "        X_train, X_test, y_train, y_test = split_data(data_copy, target=target)\n",
        "\n",
        "        # Preprocess the data\n",
        "        preprocessor = define_preprocessor(X_train)\n",
        "        X_train_processed, X_test_processed = preprocess_data(preprocessor, X_train, X_test)\n",
        "\n",
        "        # Convert data to tensors\n",
        "        X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor = convert_to_tensors(X_train_processed, y_train, X_test_processed, y_test)\n",
        "\n",
        "        # Train the model\n",
        "        nn_estimator = SklearnSimpleNN(input_dim=X_train_tensor.shape[1], pos_weight=pos_weight, threshold=threshold)\n",
        "        nn_estimator = train_model(nn_estimator, X_train_tensor, y_train_tensor)\n",
        "\n",
        "        # Evaluate the model\n",
        "        report = evaluate_model(nn_estimator, X_test_tensor, y_test_tensor, label=feature['name'])\n",
        "        results.append({'feature': feature['name'], 'report': report})\n",
        "\n",
        "    return results\n",
        "\n",
        "# # Add features incrementally and test performance\n",
        "# def add_features_incrementally(data, features_to_add, target):\n",
        "#     results = []\n",
        "\n",
        "#     for feature in features_to_add:\n",
        "#         # Create a copy of the data to avoid modifying the original DataFrame\n",
        "#         data_copy = data.copy()\n",
        "\n",
        "#         # Apply the feature engineering function\n",
        "#         print(f\"Adding feature: {feature['name']}\")\n",
        "#         data_copy = feature['func'](data_copy)\n",
        "\n",
        "#         # Split the data\n",
        "#         X_train, X_test, y_train, y_test = split_data(data_copy, target=target)\n",
        "\n",
        "#         # Preprocess the data\n",
        "#         preprocessor = define_preprocessor(X_train)\n",
        "#         X_train_processed, X_test_processed = preprocess_data(preprocessor, X_train, X_test)\n",
        "\n",
        "#         # Convert data to tensors\n",
        "#         X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor = convert_to_tensors(X_train_processed, y_train, X_test_processed, y_test)\n",
        "\n",
        "#         # Train the model\n",
        "#         nn_estimator = SklearnSimpleNN(input_dim=X_train_tensor.shape[1], pos_weight=pos_weight, threshold=threshold)\n",
        "#         nn_estimator = train_model(nn_estimator, X_train_tensor, y_train_tensor)\n",
        "\n",
        "#         # Evaluate the model\n",
        "#         report = evaluate_model(nn_estimator, X_test_tensor, y_test_tensor)\n",
        "#         results.append({'feature': feature['name'], 'report': report})\n",
        "\n",
        "#     return results\n",
        "\n",
        "def compare_classification_reports(report_before, report_after):\n",
        "    # Convert reports to DataFrame\n",
        "    report_before_df = pd.DataFrame(report_before).transpose()\n",
        "    report_after_df = pd.DataFrame(report_after).transpose()\n",
        "\n",
        "    # Merge reports\n",
        "    comparison_df = report_before_df.join(report_after_df, lsuffix='_before', rsuffix='_after')\n",
        "\n",
        "    # Calculate percentage change\n",
        "    comparison_df['precision_change'] = (comparison_df['precision_after'] - comparison_df['precision_before']) / comparison_df['precision_before'] * 100\n",
        "    comparison_df['recall_change'] = (comparison_df['recall_after'] - comparison_df['recall_before']) / comparison_df['recall_before'] * 100\n",
        "    comparison_df['f1-score_change'] = (comparison_df['f1-score_after'] - comparison_df['f1-score_before']) / comparison_df['f1-score_before'] * 100\n",
        "\n",
        "    print(\"Comparison of Classification Report Metrics:\")\n",
        "    print(comparison_df[['precision_before', 'precision_after', 'precision_change',\n",
        "                         'recall_before', 'recall_after', 'recall_change',\n",
        "                         'f1-score_before', 'f1-score_after', 'f1-score_change']])\n",
        "\n",
        "    return comparison_df\n",
        "\n",
        "# def compare_incremental_reports(report_baseline, incremental_results):\n",
        "#     # Convert the baseline report to DataFrame\n",
        "#     report_baseline_df = pd.DataFrame(report_baseline).transpose()\n",
        "\n",
        "#     # Initialize a list to store comparison results\n",
        "#     all_comparisons = []\n",
        "\n",
        "#     # Iterate through the incremental results\n",
        "#     for result in incremental_results:\n",
        "#         report_after = result['report']\n",
        "#         feature_name = result['feature']\n",
        "\n",
        "#         # Convert the current report to DataFrame\n",
        "#         report_after_df = pd.DataFrame(report_after).transpose()\n",
        "\n",
        "#         # Merge the baseline and current reports\n",
        "#         comparison_df = report_baseline_df.join(report_after_df, lsuffix='_baseline', rsuffix=f'_{feature_name}')\n",
        "\n",
        "#         # Calculate percentage change\n",
        "#         comparison_df[f'precision_change_{feature_name}'] = (comparison_df[f'precision_{feature_name}'] - comparison_df['precision_baseline']) / comparison_df['precision_baseline'] * 100\n",
        "#         comparison_df[f'recall_change_{feature_name}'] = (comparison_df[f'recall_{feature_name}'] - comparison_df['recall_baseline']) / comparison_df['recall_baseline'] * 100\n",
        "#         comparison_df[f'f1-score_change_{feature_name}'] = (comparison_df[f'f1-score_{feature_name}'] - comparison_df['f1-score_baseline']) / comparison_df['f1-score_baseline'] * 100\n",
        "\n",
        "#         # Add the comparison result to the list\n",
        "#         all_comparisons.append(comparison_df)\n",
        "\n",
        "#     # Concatenate all comparison DataFrames\n",
        "#     all_comparisons_df = pd.concat(all_comparisons, axis=1)\n",
        "\n",
        "#     return all_comparisons_df\n",
        "\n",
        "def compare_incremental_reports(report_baseline, incremental_results):\n",
        "    # Convert the baseline report to DataFrame\n",
        "    report_baseline_df = pd.DataFrame(report_baseline).transpose()\n",
        "\n",
        "    # Initialize a list to store comparison results\n",
        "    all_comparisons = []\n",
        "\n",
        "    # Iterate through the incremental results\n",
        "    for result in incremental_results:\n",
        "        report_after = result['report']\n",
        "        feature_name = result['feature']\n",
        "\n",
        "        # Convert the current report to DataFrame\n",
        "        report_after_df = pd.DataFrame(report_after).transpose()\n",
        "\n",
        "        # Merge the baseline and current reports\n",
        "        comparison_df = report_baseline_df.join(report_after_df, lsuffix='_baseline', rsuffix=f'_{feature_name}')\n",
        "\n",
        "        # Calculate percentage change\n",
        "        comparison_df[f'precision_change_{feature_name}'] = (comparison_df[f'precision_{feature_name}'] - comparison_df['precision_baseline']) / comparison_df['precision_baseline'] * 100\n",
        "        comparison_df[f'recall_change_{feature_name}'] = (comparison_df[f'recall_{feature_name}'] - comparison_df['recall_baseline']) / comparison_df['recall_baseline'] * 100\n",
        "        comparison_df[f'f1-score_change_{feature_name}'] = (comparison_df[f'f1-score_{feature_name}'] - comparison_df['f1-score_baseline']) / comparison_df['f1-score_baseline'] * 100\n",
        "\n",
        "        # Add the comparison result to the list\n",
        "        all_comparisons.append(comparison_df)\n",
        "\n",
        "    # Concatenate all comparison DataFrames\n",
        "    all_comparisons_df = pd.concat(all_comparisons, axis=1)\n",
        "\n",
        "    return all_comparisons_df\n",
        "\n",
        "\n",
        "# # visualize impact features on performance\n",
        "# def plot_metric_changes(all_comparisons_df, metric):\n",
        "#     # Extract percentage change columns for the specified metric\n",
        "#     change_cols = [col for col in all_comparisons_df.columns if col.startswith(f'{metric}_change')]\n",
        "\n",
        "#     # Extract feature names from the column headers\n",
        "#     feature_names = [col.split('_')[-1] for col in change_cols]\n",
        "\n",
        "#     # Plot the percentage changes\n",
        "#     changes = all_comparisons_df.loc[:, change_cols].iloc[0]  # assuming we want the changes for the first class (or overall if it's a macro/micro average)\n",
        "#     plt.figure(figsize=(10, 6))\n",
        "#     plt.barh(feature_names, changes, color='skyblue')\n",
        "#     plt.axvline(x=0, color='gray', linestyle='--')  # Add vertical line at 0\n",
        "#     plt.xlabel('Percentage Change')\n",
        "#     plt.title(f'Percentage Change in {metric.capitalize()} After Adding Each Feature')\n",
        "#     plt.grid(axis='x')\n",
        "#     plt.show()\n",
        "\n",
        "def plot_metric_changes(all_comparisons_df, metric):\n",
        "    # Extract percentage change columns for the specified metric\n",
        "    change_cols = [col for col in all_comparisons_df.columns if col.startswith(f'{metric}_change')]\n",
        "\n",
        "    # Extract feature names from the column headers\n",
        "    feature_names = [col.split('_')[-1] for col in change_cols]\n",
        "\n",
        "    # Plot the percentage changes\n",
        "    changes = all_comparisons_df.loc[:, change_cols].iloc[0]  # assuming we want the changes for the first class (or overall if it's a macro/micro average)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.barh(feature_names, changes, color='skyblue')\n",
        "    plt.axvline(x=0, color='gray', linestyle='--')  # Add vertical line at 0\n",
        "    plt.xlabel('Percentage Change')\n",
        "    plt.title(f'Percentage Change in {metric.capitalize()} After Adding Each Feature')\n",
        "    plt.grid(axis='x')\n",
        "    plt.show()\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Write the functions to feature_engineering.py script\n",
        "with open(\"feature_engineering.py\", \"w\") as file:\n",
        "    file.write(script_content)\n",
        "\n",
        "print(\"Functions successfully written to feature_engineering.py\")\n",
        "\n",
        "# reload script to make function available for use\n",
        "import importlib\n",
        "import feature_engineering\n",
        "importlib.reload(feature_engineering)\n",
        "\n",
        "from feature_engineering import (\n",
        "    create_interaction_features, create_payment_to_bill_ratios,\n",
        "    create_payment_to_limit_ratios, create_bill_to_limit_ratios,\n",
        "    create_lagged_payment_differences, create_debt_ratio_features,\n",
        "    create_average_payment_and_bill, create_payment_timeliness_features,\n",
        "    create_total_payment_and_bill, create_bill_difference_features,\n",
        "    add_features_incrementally, compare_incremental_reports,\n",
        "    plot_metric_changes\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhB6lENSk1-n",
        "outputId": "b60690ce-3927-4b0c-e1ed-f01f1e1e6a9d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Functions successfully written to feature_engineering.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tfR_ivhp36ia"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}