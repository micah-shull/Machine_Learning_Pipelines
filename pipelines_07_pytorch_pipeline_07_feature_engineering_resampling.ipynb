{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPxCWRRQ1t6BBpwaYkni9ze",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/pipelines/blob/main/pipelines_07_pytorch_pipeline_07_feature_engineering_resampling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initial Setup"
      ],
      "metadata": {
        "id": "T0RVIhyqOkHK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from model_pipeline import (\n",
        "    load_data_from_url, clean_column_names, rename_columns, remove_id_column,\n",
        "    convert_categorical, split_data, define_preprocessor, preprocess_data,\n",
        "    calculate_class_weights, convert_to_tensors, SklearnSimpleNN, train_model, evaluate_model\n",
        ")\n",
        "from feature_engineering import (\n",
        "    create_interaction_features, create_payment_to_bill_ratios,\n",
        "    create_payment_to_limit_ratios, create_bill_to_limit_ratios,\n",
        "    create_lagged_payment_differences, create_debt_ratio_features,\n",
        "    create_average_payment_and_bill, create_payment_timeliness_features,\n",
        "    create_total_payment_and_bill, create_bill_difference_features,\n",
        "    bin_features, target_encode, rename_columns\n",
        ")\n",
        "\n",
        "# Define Global Parameters\n",
        "best_class_weight = 3.0\n",
        "best_lower_threshold = 0.10\n",
        "\n",
        "# Load and Preprocess Data\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\"\n",
        "data = load_data_from_url(url)\n",
        "data = clean_column_names(data)\n",
        "data = rename_columns(data)\n",
        "data = remove_id_column(data)\n",
        "categorical_columns = ['sex', 'education', 'marriage']  # Specify your categorical columns\n",
        "data = convert_categorical(data, categorical_columns=categorical_columns)\n",
        "target = 'default_payment_next_month'  # Specify your target column\n"
      ],
      "metadata": {
        "id": "FwYiwrxtOJAa"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Engineering"
      ],
      "metadata": {
        "id": "KCdDQd5BOnaT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply feature engineering\n",
        "data = create_interaction_features(data)\n",
        "data = create_payment_to_bill_ratios(data)\n",
        "data = create_payment_to_limit_ratios(data)\n",
        "data = create_bill_to_limit_ratios(data)\n",
        "data = create_lagged_payment_differences(data)\n",
        "data = create_debt_ratio_features(data)\n",
        "data = create_average_payment_and_bill(data)\n",
        "data = create_payment_timeliness_features(data)\n",
        "data = create_total_payment_and_bill(data)\n",
        "data = create_bill_difference_features(data)\n",
        "data = target_encode(data, target, categorical_columns)\n",
        "data = bin_features(data, 'age', 5)"
      ],
      "metadata": {
        "id": "OruVd1USOI9C"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Resampling Methods"
      ],
      "metadata": {
        "id": "mSTXhtTHOus9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to run the pipeline with different resampling methods\n",
        "def run_resampling_pipeline(data, target, resampling_method=None):\n",
        "    X_train, X_test, y_train, y_test = split_data(data, target=target)\n",
        "    preprocessor = define_preprocessor(X_train)\n",
        "\n",
        "    if resampling_method:\n",
        "        resampling_pipeline = ImbPipeline(steps=[\n",
        "            ('preprocessor', preprocessor),\n",
        "            ('resampler', resampling_method)\n",
        "        ])\n",
        "        X_train_processed, y_train = resampling_pipeline.fit_resample(X_train, y_train)\n",
        "        X_test_processed = preprocessor.transform(X_test)\n",
        "    else:\n",
        "        X_train_processed, X_test_processed = preprocess_data(preprocessor, X_train, X_test)\n",
        "\n",
        "    X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor = convert_to_tensors(\n",
        "        X_train_processed, y_train, X_test_processed, y_test)\n",
        "\n",
        "    nn_estimator = SklearnSimpleNN(input_dim=X_train_tensor.shape[1], pos_weight=best_class_weight, threshold=best_lower_threshold)\n",
        "    nn_estimator = train_model(nn_estimator, X_train_tensor, y_train_tensor)\n",
        "    report = evaluate_model(nn_estimator, X_test_tensor, y_test_tensor, label=str(resampling_method))\n",
        "    return report\n",
        "\n",
        "# Compare different resampling methods\n",
        "smote = SMOTE(random_state=42)\n",
        "oversample = RandomOverSampler(random_state=42)\n",
        "undersample = RandomUnderSampler(random_state=42)\n",
        "\n",
        "reports = {}\n",
        "reports['SMOTE'] = run_resampling_pipeline(data, target, smote)\n",
        "reports['RandomOverSampler'] = run_resampling_pipeline(data, target, oversample)\n",
        "reports['RandomUnderSampler'] = run_resampling_pipeline(data, target, undersample)\n",
        "reports['No Resampling'] = run_resampling_pipeline(data, target)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kxSWgHROI4D",
        "outputId": "411a9873-6b7c-4e87-84a9-2767cd20a8de"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report (SMOTE(random_state=42)):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.95      0.04      0.07      4673\n",
            "         1.0       0.23      0.99      0.37      1327\n",
            "\n",
            "    accuracy                           0.25      6000\n",
            "   macro avg       0.59      0.51      0.22      6000\n",
            "weighted avg       0.79      0.25      0.13      6000\n",
            "\n",
            "Classification Report (RandomOverSampler(random_state=42)):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.96      0.03      0.05      4673\n",
            "         1.0       0.23      1.00      0.37      1327\n",
            "\n",
            "    accuracy                           0.24      6000\n",
            "   macro avg       0.59      0.51      0.21      6000\n",
            "weighted avg       0.80      0.24      0.12      6000\n",
            "\n",
            "Classification Report (RandomUnderSampler(random_state=42)):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.95      0.00      0.01      4673\n",
            "         1.0       0.22      1.00      0.36      1327\n",
            "\n",
            "    accuracy                           0.22      6000\n",
            "   macro avg       0.59      0.50      0.19      6000\n",
            "weighted avg       0.79      0.22      0.09      6000\n",
            "\n",
            "Classification Report (None):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.96      0.08      0.14      4673\n",
            "         1.0       0.23      0.99      0.38      1327\n",
            "\n",
            "    accuracy                           0.28      6000\n",
            "   macro avg       0.60      0.53      0.26      6000\n",
            "weighted avg       0.80      0.28      0.20      6000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convert reports to DataFrame for analysis\n",
        "def reports_to_dataframe(reports):\n",
        "    data = []\n",
        "    for method, report in reports.items():\n",
        "        flattened_report = {'method': method}\n",
        "        for key, subdict in report.items():\n",
        "            if isinstance(subdict, dict):\n",
        "                for subkey, value in subdict.items():\n",
        "                    flattened_report[f\"{key}_{subkey}\"] = value\n",
        "            else:\n",
        "                flattened_report[key] = subdict\n",
        "        data.append(flattened_report)\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "reports_df = reports_to_dataframe(reports)\n",
        "print(reports_df[['method', '1.0_recall', '1.0_f1-score', 'macro avg_f1-score']])\n",
        "\n",
        "# Optional: Save to CSV\n",
        "reports_df.to_csv(\"resampling_results.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qwjdc-ecOI1h",
        "outputId": "d7147f3d-c44a-404f-b170-0791670b5459"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               method  1.0_recall  1.0_f1-score  macro avg_f1-score\n",
            "0               SMOTE    0.993218      0.368465            0.218075\n",
            "1   RandomOverSampler    0.996232      0.367733            0.211309\n",
            "2  RandomUnderSampler    0.999246      0.362989            0.185755\n",
            "3       No Resampling    0.987943      0.377375            0.260540\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Write Resampling Script"
      ],
      "metadata": {
        "id": "8c-wdLe_OJWV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpGyM_S8NTX-"
      },
      "outputs": [],
      "source": [
        "# Function to write script\n",
        "script_content = \"\"\"\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.combine import SMOTEENN\n",
        "from imblearn.over_sampling import ADASYN\n",
        "from sklearn.metrics import classification_report\n",
        "from model_pipeline import load_data_from_url, clean_column_names, remove_id_column, convert_categorical, split_data\n",
        "from model_pipeline import SklearnSimpleNN, train_model\n",
        "import torch\n",
        "\n",
        "# Function to convert DataFrames to tensors\n",
        "def convert_to_tensors(X, y):\n",
        "    X_tensor = torch.tensor(X.values, dtype=torch.float32)\n",
        "    y_tensor = torch.tensor(y.values, dtype=torch.float32).unsqueeze(1)\n",
        "    return X_tensor, y_tensor\n",
        "\n",
        "# Function to apply SMOTE\n",
        "def apply_smote(X_train, y_train):\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
        "    return X_train_res, y_train_res\n",
        "\n",
        "# Function to apply SMOTEENN\n",
        "def apply_smoteenn(X_train, y_train):\n",
        "    smoteenn = SMOTEENN(random_state=42)\n",
        "    X_train_res, y_train_res = smoteenn.fit_resample(X_train, y_train)\n",
        "    return X_train_res, y_train_res\n",
        "\n",
        "# Function to apply ADASYN\n",
        "def apply_adasyn(X_train, y_train):\n",
        "    adasyn = ADASYN(random_state=42)\n",
        "    X_train_res, y_train_res = adasyn.fit_resample(X_train, y_train)\n",
        "    return X_train_res, y_train_res\n",
        "\n",
        "# Function to apply undersampling\n",
        "def apply_undersampling(X_train, y_train):\n",
        "    undersampler = RandomUnderSampler(random_state=42)\n",
        "    X_train_res, y_train_res = undersampler.fit_resample(X_train, y_train)\n",
        "    return X_train_res, y_train_res\n",
        "\n",
        "# Generalized function to plot class distribution after sampling\n",
        "def plot_class_distribution_after_sampling(y_train_res, sampling_type):\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    sns.countplot(x=y_train_res, hue=y_train_res, palette='viridis', dodge=False,\n",
        "    order=y_train_res.value_counts().index, legend=False)\n",
        "    plt.title(f'Class Distribution After Applying {sampling_type}')\n",
        "    plt.xlabel('Class')\n",
        "    plt.ylabel('Count')\n",
        "\n",
        "    # Calculate the percentage for each class\n",
        "    total = len(y_train_res)\n",
        "    class_counts = y_train_res.value_counts()\n",
        "    for i, count in enumerate(class_counts):\n",
        "        percentage = 100 * count / total\n",
        "        plt.text(i, count, f'{percentage:.1f}%', ha='center', va='bottom')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Function to train and evaluate the model\n",
        "def train_and_evaluate_model(X_train, y_train, X_test, y_test, class_weights=None):\n",
        "    input_dim = X_train.shape[1]\n",
        "    if class_weights is None:\n",
        "        class_weights = [1.0, 1.0]\n",
        "\n",
        "    nn_estimator = SklearnSimpleNN(input_dim=input_dim, pos_weight=class_weights[1])\n",
        "    nn_estimator = train_model(nn_estimator, X_train, y_train)\n",
        "    y_pred = nn_estimator.predict(X_test)\n",
        "\n",
        "    report = classification_report(y_test, y_pred, output_dict=True)\n",
        "    return report, y_pred\n",
        "\n",
        "# Function to get classification report\n",
        "def get_classification_report(y_test_tensor, y_prob, threshold, resampling_method):\n",
        "    y_pred = (y_prob > threshold).astype(int)\n",
        "    report = classification_report(y_test_tensor.numpy(), y_pred, output_dict=True)\n",
        "    df_report = pd.DataFrame(report).transpose().reset_index()\n",
        "    df_report = df_report[df_report['index'].isin(['0.0', '1.0'])]\n",
        "    df_report.rename(columns={'index': 'class'}, inplace=True)\n",
        "    df_report['resampling'] = resampling_method\n",
        "    return df_report\n",
        "\n",
        "# Function to convert report to DataFrame\n",
        "def convert_report_to_dataframe(report, resampling_method):\n",
        "    df_report = pd.DataFrame(report).transpose().reset_index()\n",
        "    df_report['resampling'] = resampling_method\n",
        "    df_report = df_report[df_report['index'].isin(['0.0', '1.0'])]\n",
        "    df_report.rename(columns={'index': 'class'}, inplace=True)\n",
        "    return df_report\n",
        "\n",
        "# Function to filter and rename columns\n",
        "def filter_and_rename_columns(df, metrics=['precision', 'recall', 'f1-score']):\n",
        "    df = df[metrics + ['class', 'resampling']]\n",
        "    return df\n",
        "\n",
        "# Function to combine results\n",
        "def combine_results(*dfs):\n",
        "    df_combined = pd.concat(dfs, ignore_index=True)\n",
        "    df_combined = filter_and_rename_columns(df_combined)\n",
        "    df_combined.reset_index(drop=True, inplace=True)\n",
        "    return df_combined\n",
        "\n",
        "# Function to plot comparison with a descriptive name\n",
        "def plot_resampling_comparison(df, metrics=['precision', 'recall', 'f1-score']):\n",
        "    for metric in metrics:\n",
        "        plt.figure(figsize=(8, 4))\n",
        "        sns.barplot(data=df, x='class', y=metric, hue='resampling', palette='viridis')\n",
        "        plt.title(f'Comparison of {metric.capitalize()} by Resampling Technique')\n",
        "        plt.xlabel('Class')\n",
        "        plt.ylabel(metric.capitalize())\n",
        "        plt.legend(title='Resampling', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "        plt.show()\n",
        "\n",
        "# Function to apply resampling and plot class distribution\n",
        "def apply_resampling_and_plot(X_train, y_train):\n",
        "    # No Resampling\n",
        "    plot_class_distribution_after_sampling(y_train, \"No Resampling\")\n",
        "\n",
        "    # Apply SMOTE\n",
        "    X_train_smote, y_train_smote = apply_smote(X_train, y_train)\n",
        "    plot_class_distribution_after_sampling(y_train_smote, \"SMOTE\")\n",
        "\n",
        "    # Apply SMOTEENN\n",
        "    X_train_smoteenn, y_train_smoteenn = apply_smoteenn(X_train, y_train)\n",
        "    plot_class_distribution_after_sampling(y_train_smoteenn, \"SMOTEENN\")\n",
        "\n",
        "    # Apply ADASYN\n",
        "    X_train_adasyn, y_train_adasyn = apply_adasyn(X_train, y_train)\n",
        "    plot_class_distribution_after_sampling(y_train_adasyn, \"ADASYN\")\n",
        "\n",
        "    # Apply Undersampling\n",
        "    X_train_under, y_train_under = apply_undersampling(X_train, y_train)\n",
        "    plot_class_distribution_after_sampling(y_train_under, \"Undersampling\")\n",
        "\n",
        "    return (X_train, y_train), (X_train_smote, y_train_smote), (X_train_smoteenn, y_train_smoteenn), (X_train_adasyn, y_train_adasyn), (X_train_under, y_train_under)\n",
        "\n",
        "# Function to train and evaluate models\n",
        "def train_and_evaluate_models(X_train_sets, y_train_sets, X_test, y_test):\n",
        "    reports = []\n",
        "    y_preds = []\n",
        "\n",
        "    for X_train, y_train in zip(X_train_sets, y_train_sets):\n",
        "        X_train_tensor, y_train_tensor = convert_to_tensors(X_train, y_train)\n",
        "        X_test_tensor, y_test_tensor = convert_to_tensors(X_test, y_test)\n",
        "        report, y_pred = train_and_evaluate_model(X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor)\n",
        "        reports.append(report)\n",
        "        y_preds.append(y_pred)\n",
        "\n",
        "    return reports, y_preds\n",
        "\n",
        "# Function to gather reports and predictions into DataFrame\n",
        "def gather_reports_and_predictions(reports, y_trues, y_preds, model_names):\n",
        "    combined_report = pd.concat(\n",
        "        [pd.DataFrame(report).transpose().drop(['accuracy', 'macro avg', 'weighted avg'], errors='ignore').assign(resampling=model_name) for report, model_name in zip(reports, model_names)]\n",
        "    ).reset_index().rename(columns={'index': 'class'})\n",
        "\n",
        "    return combined_report\n",
        "\n",
        "# Main function to load data and call other functions\n",
        "def run_resampling_report(url, categorical_columns, target):\n",
        "    # Load and preprocess data\n",
        "    data = load_data_from_url(url)\n",
        "    data = clean_column_names(data)\n",
        "    data = remove_id_column(data)\n",
        "    data = convert_categorical(data, categorical_columns=categorical_columns)\n",
        "    X_train, X_test, y_train, y_test = split_data(data, target=target)\n",
        "\n",
        "    # Apply resampling and plot class distribution\n",
        "    (X_train, y_train), (X_train_smote, y_train_smote), (X_train_smoteenn, y_train_smoteenn), (X_train_adasyn, y_train_adasyn), (X_train_under, y_train_under) = apply_resampling_and_plot(X_train, y_train)\n",
        "\n",
        "    # Train and evaluate models\n",
        "    X_train_sets = [X_train, X_train_smote, X_train_smoteenn, X_train_adasyn, X_train_under]\n",
        "    y_train_sets = [y_train, y_train_smote, y_train_smoteenn, y_train_adasyn, y_train_under]\n",
        "    reports, y_preds = train_and_evaluate_models(X_train_sets, y_train_sets, X_test, y_test)\n",
        "\n",
        "    # Gather reports and predictions\n",
        "    model_names = ['No Resampling', 'SMOTE', 'SMOTEENN', 'ADASYN', 'Undersampling']\n",
        "    combined_report = gather_reports_and_predictions(reports, [y_test]*5, y_preds, model_names)\n",
        "\n",
        "    # Plot multiple classification reports\n",
        "    plot_resampling_comparison(combined_report)\n",
        "\n",
        "    return combined_report\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Write the functions to feature_engineering.py script\n",
        "with open(\"resampling_utils.py\", \"w\") as file:\n",
        "    file.write(script_content)\n",
        "\n",
        "print(\"Functions successfully written to resampling_utils.py\")\n",
        "\n",
        "# reload script to make function available for use\n",
        "import importlib\n",
        "import resampling_utils\n",
        "importlib.reload(resampling_utils)\n",
        "\n"
      ]
    }
  ]
}