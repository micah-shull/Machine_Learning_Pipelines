{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN1TdwFouUrmFkOKRD4De42",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/pipelines/blob/main/pipelines_08_preprocess_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6K1RxZ4t4mHI",
        "outputId": "02ede005-f559-4f38-fb9f-9a7dd89455a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing pipeline created successfully.\n",
            "Preprocessing complete. Here are the shapes of the preprocessed data:\n",
            "X_train_preprocessed shape: (24000, 33)\n",
            "X_test_preprocessed shape: (6000, 33)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from data_utils import (load_data_from_url, clean_column_names, remove_id_column,\n",
        "                        rename_columns, convert_categorical, preprocess_data, split_data, create_preprocessing_pipeline)\n",
        "\n",
        "# Define your parameters\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\"\n",
        "categorical_columns = ['sex', 'education', 'marriage']\n",
        "target = 'default_payment_next_month'\n",
        "\n",
        "# Load and Preprocess Data\n",
        "data = preprocess_data(url, categorical_columns)\n",
        "\n",
        "# Check if data is loaded and preprocessed correctly\n",
        "if data is not None:\n",
        "    # Split the data\n",
        "    X_train, X_test, y_train, y_test = split_data(data, target=target)\n",
        "\n",
        "    # Define preprocessing steps for numerical and categorical columns\n",
        "    numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "    categorical_features = X_train.select_dtypes(include=['category']).columns.tolist()\n",
        "\n",
        "    # Create preprocessing pipeline\n",
        "    pipeline = create_preprocessing_pipeline(numeric_features, categorical_features)\n",
        "\n",
        "    # Fit and transform the training data\n",
        "    X_train_preprocessed = pipeline.fit_transform(X_train)\n",
        "\n",
        "    # Transform the test data\n",
        "    X_test_preprocessed = pipeline.transform(X_test)\n",
        "\n",
        "    print(\"Preprocessing complete. Here are the shapes of the preprocessed data:\")\n",
        "    print(\"X_train_preprocessed shape:\", X_train_preprocessed.shape)\n",
        "    print(\"X_test_preprocessed shape:\", X_test_preprocessed.shape)\n",
        "else:\n",
        "    print(\"Data preprocessing failed. Please check the URL and preprocessing steps.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wrapping the steps of creating an sklearn pipeline into a single function can be a good practice in many scenarios. Here are the benefits and drawbacks of doing so:\n",
        "\n",
        "### Benefits:\n",
        "1. **Modularity and Reusability**: Encapsulating the pipeline creation in a function makes it easy to reuse the same preprocessing steps across different projects or datasets.\n",
        "2. **Readability and Maintainability**: Having a single function to create the pipeline improves the readability of your code. It becomes clear where the preprocessing steps are defined and makes the code easier to maintain.\n",
        "3. **Simplifies Testing**: By having a function that sets up the pipeline, you can write unit tests to ensure that the pipeline is constructed correctly.\n",
        "4. **Parameterization**: You can easily add parameters to the function to customize the pipeline for different datasets or requirements without changing the core logic.\n",
        "\n",
        "### Drawbacks:\n",
        "1. **Flexibility**: Wrapping the pipeline steps into a single function may reduce flexibility. If you need to make small adjustments to the pipeline for different tasks, you might end up with a complex function with many parameters.\n",
        "2. **Debugging**: If something goes wrong within the pipeline, it might be harder to debug because the steps are not as visible in the main script. However, this can be mitigated by adding logging or verbose options.\n",
        "\n"
      ],
      "metadata": {
        "id": "mq5ZD-hq82d0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to write script\n",
        "script_content = '''\n",
        "\n",
        "# data_utils.py\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "# Load the dataset from a URL\n",
        "def load_data_from_url(url):\n",
        "    \"\"\"\n",
        "    Load the dataset from a specified URL.\n",
        "\n",
        "    Parameters:\n",
        "    - url: str, URL of the dataset\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame, loaded dataset\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df = pd.read_excel(url, header=1)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data from URL: {e}\")\n",
        "        return None\n",
        "    return df\n",
        "\n",
        "# Clean column names\n",
        "def clean_column_names(df):\n",
        "    \"\"\"\n",
        "    Clean the column names by converting to lowercase and replacing spaces with underscores.\n",
        "\n",
        "    Parameters:\n",
        "    - df: pd.DataFrame, input dataframe\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame, dataframe with cleaned column names\n",
        "    \"\"\"\n",
        "    df.columns = [col.lower().replace(' ', '_') for col in df.columns]\n",
        "    return df\n",
        "\n",
        "# Remove the 'id' column\n",
        "def remove_id_column(df):\n",
        "    \"\"\"\n",
        "    Remove the 'id' column if it exists.\n",
        "\n",
        "    Parameters:\n",
        "    - df: pd.DataFrame, input dataframe\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame, dataframe without 'id' column\n",
        "    \"\"\"\n",
        "    if 'id' in df.columns:\n",
        "        df = df.drop(columns=['id'])\n",
        "    return df\n",
        "\n",
        "# Rename columns (pay_0 not in dataset)\n",
        "def rename_columns(df):\n",
        "    \"\"\"\n",
        "    Rename specific columns based on a predefined dictionary.\n",
        "\n",
        "    Parameters:\n",
        "    - df: pd.DataFrame, input dataframe\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame, dataframe with renamed columns\n",
        "    \"\"\"\n",
        "    rename_dict = {\n",
        "        'pay_0': 'pay_1'\n",
        "    }\n",
        "    df = df.rename(columns=rename_dict)\n",
        "    return df\n",
        "\n",
        "# Convert specified columns to categorical type\n",
        "def convert_categorical(df, categorical_columns):\n",
        "    \"\"\"\n",
        "    Convert specified columns to categorical type.\n",
        "\n",
        "    Parameters:\n",
        "    - df: pd.DataFrame, input dataframe\n",
        "    - categorical_columns: list of str, columns to convert to categorical type\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame, dataframe with converted columns\n",
        "    \"\"\"\n",
        "    df[categorical_columns] = df[categorical_columns].astype('category')\n",
        "    return df\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "def split_data(df, target):\n",
        "    \"\"\"\n",
        "    Split the data into training and testing sets.\n",
        "\n",
        "    Parameters:\n",
        "    - df: pd.DataFrame, input dataframe\n",
        "    - target: str, name of the target column\n",
        "\n",
        "    Returns:\n",
        "    - tuple, (X_train, X_test, y_train, y_test)\n",
        "    \"\"\"\n",
        "    X = df.drop(columns=[target])\n",
        "    y = df[target]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "def preprocess_data(url, categorical_columns):\n",
        "    \"\"\"\n",
        "    Load and preprocess the data.\n",
        "\n",
        "    Parameters:\n",
        "    - url: str, URL of the dataset\n",
        "    - categorical_columns: list of str, columns to convert to categorical type\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame, preprocessed data\n",
        "    \"\"\"\n",
        "    # Load the dataset\n",
        "    data = load_data_from_url(url)\n",
        "\n",
        "    if data is None:\n",
        "        return None\n",
        "\n",
        "    # Clean column names\n",
        "    data = clean_column_names(data)\n",
        "\n",
        "    # Remove the 'id' column\n",
        "    data = remove_id_column(data)\n",
        "\n",
        "    # Rename columns\n",
        "    data = rename_columns(data)\n",
        "\n",
        "    # Convert specified columns to categorical type\n",
        "    data = convert_categorical(data, categorical_columns)\n",
        "\n",
        "    return data\n",
        "\n",
        "def create_preprocessing_pipeline(numeric_features, categorical_features):\n",
        "    \"\"\"\n",
        "    Create a preprocessing pipeline for numeric and categorical features.\n",
        "\n",
        "    Parameters:\n",
        "    - numeric_features: list of str, names of numeric features\n",
        "    - categorical_features: list of str, names of categorical features\n",
        "\n",
        "    Returns:\n",
        "    - sklearn.pipeline.Pipeline, the complete preprocessing pipeline\n",
        "    \"\"\"\n",
        "    # Define the transformers for numerical and categorical data\n",
        "    numeric_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "    ])\n",
        "\n",
        "    # Combine the transformers using ColumnTransformer\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numeric_transformer, numeric_features),\n",
        "            ('cat', categorical_transformer, categorical_features)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Create the full pipeline with preprocessing and a placeholder for the model\n",
        "    pipeline = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor)\n",
        "        # You can add your model here, e.g., ('model', RandomForestClassifier())\n",
        "    ])\n",
        "\n",
        "    print(\"Preprocessing pipeline created successfully.\")\n",
        "    return pipeline\n",
        "\n",
        "'''\n",
        "\n",
        "# Write the script to a file\n",
        "with open(\"data_utils.py\", \"w\") as file:\n",
        "    file.write(script_content)\n",
        "\n",
        "print(\"Script successfully written to data_utils.py\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cww3l7cM7awU",
        "outputId": "e4d73d56-f8b0-403f-9d14-913b43c98ea6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Script successfully written to data_utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SAsIcx5O-pLt"
      },
      "execution_count": 2,
      "outputs": []
    }
  ]
}