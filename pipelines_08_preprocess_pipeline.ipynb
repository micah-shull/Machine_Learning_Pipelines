{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNaJs6sBQ5y5hW5sLoKD8Ix",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/pipelines/blob/main/pipelines_08_preprocess_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load & Preprocess Data"
      ],
      "metadata": {
        "id": "zx22VkQVFcIU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6K1RxZ4t4mHI",
        "outputId": "33987993-d190-41ea-fb3c-b8bbee5a9878"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing pipeline created successfully.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from data_utils import (load_data_from_url, clean_column_names, remove_id_column,\n",
        "                        rename_columns, convert_categorical, preprocess_data, split_data,\n",
        "                        create_preprocessing_pipeline, add_model_to_pipeline, evaluate_model,\n",
        "                        hyperparameter_tuning)\n",
        "\n",
        "# Define your parameters\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\"\n",
        "categorical_columns = ['sex', 'education', 'marriage']\n",
        "target = 'default_payment_next_month'\n",
        "\n",
        "\n",
        "# Load and Preprocess Data\n",
        "data = preprocess_data(url, categorical_columns)\n",
        "\n",
        "# Check if data is loaded and preprocessed correctly\n",
        "if data is not None:\n",
        "    # Split the data\n",
        "    X_train, X_test, y_train, y_test = split_data(data, target=target)\n",
        "\n",
        "    # Define preprocessing steps for numerical and categorical columns\n",
        "    numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "    categorical_features = X_train.select_dtypes(include=['category']).columns.tolist()\n",
        "\n",
        "    # Create preprocessing pipeline\n",
        "    pipeline = create_preprocessing_pipeline(numeric_features, categorical_features)\n",
        "else:\n",
        "    print(\"Data preprocessing failed. Please check the URL and preprocessing steps.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pipeline Function\n",
        "\n",
        "Wrapping the steps of creating an sklearn pipeline into a single function can be a good practice in many scenarios. Here are the benefits and drawbacks of doing so:\n",
        "\n",
        "### Benefits:\n",
        "1. **Modularity and Reusability**: Encapsulating the pipeline creation in a function makes it easy to reuse the same preprocessing steps across different projects or datasets.\n",
        "2. **Readability and Maintainability**: Having a single function to create the pipeline improves the readability of your code. It becomes clear where the preprocessing steps are defined and makes the code easier to maintain.\n",
        "3. **Simplifies Testing**: By having a function that sets up the pipeline, you can write unit tests to ensure that the pipeline is constructed correctly.\n",
        "4. **Parameterization**: You can easily add parameters to the function to customize the pipeline for different datasets or requirements without changing the core logic.\n",
        "\n",
        "### Drawbacks:\n",
        "1. **Flexibility**: Wrapping the pipeline steps into a single function may reduce flexibility. If you need to make small adjustments to the pipeline for different tasks, you might end up with a complex function with many parameters.\n",
        "2. **Debugging**: If something goes wrong within the pipeline, it might be harder to debug because the steps are not as visible in the main script. However, this can be mitigated by adding logging or verbose options.\n",
        "\n"
      ],
      "metadata": {
        "id": "mq5ZD-hq82d0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define & Evaluate Model"
      ],
      "metadata": {
        "id": "k33PWuXFPLrF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "model = LogisticRegression(max_iter=500, random_state=42)\n",
        "\n",
        "# Add the model to the pipeline\n",
        "pipeline_with_model = add_model_to_pipeline(pipeline, model)\n",
        "\n",
        "# Fit the pipeline with training data\n",
        "pipeline_with_model.fit(X_train, y_train)\n",
        "\n",
        "# Transform the test data and evaluate the model\n",
        "evaluate_model(pipeline_with_model, X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSxKcwq_Ngxi",
        "outputId": "294420d8-76db-497e-9b95-e2dcf8d776b7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.97      0.89      4673\n",
            "           1       0.69      0.24      0.36      1327\n",
            "\n",
            "    accuracy                           0.81      6000\n",
            "   macro avg       0.76      0.61      0.62      6000\n",
            "weighted avg       0.79      0.81      0.77      6000\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Write Data Utils Script"
      ],
      "metadata": {
        "id": "9-P3rfxXFhO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to write script\n",
        "script_content = r'''\n",
        "\n",
        "# data_utils.py\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
        "\n",
        "# Load the dataset from a URL\n",
        "def load_data_from_url(url):\n",
        "    \"\"\"\n",
        "    Load the dataset from a specified URL.\n",
        "\n",
        "    Parameters:\n",
        "    - url: str, URL of the dataset\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame, loaded dataset\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df = pd.read_excel(url, header=1)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data from URL: {e}\")\n",
        "        return None\n",
        "    return df\n",
        "\n",
        "# Clean column names\n",
        "def clean_column_names(df):\n",
        "    \"\"\"\n",
        "    Clean the column names by converting to lowercase and replacing spaces with underscores.\n",
        "\n",
        "    Parameters:\n",
        "    - df: pd.DataFrame, input dataframe\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame, dataframe with cleaned column names\n",
        "    \"\"\"\n",
        "    df.columns = [col.lower().replace(' ', '_') for col in df.columns]\n",
        "    return df\n",
        "\n",
        "# Remove the 'id' column\n",
        "def remove_id_column(df):\n",
        "    \"\"\"\n",
        "    Remove the 'id' column if it exists.\n",
        "\n",
        "    Parameters:\n",
        "    - df: pd.DataFrame, input dataframe\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame, dataframe without 'id' column\n",
        "    \"\"\"\n",
        "    if 'id' in df.columns:\n",
        "        df = df.drop(columns=['id'])\n",
        "    return df\n",
        "\n",
        "# Rename columns (pay_0 not in dataset)\n",
        "def rename_columns(df):\n",
        "    \"\"\"\n",
        "    Rename specific columns based on a predefined dictionary.\n",
        "\n",
        "    Parameters:\n",
        "    - df: pd.DataFrame, input dataframe\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame, dataframe with renamed columns\n",
        "    \"\"\"\n",
        "    rename_dict = {\n",
        "        'pay_0': 'pay_1'\n",
        "    }\n",
        "    df = df.rename(columns=rename_dict)\n",
        "    return df\n",
        "\n",
        "# Convert specified columns to categorical type\n",
        "def convert_categorical(df, categorical_columns):\n",
        "    \"\"\"\n",
        "    Convert specified columns to categorical type.\n",
        "\n",
        "    Parameters:\n",
        "    - df: pd.DataFrame, input dataframe\n",
        "    - categorical_columns: list of str, columns to convert to categorical type\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame, dataframe with converted columns\n",
        "    \"\"\"\n",
        "    df[categorical_columns] = df[categorical_columns].astype('category')\n",
        "    return df\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "def split_data(df, target):\n",
        "    \"\"\"\n",
        "    Split the data into training and testing sets.\n",
        "\n",
        "    Parameters:\n",
        "    - df: pd.DataFrame, input dataframe\n",
        "    - target: str, name of the target column\n",
        "\n",
        "    Returns:\n",
        "    - tuple, (X_train, X_test, y_train, y_test)\n",
        "    \"\"\"\n",
        "    X = df.drop(columns=[target])\n",
        "    y = df[target]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "def preprocess_data(url, categorical_columns):\n",
        "    \"\"\"\n",
        "    Load and preprocess the data.\n",
        "\n",
        "    Parameters:\n",
        "    - url: str, URL of the dataset\n",
        "    - categorical_columns: list of str, columns to convert to categorical type\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame, preprocessed data\n",
        "    \"\"\"\n",
        "    # Load the dataset\n",
        "    data = load_data_from_url(url)\n",
        "\n",
        "    if data is None:\n",
        "        return None\n",
        "\n",
        "    # Clean column names\n",
        "    data = clean_column_names(data)\n",
        "\n",
        "    # Remove the 'id' column\n",
        "    data = remove_id_column(data)\n",
        "\n",
        "    # Rename columns\n",
        "    data = rename_columns(data)\n",
        "\n",
        "    # Convert specified columns to categorical type\n",
        "    data = convert_categorical(data, categorical_columns)\n",
        "\n",
        "    return data\n",
        "\n",
        "def create_preprocessing_pipeline(numeric_features, categorical_features):\n",
        "    \"\"\"\n",
        "    Create a preprocessing pipeline for numeric and categorical features.\n",
        "\n",
        "    Parameters:\n",
        "    - numeric_features: list of str, names of numeric features\n",
        "    - categorical_features: list of str, names of categorical features\n",
        "\n",
        "    Returns:\n",
        "    - sklearn.pipeline.Pipeline, the complete preprocessing pipeline\n",
        "    \"\"\"\n",
        "    # Define the transformers for numerical and categorical data\n",
        "    numeric_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "    ])\n",
        "\n",
        "    # Combine the transformers using ColumnTransformer\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numeric_transformer, numeric_features),\n",
        "            ('cat', categorical_transformer, categorical_features)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Create the full pipeline with preprocessing and a placeholder for the model\n",
        "    pipeline = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor)\n",
        "        # You can add your model here, e.g., ('model', RandomForestClassifier())\n",
        "    ])\n",
        "\n",
        "    print(\"Preprocessing pipeline created successfully.\")\n",
        "    return pipeline\n",
        "\n",
        "def add_model_to_pipeline(pipeline, model):\n",
        "    \"\"\"\n",
        "    Add a model to the preprocessing pipeline.\n",
        "\n",
        "    Parameters:\n",
        "    - pipeline: sklearn.pipeline.Pipeline, the preprocessing pipeline\n",
        "    - model: sklearn estimator, the model to add to the pipeline\n",
        "\n",
        "    Returns:\n",
        "    - sklearn.pipeline.Pipeline, the complete pipeline with the model added\n",
        "    \"\"\"\n",
        "    return Pipeline(steps=pipeline.steps + [('model', model)])\n",
        "\n",
        "def evaluate_model(pipeline, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Evaluate the model using the test data.\n",
        "\n",
        "    Parameters:\n",
        "    - pipeline: sklearn.pipeline.Pipeline, the complete pipeline with preprocessing and model\n",
        "    - X_test: pd.DataFrame or np.ndarray, the test features\n",
        "    - y_test: pd.Series or np.ndarray, the test labels\n",
        "\n",
        "    Returns:\n",
        "    - np.ndarray, the predicted labels\n",
        "    \"\"\"\n",
        "    y_pred = pipeline.predict(X_test)\n",
        "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "    # print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "    return y_pred\n",
        "\n",
        "def hyperparameter_tuning(pipeline, param_grid, X_train, y_train):\n",
        "    \"\"\"\n",
        "    Perform hyperparameter tuning using GridSearchCV.\n",
        "\n",
        "    Parameters:\n",
        "    - pipeline: sklearn.pipeline.Pipeline, the complete pipeline with preprocessing and model\n",
        "    - param_grid: dict, the parameter grid for GridSearchCV\n",
        "    - X_train: pd.DataFrame or np.ndarray, the training features\n",
        "    - y_train: pd.Series or np.ndarray, the training labels\n",
        "\n",
        "    Returns:\n",
        "    - sklearn estimator, the best estimator found by GridSearchCV\n",
        "    \"\"\"\n",
        "    grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    print(\"Best Parameters:\\n\", grid_search.best_params_)\n",
        "    return grid_search.best_estimator_\n",
        "\n",
        "'''\n",
        "\n",
        "# Write the script to a file\n",
        "with open(\"data_utils.py\", \"w\") as file:\n",
        "    file.write(script_content)\n",
        "\n",
        "print(\"Script successfully written to data_utils.py\")\n",
        "\n",
        "# reload script to make function available for use\n",
        "import importlib\n",
        "import data_utils\n",
        "importlib.reload(data_utils)\n",
        "\n",
        "from data_utils import (load_data_from_url, clean_column_names, remove_id_column,\n",
        "                        rename_columns, convert_categorical, preprocess_data, split_data,\n",
        "                        create_preprocessing_pipeline, add_model_to_pipeline, evaluate_model,\n",
        "                        hyperparameter_tuning)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNvCByF2F5j1",
        "outputId": "8c4893da-396d-4d46-b8f7-58b0577afd0b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Script successfully written to data_utils.py\n"
          ]
        }
      ]
    }
  ]
}