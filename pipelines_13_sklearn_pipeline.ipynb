{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNmkZ9PzY5WH/87r4dAuMz7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/pipelines/blob/main/pipelines_13_sklearn_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scikit-Learn Pipelines\n",
        "\n",
        "To achieve maximum flexibility in your pipeline setup, especially for swapping in different resampling methods or adding new features, leveraging scikit-learn pipelines is a great approach. Scikit-learn pipelines allow you to seamlessly integrate various preprocessing steps, feature engineering, resampling methods, and model training in a streamlined manner. Here’s a structured way to set this up:\n",
        "\n",
        "1. **Use Scikit-Learn Pipelines for Preprocessing and Feature Engineering**: Define pipelines for common preprocessing tasks and feature engineering. These pipelines can be easily modified to include new steps as needed.\n",
        "\n",
        "2. **Create Configurable Resampling Pipelines**: Set up your pipelines to allow easy swapping of different resampling techniques using scikit-learn's `ColumnTransformer` and `Pipeline`.\n",
        "\n",
        "3. **Modular Functions for Flexibility**: Write modular functions to create and configure pipelines, making it easy to switch out different components.\n",
        "\n",
        "4. **Parameterize the Pipeline**: Use function parameters to pass different resampling methods or feature engineering steps to your pipeline functions.\n",
        "\n",
        "### Advantages of This Approach\n",
        "\n",
        "1. **Flexibility**: Easily switch between different resampling methods, feature selection techniques, or models by simply changing the function parameters.\n",
        "2. **Modularity**: The code is organized into modular functions, making it easier to maintain and extend.\n",
        "3. **Readability**: Using scikit-learn pipelines keeps the workflow clear and concise, improving code readability.\n",
        "4. **Reusability**: The same pipeline structure can be reused for different datasets or experiments with minimal changes.\n",
        "\n",
        "By parameterizing the pipeline creation functions and leveraging scikit-learn’s pipeline capabilities, you can create a robust, flexible, and reusable framework for machine learning experiments."
      ],
      "metadata": {
        "id": "XASioSlmP1Bn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load, Preprocess, Train & Evaluate Multiple Models"
      ],
      "metadata": {
        "id": "T_v7HCdGOYdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
        "from xgboost import XGBClassifier\n",
        "import logging\n",
        "from loan_data_utils import load_and_preprocess_data\n",
        "\n",
        "# Parameters\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\"\n",
        "categorical_columns = ['sex', 'education', 'marriage']\n",
        "target = 'default_payment_next_month'\n",
        "\n",
        "# Load and preprocess data\n",
        "X, y = load_and_preprocess_data(url, categorical_columns, target)\n",
        "\n",
        "# Identify numeric and categorical columns\n",
        "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "categorical_features = X.select_dtypes(include=['category']).columns.tolist()\n",
        "\n",
        "# Define the column transformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', Pipeline(steps=[\n",
        "            ('imputer', SimpleImputer(strategy='median')),\n",
        "            ('scaler', StandardScaler())\n",
        "        ]), numeric_features),\n",
        "        ('cat', Pipeline(steps=[\n",
        "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "            ('encoder', OneHotEncoder(drop='first'))\n",
        "        ]), categorical_features)\n",
        "    ])\n",
        "\n",
        "# Define the models to compare\n",
        "models = [\n",
        "    ('Logistic Regression', LogisticRegression(random_state=42, max_iter=1000)),\n",
        "    ('Random Forest', RandomForestClassifier(random_state=42)),\n",
        "    ('Support Vector Machine', SVC(random_state=42)),\n",
        "    ('K-Nearest Neighbors', KNeighborsClassifier()),\n",
        "    ('XGBoost', XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'))\n",
        "]\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Evaluate each model\n",
        "results = {}\n",
        "for name, model in models:\n",
        "    # Create the pipeline for the current model\n",
        "    pipeline = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', model)\n",
        "    ])\n",
        "\n",
        "    # Fit the pipeline to the training data\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # Predict the test data\n",
        "    y_pred = pipeline.predict(X_test)\n",
        "\n",
        "    # Evaluate the model\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred, average='macro')\n",
        "    results[name] = {'Accuracy': accuracy, 'F1 Score': f1}\n",
        "\n",
        "    # Print the classification report\n",
        "    print(f\"Model: {name}\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(\"\\n\")\n",
        "\n",
        "# Print the comparison of models\n",
        "print(\"Model Comparison:\")\n",
        "for name, metrics in results.items():\n",
        "    print(f\"{name}: Accuracy = {metrics['Accuracy']:.4f}, F1 Score = {metrics['F1 Score']:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVNA8aqmSESf",
        "outputId": "61d86f21-3880-4fc4-ba70-fa080e0d927f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.97      0.89      4687\n",
            "           1       0.70      0.24      0.36      1313\n",
            "\n",
            "    accuracy                           0.81      6000\n",
            "   macro avg       0.76      0.61      0.62      6000\n",
            "weighted avg       0.79      0.81      0.77      6000\n",
            "\n",
            "\n",
            "\n",
            "Model: Random Forest\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.94      0.89      4687\n",
            "           1       0.63      0.37      0.46      1313\n",
            "\n",
            "    accuracy                           0.81      6000\n",
            "   macro avg       0.74      0.65      0.68      6000\n",
            "weighted avg       0.80      0.81      0.80      6000\n",
            "\n",
            "\n",
            "\n",
            "Model: Support Vector Machine\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.96      0.89      4687\n",
            "           1       0.68      0.34      0.45      1313\n",
            "\n",
            "    accuracy                           0.82      6000\n",
            "   macro avg       0.76      0.65      0.67      6000\n",
            "weighted avg       0.80      0.82      0.80      6000\n",
            "\n",
            "\n",
            "\n",
            "Model: K-Nearest Neighbors\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.92      0.87      4687\n",
            "           1       0.55      0.36      0.43      1313\n",
            "\n",
            "    accuracy                           0.79      6000\n",
            "   macro avg       0.69      0.64      0.65      6000\n",
            "weighted avg       0.77      0.79      0.78      6000\n",
            "\n",
            "\n",
            "\n",
            "Model: XGBoost\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.94      0.89      4687\n",
            "           1       0.64      0.36      0.46      1313\n",
            "\n",
            "    accuracy                           0.82      6000\n",
            "   macro avg       0.74      0.65      0.68      6000\n",
            "weighted avg       0.80      0.82      0.80      6000\n",
            "\n",
            "\n",
            "\n",
            "Model Comparison:\n",
            "Logistic Regression: Accuracy = 0.8110, F1 Score = 0.6232\n",
            "Random Forest: Accuracy = 0.8150, F1 Score = 0.6760\n",
            "Support Vector Machine: Accuracy = 0.8202, F1 Score = 0.6714\n",
            "K-Nearest Neighbors: Accuracy = 0.7947, F1 Score = 0.6540\n",
            "XGBoost: Accuracy = 0.8153, F1 Score = 0.6753\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Write Loan Data Utils Script"
      ],
      "metadata": {
        "id": "tk50KaY4FAJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "script_content=r'''\n",
        "import pandas as pd\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "def load_data_from_url(url):\n",
        "    try:\n",
        "        df = pd.read_excel(url, header=1)\n",
        "        logging.info(\"Data loaded successfully from URL.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error loading data from URL: {e}\")\n",
        "        return None\n",
        "    return df\n",
        "\n",
        "def clean_column_names(df):\n",
        "    df.columns = [col.lower().replace(' ', '_') for col in df.columns]\n",
        "    return df\n",
        "\n",
        "def remove_id_column(df):\n",
        "    if 'id' in df.columns:\n",
        "        df = df.drop(columns=['id'])\n",
        "    return df\n",
        "\n",
        "def rename_columns(df):\n",
        "    rename_dict = {'pay_0': 'pay_1'}\n",
        "    df = df.rename(columns=rename_dict)\n",
        "    return df\n",
        "\n",
        "def convert_categorical(df, categorical_columns):\n",
        "    df[categorical_columns] = df[categorical_columns].astype('category')\n",
        "    return df\n",
        "\n",
        "def split_features_target(df, target):\n",
        "    X = df.drop(columns=[target])\n",
        "    y = df[target]\n",
        "    return X, y\n",
        "\n",
        "def load_and_preprocess_data(url, categorical_columns, target):\n",
        "    df = load_data_from_url(url)\n",
        "    if df is not None:\n",
        "        df = clean_column_names(df)\n",
        "        df = remove_id_column(df)\n",
        "        df = rename_columns(df)\n",
        "        df = convert_categorical(df, categorical_columns)\n",
        "        X, y = split_features_target(df, target)\n",
        "        return X, y\n",
        "    return None, None\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "# Write the script to a file\n",
        "with open(\"loan_data_utils.py\", \"w\") as file:\n",
        "    file.write(script_content)\n",
        "\n",
        "print(\"Script successfully written to loan_data_utils.py\")\n",
        "# Reload script to make functions available for use\n",
        "import importlib\n",
        "import loan_data_utils\n",
        "importlib.reload(loan_data_utils)\n",
        "\n",
        "from loan_data_utils import *\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83HsNhnoNXlh",
        "outputId": "df498b25-8ab2-4b3c-c29d-1116e2f2415c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Script successfully written to loan_data_utils.py\n"
          ]
        }
      ]
    }
  ]
}