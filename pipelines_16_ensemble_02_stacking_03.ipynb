{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNAf6PT9+vYE+Jb87muJ3r7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/pipelines/blob/main/pipelines_16_ensemble_02_stacking_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is Stacking?\n",
        "\n",
        "**Stacking (Stacked Generalization)** is an ensemble learning technique used to combine multiple machine learning models to improve overall performance. The fundamental idea behind stacking is to leverage the strengths of different models by combining their predictions to make more accurate and robust predictions.\n",
        "\n",
        "#### What is Stacking?\n",
        "\n",
        "1. **Base Models (Level-0 Models)**:\n",
        "   - These are the individual models that are trained on the same dataset. Each base model may learn different aspects of the data, potentially making different types of errors.\n",
        "   - Common base models include logistic regression, decision trees, random forests, support vector machines, and gradient boosting machines.\n",
        "\n",
        "2. **Meta-Model (Level-1 Model)**:\n",
        "   - The meta-model is trained to combine the predictions of the base models. It takes the outputs (predictions) of the base models as input features.\n",
        "   - The meta-model learns to predict the final output based on the patterns and correlations it finds in the predictions of the base models.\n",
        "\n",
        "3. **Training Process**:\n",
        "   - The dataset is typically split into training and validation sets.\n",
        "   - Base models are trained on the training set and make predictions on the validation set.\n",
        "   - The predictions of the base models on the validation set are used as input features to train the meta-model.\n",
        "   - In the final prediction phase, the base models make predictions on new data, and these predictions are fed into the meta-model to make the final prediction.\n",
        "\n",
        "#### Why Use Stacking?\n",
        "\n",
        "1. **Improved Performance**:\n",
        "   - By combining the strengths of multiple models, stacking often results in better predictive performance than any single model alone.\n",
        "   - Different models may capture different aspects of the data, and the meta-model can learn to weigh these appropriately.\n",
        "\n",
        "2. **Reduced Overfitting**:\n",
        "   - Stacking can help reduce overfitting, especially when combining models that are prone to different types of overfitting.\n",
        "   - The meta-model helps to smooth out the biases and variances of the individual base models.\n",
        "\n",
        "3. **Flexibility**:\n",
        "   - Stacking allows the use of a wide variety of base models, including both linear and non-linear models.\n",
        "   - It can be easily extended to include more complex meta-models, such as neural networks.\n"
      ],
      "metadata": {
        "id": "LYpvr0AA2mA_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "That sounds like a sound and systematic strategy. Here's a proposed approach to tackle the problem by breaking it down into the specified metrics and optimizing the models individually before feeding them into a meta-model:\n",
        "\n",
        "### Proposed Strategy:\n",
        "\n",
        "1. **Identify Best Performing Models**:\n",
        "   - For each metric (class 1 recall, class 1 precision, class 0 recall, class 0 precision), identify the models that have historically performed the best.\n",
        "   - This might involve running initial evaluations to gather performance metrics for various models.\n",
        "\n",
        "2. **Tune Models Individually**:\n",
        "   - Tune the identified best performing models for each metric using grid search or random search.\n",
        "   - Collect the best parameters for each model based on their performance on the respective metrics.\n",
        "\n",
        "3. **Combine Models into Meta-Model**:\n",
        "   - Use the tuned models as inputs to a meta-model (e.g., stacking classifier).\n",
        "   - Ensure that the meta-model leverages the strengths of each individual model.\n",
        "\n",
        "4. **Evaluate and Adjust**:\n",
        "   - Evaluate the performance of the meta-model.\n",
        "   - If needed, make adjustments to the models or the meta-model based on performance.\n",
        "\n",
        "### Steps in Detail:\n",
        "\n",
        "1. **Initial Evaluation**:\n",
        "   - Evaluate a set of candidate models (e.g., Logistic Regression, Random Forest, LightGBM, HistGradientBoosting) for each metric.\n",
        "   - Record their performance for class 1 recall, class 1 precision, class 0 recall, and class 0 precision.\n",
        "\n",
        "2. **Tune the Best Models**:\n",
        "   - For the models identified as best for each metric, perform hyperparameter tuning.\n",
        "   - Use grid search or random search to find the best parameters that maximize the respective metrics.\n",
        "\n",
        "3. **Create Meta-Model**:\n",
        "   - Use the tuned models in a stacking classifier or a weighted voting classifier.\n",
        "   - The meta-learner in a stacking classifier can be a simple model (e.g., Logistic Regression) to combine the strengths of the base models.\n",
        "\n"
      ],
      "metadata": {
        "id": "Kw46FvQ1TWNG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initial Evaluation"
      ],
      "metadata": {
        "id": "DPj9aj-HERb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import recall_score, precision_score\n",
        "from loan_data_utils import load_and_preprocess_data\n",
        "\n",
        "# Load and preprocess data (assuming this function is defined)\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\"\n",
        "categorical_columns = ['sex', 'education', 'marriage']\n",
        "target = 'default_payment_next_month'\n",
        "\n",
        "# Assuming the `load_and_preprocess_data` function is defined elsewhere\n",
        "X, y = load_and_preprocess_data(url, categorical_columns, target)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Identify numeric and categorical columns\n",
        "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "categorical_features = X.select_dtypes(include=['category']).columns.tolist()\n",
        "\n",
        "# Define the column transformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', Pipeline(steps=[\n",
        "            ('imputer', SimpleImputer(strategy='median')),\n",
        "            ('scaler', StandardScaler())\n",
        "        ]), numeric_features),\n",
        "        ('cat', Pipeline(steps=[\n",
        "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "            ('encoder', OneHotEncoder(drop='first'))\n",
        "        ]), categorical_features)\n",
        "    ])\n",
        "\n",
        "# Define candidate models\n",
        "candidate_models = {\n",
        "    'LogReg': LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000),\n",
        "    'RF': RandomForestClassifier(random_state=42, class_weight='balanced'),\n",
        "    'LGBM': LGBMClassifier(random_state=42, class_weight='balanced'),\n",
        "    'HGB': HistGradientBoostingClassifier(random_state=42, class_weight='balanced')\n",
        "}\n",
        "\n",
        "# Create pipelines for each candidate model\n",
        "pipelines = {name: Pipeline(steps=[('preprocessor', preprocessor), ('classifier', model)])\n",
        "             for name, model in candidate_models.items()}\n",
        "\n",
        "# Set threshold for classification\n",
        "THRESHOLD = 0.25\n",
        "\n",
        "# Function to evaluate models\n",
        "def evaluate_models(pipelines, X_train, y_train, X_test, y_test, threshold=0.25):\n",
        "    results = []\n",
        "    for name, pipeline in pipelines.items():\n",
        "        pipeline.fit(X_train, y_train)\n",
        "        y_proba = pipeline.predict_proba(X_test)[:, 1]\n",
        "        y_pred = (y_proba >= threshold).astype(int)\n",
        "        recall_1 = recall_score(y_test, y_pred, pos_label=1)\n",
        "        precision_1 = precision_score(y_test, y_pred, pos_label=1, zero_division=0)\n",
        "        recall_0 = recall_score(y_test, y_pred, pos_label=0)\n",
        "        precision_0 = precision_score(y_test, y_pred, pos_label=0, zero_division=0)\n",
        "        results.append({\n",
        "            'Model': name,\n",
        "            'Recall Class 1': recall_1,\n",
        "            'Precision Class 1': precision_1,\n",
        "            'Recall Class 0': recall_0,\n",
        "            'Precision Class 0': precision_0\n",
        "        })\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Evaluate candidate models\n",
        "evaluation_results = evaluate_models(pipelines, X_train, y_train, X_test, y_test, threshold=THRESHOLD)\n",
        "evaluation_results\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "fhzxgJfqG7uT",
        "outputId": "e0d1ec2d-8390-4bcf-c027-7ada89be3a82"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 5309, number of negative: 18691\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001846 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3276\n",
            "[LightGBM] [Info] Number of data points in the train set: 24000, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Info] Start training from score 0.000000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Model  Recall Class 1  Precision Class 1  Recall Class 0  \\\n",
              "0  LogReg        0.925396           0.238678        0.161780   \n",
              "1      RF        0.599849           0.461182        0.800984   \n",
              "2    LGBM        0.887717           0.288160        0.377274   \n",
              "3     HGB        0.915599           0.280341        0.332549   \n",
              "\n",
              "   Precision Class 0  \n",
              "0           0.884211  \n",
              "1           0.875760  \n",
              "2           0.922071  \n",
              "3           0.932773  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b84c4d10-e5e8-4608-8aa6-f2568f780a9e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Recall Class 1</th>\n",
              "      <th>Precision Class 1</th>\n",
              "      <th>Recall Class 0</th>\n",
              "      <th>Precision Class 0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LogReg</td>\n",
              "      <td>0.925396</td>\n",
              "      <td>0.238678</td>\n",
              "      <td>0.161780</td>\n",
              "      <td>0.884211</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>RF</td>\n",
              "      <td>0.599849</td>\n",
              "      <td>0.461182</td>\n",
              "      <td>0.800984</td>\n",
              "      <td>0.875760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>LGBM</td>\n",
              "      <td>0.887717</td>\n",
              "      <td>0.288160</td>\n",
              "      <td>0.377274</td>\n",
              "      <td>0.922071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>HGB</td>\n",
              "      <td>0.915599</td>\n",
              "      <td>0.280341</td>\n",
              "      <td>0.332549</td>\n",
              "      <td>0.932773</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b84c4d10-e5e8-4608-8aa6-f2568f780a9e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b84c4d10-e5e8-4608-8aa6-f2568f780a9e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b84c4d10-e5e8-4608-8aa6-f2568f780a9e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-9e18057f-e731-4fe6-8cc9-d4776d993c37\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9e18057f-e731-4fe6-8cc9-d4776d993c37')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-9e18057f-e731-4fe6-8cc9-d4776d993c37 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_bf8b7b88-7d52-4f1d-a473-ef43c8922218\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('evaluation_results')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_bf8b7b88-7d52-4f1d-a473-ef43c8922218 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('evaluation_results');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "evaluation_results",
              "summary": "{\n  \"name\": \"evaluation_results\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"Model\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"RF\",\n          \"HGB\",\n          \"LogReg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Recall Class 1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.15568105666797216,\n        \"min\": 0.5998492840994725,\n        \"max\": 0.9253956292388847,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.5998492840994725,\n          0.9155990957045969,\n          0.9253956292388847\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Precision Class 1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.09848560582924111,\n        \"min\": 0.23867832847424683,\n        \"max\": 0.46118192352259557,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.46118192352259557,\n          0.2803414859252423,\n          0.23867832847424683\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Recall Class 0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2715915863241772,\n        \"min\": 0.16178044083030174,\n        \"max\": 0.8009843783436764,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.8009843783436764,\n          0.33254868392895354,\n          0.16178044083030174\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Precision Class 0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.02794759503029996,\n        \"min\": 0.8757604117922321,\n        \"max\": 0.9327731092436975,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.8757604117922321,\n          0.9327731092436975,\n          0.8842105263157894\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Select & Tune Models"
      ],
      "metadata": {
        "id": "s3KOouj7UbK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import make_scorer, recall_score, precision_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import joblib\n",
        "import json\n",
        "\n",
        "# Custom scorers for recall and precision for class 0 and class 1\n",
        "scorers = {\n",
        "    'recall_class_1': make_scorer(recall_score, pos_label=1),\n",
        "    'precision_class_1': make_scorer(precision_score, pos_label=1),\n",
        "    'recall_class_0': make_scorer(recall_score, pos_label=0),\n",
        "    'precision_class_0': make_scorer(precision_score, pos_label=0)\n",
        "}\n",
        "\n",
        "def tune_and_save_models(pipelines, param_grids, X_train, y_train, evaluation_results, scorers, models_file, params_file):\n",
        "    best_models = {}\n",
        "    best_params = {}\n",
        "\n",
        "    for metric, scorer in scorers.items():\n",
        "        if 'recall' in metric:\n",
        "            class_num = metric.split('_')[-1]\n",
        "            model_name = evaluation_results.loc[evaluation_results[f'Recall Class {class_num}'].idxmax(), 'Model']\n",
        "        else:\n",
        "            class_num = metric.split('_')[-1]\n",
        "            model_name = evaluation_results.loc[evaluation_results[f'Precision Class {class_num}'].idxmax(), 'Model']\n",
        "\n",
        "        tuned_model = tune_model(pipelines[model_name], param_grids[model_name], X_train, y_train, scoring=scorer)\n",
        "\n",
        "        best_models[metric] = tuned_model.best_estimator_\n",
        "        best_params[metric] = tuned_model.best_params_\n",
        "\n",
        "    joblib.dump(best_models, models_file)\n",
        "\n",
        "    with open(params_file, 'w') as json_file:\n",
        "        json.dump(best_params, json_file, indent=4)\n",
        "\n",
        "    return best_models, best_params\n",
        "\n",
        "# Function to perform grid search for a given model\n",
        "def tune_model(pipeline, param_grid, X_train, y_train, scoring):\n",
        "    grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring=scoring)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    return grid_search\n",
        "\n",
        "# Define parameter grids for the selected models\n",
        "param_grids = {\n",
        "    'LogReg': {'classifier__C': [0.1, 1, 10]},\n",
        "    'RF': {'classifier__n_estimators': [100, 200], 'classifier__max_depth': [10, 20]},\n",
        "    'LGBM': {'classifier__n_estimators': [100, 200], 'classifier__learning_rate': [0.01, 0.1]},\n",
        "    'HGB': {'classifier__max_iter': [100, 200], 'classifier__learning_rate': [0.01, 0.1]}\n",
        "}\n",
        "\n",
        "# Assuming evaluation_results is already defined and populated\n",
        "# Here is an example placeholder; replace it with actual evaluation results\n",
        "evaluation_results = pd.DataFrame({\n",
        "    'Model': ['LogReg', 'RF', 'LGBM', 'HGB'],\n",
        "    'Recall Class 1': [0.8, 0.82, 0.85, 0.83],\n",
        "    'Precision Class 1': [0.75, 0.78, 0.81, 0.79],\n",
        "    'Recall Class 0': [0.9, 0.88, 0.89, 0.87],\n",
        "    'Precision Class 0': [0.85, 0.86, 0.84, 0.83]\n",
        "})\n",
        "\n",
        "# Define scoring metrics using custom scorers\n",
        "scoring_metrics = ['recall_class_1', 'precision_class_1', 'recall_class_0', 'precision_class_0']\n",
        "\n",
        "# Tune and save models\n",
        "models_file = 'best_models.pkl'\n",
        "params_file = 'best_params.json'\n",
        "\n",
        "best_models, best_params = tune_and_save_models(pipelines, param_grids, X_train, y_train, evaluation_results, scorers, models_file, params_file)\n",
        "\n",
        "# Print the best parameters\n",
        "for metric, params in best_params.items():\n",
        "    print(f\"Best parameters for {metric}: {params}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SK_dz4OJH2WM",
        "outputId": "413f8603-0ae5-4749-fd05-b33004f7fc8a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 4248, number of negative: 14952\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006647 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3276\n",
            "[LightGBM] [Info] Number of data points in the train set: 19200, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Info] Start training from score 0.000000\n",
            "[LightGBM] [Info] Number of positive: 4247, number of negative: 14953\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002626 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3269\n",
            "[LightGBM] [Info] Number of data points in the train set: 19200, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
            "[LightGBM] [Info] Start training from score -0.000000\n",
            "[LightGBM] [Info] Number of positive: 4247, number of negative: 14953\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002706 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3272\n",
            "[LightGBM] [Info] Number of data points in the train set: 19200, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
            "[LightGBM] [Info] Start training from score -0.000000\n",
            "[LightGBM] [Info] Number of positive: 4247, number of negative: 14953\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003903 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3271\n",
            "[LightGBM] [Info] Number of data points in the train set: 19200, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
            "[LightGBM] [Info] Start training from score -0.000000\n",
            "[LightGBM] [Info] Number of positive: 4247, number of negative: 14953\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006008 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3273\n",
            "[LightGBM] [Info] Number of data points in the train set: 19200, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
            "[LightGBM] [Info] Start training from score -0.000000\n",
            "[LightGBM] [Info] Number of positive: 4248, number of negative: 14952\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001493 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3276\n",
            "[LightGBM] [Info] Number of data points in the train set: 19200, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Info] Start training from score 0.000000\n",
            "[LightGBM] [Info] Number of positive: 4247, number of negative: 14953\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001460 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3269\n",
            "[LightGBM] [Info] Number of data points in the train set: 19200, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
            "[LightGBM] [Info] Start training from score -0.000000\n",
            "[LightGBM] [Info] Number of positive: 4247, number of negative: 14953\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001497 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3272\n",
            "[LightGBM] [Info] Number of data points in the train set: 19200, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
            "[LightGBM] [Info] Start training from score -0.000000\n",
            "[LightGBM] [Info] Number of positive: 4247, number of negative: 14953\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006753 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3271\n",
            "[LightGBM] [Info] Number of data points in the train set: 19200, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
            "[LightGBM] [Info] Start training from score -0.000000\n",
            "[LightGBM] [Info] Number of positive: 4247, number of negative: 14953\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002588 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3273\n",
            "[LightGBM] [Info] Number of data points in the train set: 19200, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
            "[LightGBM] [Info] Start training from score -0.000000\n",
            "[LightGBM] [Info] Number of positive: 4248, number of negative: 14952\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001445 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3276\n",
            "[LightGBM] [Info] Number of data points in the train set: 19200, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Info] Start training from score 0.000000\n",
            "[LightGBM] [Info] Number of positive: 4247, number of negative: 14953\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001438 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3269\n",
            "[LightGBM] [Info] Number of data points in the train set: 19200, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
            "[LightGBM] [Info] Start training from score -0.000000\n",
            "[LightGBM] [Info] Number of positive: 4247, number of negative: 14953\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001470 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3272\n",
            "[LightGBM] [Info] Number of data points in the train set: 19200, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
            "[LightGBM] [Info] Start training from score -0.000000\n",
            "[LightGBM] [Info] Number of positive: 4247, number of negative: 14953\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001464 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3271\n",
            "[LightGBM] [Info] Number of data points in the train set: 19200, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
            "[LightGBM] [Info] Start training from score -0.000000\n",
            "[LightGBM] [Info] Number of positive: 4247, number of negative: 14953\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001438 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3273\n",
            "[LightGBM] [Info] Number of data points in the train set: 19200, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
            "[LightGBM] [Info] Start training from score -0.000000\n",
            "[LightGBM] [Info] Number of positive: 4248, number of negative: 14952\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001510 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3276\n",
            "[LightGBM] [Info] Number of data points in the train set: 19200, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Info] Start training from score 0.000000\n",
            "[LightGBM] [Info] Number of positive: 4247, number of negative: 14953\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001512 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3269\n",
            "[LightGBM] [Info] Number of data points in the train set: 19200, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
            "[LightGBM] [Info] Start training from score -0.000000\n",
            "[LightGBM] [Info] Number of positive: 4247, number of negative: 14953\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001502 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3272\n",
            "[LightGBM] [Info] Number of data points in the train set: 19200, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
            "[LightGBM] [Info] Start training from score -0.000000\n",
            "[LightGBM] [Info] Number of positive: 4247, number of negative: 14953\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001446 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3271\n",
            "[LightGBM] [Info] Number of data points in the train set: 19200, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
            "[LightGBM] [Info] Start training from score -0.000000\n",
            "[LightGBM] [Info] Number of positive: 4247, number of negative: 14953\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001465 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3273\n",
            "[LightGBM] [Info] Number of data points in the train set: 19200, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
            "[LightGBM] [Info] Start training from score -0.000000\n",
            "[LightGBM] [Info] Number of positive: 5309, number of negative: 18691\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001774 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3276\n",
            "[LightGBM] [Info] Number of data points in the train set: 24000, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Info] Start training from score 0.000000\n",
            "[LightGBM] [Info] Number of positive: 4248, number of negative: 14952\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001457 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3276\n",
            "[LightGBM] [Info] Number of data points in the train set: 19200, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Info] Start training from score 0.000000\n",
            "[LightGBM] [Info] Number of positive: 4247, number of negative: 14953\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002808 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3269\n",
            "[LightGBM] [Info] Number of data points in the train set: 19200, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
            "[LightGBM] [Info] Start training from score -0.000000\n",
            "[LightGBM] [Info] Number of positive: 4247, number of negative: 14953\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002743 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3272\n",
            "[LightGBM] [Info] Number of data points in the train set: 19200, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
            "[LightGBM] [Info] Start training from score -0.000000\n",
            "[LightGBM] [Info] Number of positive: 4247, number of negative: 14953\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002589 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3271\n",
            "[LightGBM] [Info] Number of data points in the train set: 19200, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
            "[LightGBM] [Info] Start training from score -0.000000\n",
            "[LightGBM] [Info] Number of positive: 4247, number of negative: 14953\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001439 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3273\n",
            "[LightGBM] [Info] Number of data points in the train set: 19200, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
            "[LightGBM] [Info] Start training from score -0.000000\n",
            "[LightGBM] [Info] Number of positive: 4248, number of negative: 14952\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001454 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3276\n",
            "[LightGBM] [Info] Number of data points in the train set: 19200, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Info] Start training from score 0.000000\n",
            "[LightGBM] [Info] Number of positive: 4247, number of negative: 14953\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001438 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3269\n",
            "[LightGBM] [Info] Number of data points in the train set: 19200, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
            "[LightGBM] [Info] Start training from score -0.000000\n",
            "[LightGBM] [Info] Number of positive: 4247, number of negative: 14953\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001467 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3272\n",
            "[LightGBM] [Info] Number of data points in the train set: 19200, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
            "[LightGBM] [Info] Start training from score -0.000000\n",
            "[LightGBM] [Info] Number of positive: 4247, number of negative: 14953\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001460 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3271\n",
            "[LightGBM] [Info] Number of data points in the train set: 19200, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
            "[LightGBM] [Info] Start training from score -0.000000\n",
            "[LightGBM] [Info] Number of positive: 4247, number of negative: 14953\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001484 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3273\n",
            "[LightGBM] [Info] Number of data points in the train set: 19200, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
            "[LightGBM] [Info] Start training from score -0.000000\n",
            "[LightGBM] [Info] Number of positive: 4248, number of negative: 14952\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001454 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3276\n",
            "[LightGBM] [Info] Number of data points in the train set: 19200, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Info] Start training from score 0.000000\n",
            "[LightGBM] [Info] Number of positive: 4247, number of negative: 14953\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002092 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3269\n",
            "[LightGBM] [Info] Number of data points in the train set: 19200, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
            "[LightGBM] [Info] Start training from score -0.000000\n",
            "[LightGBM] [Info] Number of positive: 4247, number of negative: 14953\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001515 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3272\n",
            "[LightGBM] [Info] Number of data points in the train set: 19200, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
            "[LightGBM] [Info] Start training from score -0.000000\n",
            "[LightGBM] [Info] Number of positive: 4247, number of negative: 14953\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001450 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3271\n",
            "[LightGBM] [Info] Number of data points in the train set: 19200, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
            "[LightGBM] [Info] Start training from score -0.000000\n",
            "[LightGBM] [Info] Number of positive: 4247, number of negative: 14953\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001444 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3273\n",
            "[LightGBM] [Info] Number of data points in the train set: 19200, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
            "[LightGBM] [Info] Start training from score -0.000000\n",
            "[LightGBM] [Info] Number of positive: 4248, number of negative: 14952\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005796 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3276\n",
            "[LightGBM] [Info] Number of data points in the train set: 19200, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Info] Start training from score 0.000000\n",
            "[LightGBM] [Info] Number of positive: 4247, number of negative: 14953\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001678 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3269\n",
            "[LightGBM] [Info] Number of data points in the train set: 19200, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
            "[LightGBM] [Info] Start training from score -0.000000\n",
            "[LightGBM] [Info] Number of positive: 4247, number of negative: 14953\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002586 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3272\n",
            "[LightGBM] [Info] Number of data points in the train set: 19200, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
            "[LightGBM] [Info] Start training from score -0.000000\n",
            "[LightGBM] [Info] Number of positive: 4247, number of negative: 14953\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001456 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3271\n",
            "[LightGBM] [Info] Number of data points in the train set: 19200, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
            "[LightGBM] [Info] Start training from score -0.000000\n",
            "[LightGBM] [Info] Number of positive: 4247, number of negative: 14953\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001445 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3273\n",
            "[LightGBM] [Info] Number of data points in the train set: 19200, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
            "[LightGBM] [Info] Start training from score -0.000000\n",
            "[LightGBM] [Info] Number of positive: 5309, number of negative: 18691\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001801 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3276\n",
            "[LightGBM] [Info] Number of data points in the train set: 24000, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Info] Start training from score 0.000000\n",
            "Best parameters for recall_class_1: {'classifier__learning_rate': 0.01, 'classifier__n_estimators': 200}\n",
            "Best parameters for precision_class_1: {'classifier__learning_rate': 0.1, 'classifier__n_estimators': 200}\n",
            "Best parameters for recall_class_0: {'classifier__C': 0.1}\n",
            "Best parameters for precision_class_0: {'classifier__max_depth': 10, 'classifier__n_estimators': 200}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the best parameters\n",
        "for metric, params in best_params.items():\n",
        "    print(f\"Best parameters for {metric}: {params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQgFhonguJ7A",
        "outputId": "e2e96f7d-3cfc-4407-e33b-2d8357d5323c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters for recall_class_1: {'classifier__learning_rate': 0.01, 'classifier__n_estimators': 200}\n",
            "Best parameters for precision_class_1: {'classifier__learning_rate': 0.1, 'classifier__n_estimators': 200}\n",
            "Best parameters for recall_class_0: {'classifier__C': 0.1}\n",
            "Best parameters for precision_class_0: {'classifier__max_depth': 10, 'classifier__n_estimators': 200}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Meta-Model - Stacking Classifier\n",
        "A stacking classifier is an ensemble method that uses a meta-learner to combine the predictions of multiple base models. The base models are first trained on the training data, and their predictions are then used as input features for the meta-learner. The meta-learner learns the best way to combine these predictions to make the final prediction. This approach leverages the strengths of different models to achieve better overall performance."
      ],
      "metadata": {
        "id": "HQyeP_LvOtzp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.metrics import recall_score, precision_score, f1_score, classification_report\n",
        "\n",
        "# Load the best models\n",
        "best_models = joblib.load('best_models.pkl')\n",
        "\n",
        "# Create a list of (name, model) tuples for the VotingClassifier\n",
        "estimators = [\n",
        "    ('recall_class_1', best_models['recall_class_1']),\n",
        "    ('precision_class_1', best_models['precision_class_1']),\n",
        "    ('recall_class_0', best_models['recall_class_0']),\n",
        "    ('precision_class_0', best_models['precision_class_0'])\n",
        "]\n",
        "\n",
        "# Initialize the VotingClassifier with the best models\n",
        "voting_clf = VotingClassifier(estimators=estimators, voting='soft')\n",
        "\n",
        "# Fit the VotingClassifier on the training data\n",
        "voting_clf.fit(X_train, y_train)\n",
        "\n",
        "# Set threshold for classification\n",
        "THRESHOLD = 0.25\n",
        "\n",
        "# Predict probabilities\n",
        "y_proba_voting = voting_clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Apply the threshold to get the final predictions\n",
        "y_pred_voting = (y_proba_voting >= THRESHOLD).astype(int)\n",
        "\n",
        "# Evaluate the performance of the VotingClassifier\n",
        "recall_1 = recall_score(y_test, y_pred_voting, pos_label=1)\n",
        "precision_1 = precision_score(y_test, y_pred_voting, pos_label=1, zero_division=0)\n",
        "recall_0 = recall_score(y_test, y_pred_voting, pos_label=0)\n",
        "precision_0 = precision_score(y_test, y_pred_voting, pos_label=0, zero_division=0)\n",
        "f1_macro = f1_score(y_test, y_pred_voting, average='macro')\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f'Recall Class 1: {recall_1:.4f}')\n",
        "print(f'Precision Class 1: {precision_1:.4f}')\n",
        "print(f'Recall Class 0: {recall_0:.4f}')\n",
        "print(f'Precision Class 0: {precision_0:.4f}')\n",
        "print(f'F1 Macro: {f1_macro:.4f}')\n",
        "\n",
        "# Print the classification report\n",
        "print(classification_report(y_test, y_pred_voting))\n",
        "\n",
        "# Save the final VotingClassifier model\n",
        "joblib.dump(voting_clf, 'voting_classifier.pkl')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lhSqYqENQNV",
        "outputId": "535f6600-5bb2-4b51-81bc-e6a679b301d4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 5309, number of negative: 18691\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003467 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3276\n",
            "[LightGBM] [Info] Number of data points in the train set: 24000, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Info] Start training from score 0.000000\n",
            "[LightGBM] [Info] Number of positive: 5309, number of negative: 18691\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001807 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 3276\n",
            "[LightGBM] [Info] Number of data points in the train set: 24000, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Info] Start training from score 0.000000\n",
            "Recall Class 1: 0.9540\n",
            "Precision Class 1: 0.2575\n",
            "Recall Class 0: 0.2187\n",
            "Precision Class 0: 0.9437\n",
            "F1 Macro: 0.3803\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.22      0.36      4673\n",
            "           1       0.26      0.95      0.41      1327\n",
            "\n",
            "    accuracy                           0.38      6000\n",
            "   macro avg       0.60      0.59      0.38      6000\n",
            "weighted avg       0.79      0.38      0.37      6000\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['voting_classifier.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jxlDnkz5Ty4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wKl0jij6Ty1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Custom Aggregation Rules\n",
        "Custom aggregation rules involve combining the predictions of multiple models using predefined rules or heuristics. Instead of simply averaging the predictions, custom aggregation can apply different weights or thresholds to the models' predictions based on their performance. This allows for more control over how the final prediction is made, ensuring that the combined model leverages the strengths of individual models more effectively."
      ],
      "metadata": {
        "id": "zb0ayti4O2SY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict probabilities for all models\n",
        "probs_logreg = pipelines_2['LogReg_2'].predict_proba(X_test)[:, 1]\n",
        "probs_rf = pipelines_2['RF_2'].predict_proba(X_test)[:, 1]\n",
        "probs_lgbm = pipelines_2['LGBM_2'].predict_proba(X_test)[:, 1]\n",
        "probs_hgb = pipelines_2['HGB_2'].predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Combine probabilities using custom rules\n",
        "combined_probs = (probs_logreg + 2 * probs_rf + 2 * probs_lgbm + probs_hgb) / 6\n",
        "\n",
        "# Apply threshold\n",
        "final_y_pred_2 = (combined_probs >= LOW_THRESHOLD).astype(int)\n",
        "\n",
        "# Evaluate the custom aggregation approach\n",
        "recall = recall_score(y_test, final_y_pred_2, pos_label=1)\n",
        "precision = precision_score(y_test, final_y_pred_2, pos_label=1, zero_division=0)\n",
        "f1 = f1_score(y_test, final_y_pred_2, pos_label=1)\n",
        "\n",
        "print(f'Custom Aggregation Performance:')\n",
        "print(f'Recall: {recall}')\n",
        "print(f'Precision: {precision}')\n",
        "print(f'F1 Score: {f1}')\n",
        "\n",
        "# Classification report for detailed evaluation\n",
        "print(classification_report(y_test, final_y_pred_2))\n"
      ],
      "metadata": {
        "id": "uaT1M2cGOyiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Write Loan Data Utils Script"
      ],
      "metadata": {
        "id": "tk50KaY4FAJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "script_content=r'''\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "def load_data_from_url(url):\n",
        "    try:\n",
        "        df = pd.read_excel(url, header=1)\n",
        "        logging.info(\"Data loaded successfully from URL.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error loading data from URL: {e}\")\n",
        "        return None\n",
        "    return df\n",
        "\n",
        "def clean_column_names(df):\n",
        "    df.columns = [col.lower().replace(' ', '_') for col in df.columns]\n",
        "    return df\n",
        "\n",
        "def remove_id_column(df):\n",
        "    if 'id' in df.columns:\n",
        "        df = df.drop(columns=['id'])\n",
        "    return df\n",
        "\n",
        "def rename_columns(df):\n",
        "    rename_dict = {'pay_0': 'pay_1'}\n",
        "    df = df.rename(columns=rename_dict)\n",
        "    return df\n",
        "\n",
        "def convert_categorical(df, categorical_columns):\n",
        "    df[categorical_columns] = df[categorical_columns].astype('category')\n",
        "    return df\n",
        "\n",
        "def split_features_target(df, target):\n",
        "    X = df.drop(columns=[target])\n",
        "    y = df[target]\n",
        "    return X, y\n",
        "\n",
        "def load_and_preprocess_data(url, categorical_columns, target):\n",
        "    df = load_data_from_url(url)\n",
        "    if df is not None:\n",
        "        df = clean_column_names(df)\n",
        "        df = remove_id_column(df)\n",
        "        df = rename_columns(df)\n",
        "        df = convert_categorical(df, categorical_columns)\n",
        "        X, y = split_features_target(df, target)\n",
        "        return X, y\n",
        "    return None, None\n",
        "\n",
        "def plot_class_distribution(y_train, target_name):\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    sns.countplot(x=y_train, hue=y_train, palette='mako')\n",
        "    plt.title(f'Class Distribution in Training Set: {target_name}')\n",
        "    plt.xlabel('Class')\n",
        "    plt.ylabel('Count')\n",
        "    plt.legend([], [], frameon=False)\n",
        "\n",
        "    # Calculate the percentage for each class\n",
        "    total = len(y_train)\n",
        "    class_counts = y_train.value_counts()\n",
        "    for i, count in enumerate(class_counts):\n",
        "        percentage = 100 * count / total\n",
        "        plt.text(i, count, f'{percentage:.1f}%', ha='center', va='bottom')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "# Write the script to a file\n",
        "with open(\"loan_data_utils.py\", \"w\") as file:\n",
        "    file.write(script_content)\n",
        "\n",
        "print(\"Script successfully written to loan_data_utils.py\")\n",
        "# Reload script to make functions available for use\n",
        "import importlib\n",
        "import loan_data_utils\n",
        "importlib.reload(loan_data_utils)\n",
        "\n",
        "from loan_data_utils import *\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83HsNhnoNXlh",
        "outputId": "3ac0627d-58e8-4228-c232-e51b4ee6ef9c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Script successfully written to loan_data_utils.py\n"
          ]
        }
      ]
    }
  ]
}