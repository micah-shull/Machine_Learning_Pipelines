{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPryaFvuO4t7wZa2XKyWToW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/pipelines/blob/main/pipelines_16_ensemble_048_optimal_thresholds_ROC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ROC Threshold Tuning Notebook\n",
        "\n",
        "# 1. Load and Preprocess Data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "import warnings\n",
        "from loan_data_utils import load_and_preprocess_data\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "# Define your URL, categorical columns, and target\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\"\n",
        "categorical_columns = ['sex', 'education', 'marriage']\n",
        "target = 'default_payment_next_month'\n",
        "\n",
        "# Load and preprocess data\n",
        "X, y = load_and_preprocess_data(url, categorical_columns, target)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Identify numeric and categorical columns\n",
        "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "categorical_features = X.select_dtypes(include=['category']).columns.tolist()\n",
        "\n",
        "# Define the column transformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', Pipeline(steps=[\n",
        "            ('imputer', SimpleImputer(strategy='median')),\n",
        "            ('scaler', StandardScaler())\n",
        "        ]), numeric_features),\n",
        "        ('cat', Pipeline(steps=[\n",
        "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "            ('encoder', OneHotEncoder(drop='first'))\n",
        "        ]), categorical_features)\n",
        "    ])\n",
        "\n",
        "# 2. Load Optimal Parameters and Thresholds\n",
        "import json\n",
        "\n",
        "# Load optimal parameters and thresholds\n",
        "filename = 'optimal_model_params_thresholds_roc.json'\n",
        "with open(filename, 'r') as file:\n",
        "    best_params = json.load(file)\n",
        "\n",
        "# 3. Define Models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, StackingClassifier\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from imblearn.over_sampling import SMOTE, ADASYN\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "\n",
        "def define_models(best_params):\n",
        "    models = {\n",
        "        \"Logistic Regression (ADASYN)\": ImbPipeline(steps=[\n",
        "            ('preprocessor', preprocessor),\n",
        "            ('resampler', ADASYN()),\n",
        "            ('classifier', LogisticRegression(random_state=42, **best_params['Class 1 Recall']['Logistic Regression (ADASYN)']['best_params']))\n",
        "        ]),\n",
        "        \"Logistic Regression (SMOTE)\": ImbPipeline(steps=[\n",
        "            ('preprocessor', preprocessor),\n",
        "            ('resampler', SMOTE()),\n",
        "            ('classifier', LogisticRegression(random_state=42, **best_params['Class 1 Recall']['Logistic Regression (SMOTE)']['best_params']))\n",
        "        ]),\n",
        "        \"LGBM (SMOTE)\": ImbPipeline(steps=[\n",
        "            ('preprocessor', preprocessor),\n",
        "            ('resampler', SMOTE()),\n",
        "            ('classifier', LGBMClassifier(random_state=42, **best_params['Class 1 Recall']['LGBM (SMOTE)']['best_params'], force_row_wise=True))\n",
        "        ]),\n",
        "        \"Logistic Regression (baseline)\": Pipeline(steps=[\n",
        "            ('preprocessor', preprocessor),\n",
        "            ('classifier', LogisticRegression(random_state=42, **best_params['Class 1 Precision']['Logistic Regression (baseline)']['best_params']))\n",
        "        ]),\n",
        "        \"LGBM (baseline)\": Pipeline(steps=[\n",
        "            ('preprocessor', preprocessor),\n",
        "            ('classifier', LGBMClassifier(random_state=42, **best_params['Class 1 Precision']['LGBM (baseline)']['best_params'], force_row_wise=True))\n",
        "        ]),\n",
        "        \"Random Forest (class_weight_balanced)\": Pipeline(steps=[\n",
        "            ('preprocessor', preprocessor),\n",
        "            ('classifier', RandomForestClassifier(random_state=42, class_weight='balanced', **best_params['Class 1 Precision']['Random Forest (class_weight_balanced)']['best_params']))\n",
        "        ]),\n",
        "        \"Logistic Regression (baseline for Class 0 Recall)\": Pipeline(steps=[\n",
        "            ('preprocessor', preprocessor),\n",
        "            ('classifier', LogisticRegression(random_state=42, **best_params['Class 0 Recall']['Logistic Regression (baseline)']['best_params']))\n",
        "        ]),\n",
        "        \"LGBM (baseline for Class 0 Recall)\": Pipeline(steps=[\n",
        "            ('preprocessor', preprocessor),\n",
        "            ('classifier', LGBMClassifier(random_state=42, **best_params['Class 0 Recall']['LGBM (baseline)']['best_params'], force_row_wise=True))\n",
        "        ]),\n",
        "        \"Random Forest (class_weight_balanced for Class 0 Recall)\": Pipeline(steps=[\n",
        "            ('preprocessor', preprocessor),\n",
        "            ('classifier', RandomForestClassifier(random_state=42, class_weight='balanced', **best_params['Class 0 Recall']['Random Forest (class_weight_balanced)']['best_params']))\n",
        "        ]),\n",
        "        \"LGBM (RandomUnderSampler)\": ImbPipeline(steps=[\n",
        "            ('preprocessor', preprocessor),\n",
        "            ('resampler', RandomUnderSampler()),\n",
        "            ('classifier', LGBMClassifier(random_state=42, **best_params['Class 0 Precision']['LGBM (RandomUnderSampler)']['best_params'], force_row_wise=True))\n",
        "        ]),\n",
        "        \"HistGradientBoosting (RandomUnderSampler)\": ImbPipeline(steps=[\n",
        "            ('preprocessor', preprocessor),\n",
        "            ('resampler', RandomUnderSampler()),\n",
        "            ('classifier', HistGradientBoostingClassifier(random_state=42, **best_params['Class 0 Precision']['HistGradientBoosting (RandomUnderSampler)']['best_params']))\n",
        "        ]),\n",
        "        \"Random Forest (RandomUnderSampler)\": ImbPipeline(steps=[\n",
        "            ('preprocessor', preprocessor),\n",
        "            ('resampler', RandomUnderSampler()),\n",
        "            ('classifier', RandomForestClassifier(random_state=42, **best_params['Class 0 Precision']['Random Forest (RandomUnderSampler)']['best_params']))\n",
        "        ])\n",
        "    }\n",
        "    return models\n",
        "\n",
        "models = define_models(best_params)\n",
        "\n",
        "# 4. Train and Evaluate Voting and Stacking Classifiers\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def save_results(results, filename):\n",
        "    # Strip 'optimal_model_params_thresholds_' and '.json' from the filename\n",
        "    method_name = filename.replace('optimal_model_params_thresholds_', '').replace('.json', '')\n",
        "    results_filename = f'voting_stacking_results_{method_name}.json'\n",
        "\n",
        "    # Save the results to a JSON file\n",
        "    with open(results_filename, 'w') as file:\n",
        "        json.dump(results, file, indent=4)\n",
        "\n",
        "def train_and_evaluate_voting_stacking(models, X_train, y_train, X_test, y_test, filename):\n",
        "    voting_clf = VotingClassifier(estimators=list(models.items()), voting='soft')\n",
        "    stacking_clf = StackingClassifier(estimators=list(models.items()), final_estimator=LogisticRegression())\n",
        "\n",
        "    # Train and evaluate the Voting classifier\n",
        "    voting_clf.fit(X_train, y_train)\n",
        "    y_pred_voting = voting_clf.predict(X_test)\n",
        "    print(f'Voting Classifier Performance:')\n",
        "    print(classification_report(y_test, y_pred_voting))\n",
        "\n",
        "    # Train and evaluate the Stacking classifier\n",
        "    stacking_clf.fit(X_train, y_train)\n",
        "    y_pred_stacking = stacking_clf.predict(X_test)\n",
        "    print(f'Stacking Classifier Performance:')\n",
        "    print(classification_report(y_test, y_pred_stacking))\n",
        "\n",
        "    # Prepare results\n",
        "    results = {\n",
        "        'voting': classification_report(y_test, y_pred_voting, output_dict=True),\n",
        "        'stacking': classification_report(y_test, y_pred_stacking, output_dict=True)\n",
        "    }\n",
        "\n",
        "    # Save the results\n",
        "    save_results(results, filename)\n",
        "\n",
        "# Define models\n",
        "models = define_models(best_params)\n",
        "\n",
        "# Train and evaluate classifiers\n",
        "train_and_evaluate_voting_stacking(models, X_train, y_train, X_test, y_test, filename)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8GEBPu0iL41",
        "outputId": "503fc3b0-1b6f-4e18-f8d4-340e2bac63d9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 18691, number of negative: 18691\n",
            "[LightGBM] [Info] Total Bins 6544\n",
            "[LightGBM] [Info] Number of data points in the train set: 37382, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Info] Number of positive: 5309, number of negative: 18691\n",
            "[LightGBM] [Info] Total Bins 3276\n",
            "[LightGBM] [Info] Number of data points in the train set: 24000, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221208 -> initscore=-1.258639\n",
            "[LightGBM] [Info] Start training from score -1.258639\n",
            "[LightGBM] [Info] Number of positive: 5309, number of negative: 18691\n",
            "[LightGBM] [Info] Total Bins 3276\n",
            "[LightGBM] [Info] Number of data points in the train set: 24000, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221208 -> initscore=-1.258639\n",
            "[LightGBM] [Info] Start training from score -1.258639\n",
            "[LightGBM] [Info] Number of positive: 5309, number of negative: 5309\n",
            "[LightGBM] [Info] Total Bins 3264\n",
            "[LightGBM] [Info] Number of data points in the train set: 10618, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "Voting Classifier Performance:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.92      0.88      4673\n",
            "           1       0.60      0.44      0.51      1327\n",
            "\n",
            "    accuracy                           0.81      6000\n",
            "   macro avg       0.72      0.68      0.69      6000\n",
            "weighted avg       0.80      0.81      0.80      6000\n",
            "\n",
            "[LightGBM] [Info] Number of positive: 18691, number of negative: 18691\n",
            "[LightGBM] [Info] Total Bins 6539\n",
            "[LightGBM] [Info] Number of data points in the train set: 37382, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Info] Number of positive: 5309, number of negative: 18691\n",
            "[LightGBM] [Info] Total Bins 3276\n",
            "[LightGBM] [Info] Number of data points in the train set: 24000, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221208 -> initscore=-1.258639\n",
            "[LightGBM] [Info] Start training from score -1.258639\n",
            "[LightGBM] [Info] Number of positive: 5309, number of negative: 18691\n",
            "[LightGBM] [Info] Total Bins 3276\n",
            "[LightGBM] [Info] Number of data points in the train set: 24000, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221208 -> initscore=-1.258639\n",
            "[LightGBM] [Info] Start training from score -1.258639\n",
            "[LightGBM] [Info] Number of positive: 5309, number of negative: 5309\n",
            "[LightGBM] [Info] Total Bins 3263\n",
            "[LightGBM] [Info] Number of data points in the train set: 10618, number of used features: 29\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Info] Number of positive: 14952, number of negative: 14952\n",
            "[LightGBM] [Info] Total Bins 6556\n",
            "[LightGBM] [Info] Number of data points in the train set: 29904, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Info] Number of positive: 14953, number of negative: 14953\n",
            "[LightGBM] [Info] Total Bins 6496\n",
            "[LightGBM] [Info] Number of data points in the train set: 29906, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Info] Number of positive: 14953, number of negative: 14953\n",
            "[LightGBM] [Info] Total Bins 6526\n",
            "[LightGBM] [Info] Number of data points in the train set: 29906, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Info] Number of positive: 14953, number of negative: 14953\n",
            "[LightGBM] [Info] Total Bins 6553\n",
            "[LightGBM] [Info] Number of data points in the train set: 29906, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Info] Number of positive: 14953, number of negative: 14953\n",
            "[LightGBM] [Info] Total Bins 6558\n",
            "[LightGBM] [Info] Number of data points in the train set: 29906, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Info] Number of positive: 4248, number of negative: 14952\n",
            "[LightGBM] [Info] Total Bins 3276\n",
            "[LightGBM] [Info] Number of data points in the train set: 19200, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221250 -> initscore=-1.258397\n",
            "[LightGBM] [Info] Start training from score -1.258397\n",
            "[LightGBM] [Info] Number of positive: 4247, number of negative: 14953\n",
            "[LightGBM] [Info] Total Bins 3269\n",
            "[LightGBM] [Info] Number of data points in the train set: 19200, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221198 -> initscore=-1.258699\n",
            "[LightGBM] [Info] Start training from score -1.258699\n",
            "[LightGBM] [Info] Number of positive: 4247, number of negative: 14953\n",
            "[LightGBM] [Info] Total Bins 3272\n",
            "[LightGBM] [Info] Number of data points in the train set: 19200, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221198 -> initscore=-1.258699\n",
            "[LightGBM] [Info] Start training from score -1.258699\n",
            "[LightGBM] [Info] Number of positive: 4247, number of negative: 14953\n",
            "[LightGBM] [Info] Total Bins 3271\n",
            "[LightGBM] [Info] Number of data points in the train set: 19200, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221198 -> initscore=-1.258699\n",
            "[LightGBM] [Info] Start training from score -1.258699\n",
            "[LightGBM] [Info] Number of positive: 4247, number of negative: 14953\n",
            "[LightGBM] [Info] Total Bins 3273\n",
            "[LightGBM] [Info] Number of data points in the train set: 19200, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221198 -> initscore=-1.258699\n",
            "[LightGBM] [Info] Start training from score -1.258699\n",
            "[LightGBM] [Info] Number of positive: 4248, number of negative: 14952\n",
            "[LightGBM] [Info] Total Bins 3276\n",
            "[LightGBM] [Info] Number of data points in the train set: 19200, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221250 -> initscore=-1.258397\n",
            "[LightGBM] [Info] Start training from score -1.258397\n",
            "[LightGBM] [Info] Number of positive: 4247, number of negative: 14953\n",
            "[LightGBM] [Info] Total Bins 3269\n",
            "[LightGBM] [Info] Number of data points in the train set: 19200, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221198 -> initscore=-1.258699\n",
            "[LightGBM] [Info] Start training from score -1.258699\n",
            "[LightGBM] [Info] Number of positive: 4247, number of negative: 14953\n",
            "[LightGBM] [Info] Total Bins 3272\n",
            "[LightGBM] [Info] Number of data points in the train set: 19200, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221198 -> initscore=-1.258699\n",
            "[LightGBM] [Info] Start training from score -1.258699\n",
            "[LightGBM] [Info] Number of positive: 4247, number of negative: 14953\n",
            "[LightGBM] [Info] Total Bins 3271\n",
            "[LightGBM] [Info] Number of data points in the train set: 19200, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221198 -> initscore=-1.258699\n",
            "[LightGBM] [Info] Start training from score -1.258699\n",
            "[LightGBM] [Info] Number of positive: 4247, number of negative: 14953\n",
            "[LightGBM] [Info] Total Bins 3273\n",
            "[LightGBM] [Info] Number of data points in the train set: 19200, number of used features: 30\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221198 -> initscore=-1.258699\n",
            "[LightGBM] [Info] Start training from score -1.258699\n",
            "[LightGBM] [Info] Number of positive: 4248, number of negative: 4248\n",
            "[LightGBM] [Info] Total Bins 3257\n",
            "[LightGBM] [Info] Number of data points in the train set: 8496, number of used features: 29\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Info] Number of positive: 4247, number of negative: 4247\n",
            "[LightGBM] [Info] Total Bins 3256\n",
            "[LightGBM] [Info] Number of data points in the train set: 8494, number of used features: 29\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Info] Number of positive: 4247, number of negative: 4247\n",
            "[LightGBM] [Info] Total Bins 3257\n",
            "[LightGBM] [Info] Number of data points in the train set: 8494, number of used features: 29\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Info] Number of positive: 4247, number of negative: 4247\n",
            "[LightGBM] [Info] Total Bins 3259\n",
            "[LightGBM] [Info] Number of data points in the train set: 8494, number of used features: 29\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Info] Number of positive: 4247, number of negative: 4247\n",
            "[LightGBM] [Info] Total Bins 3255\n",
            "[LightGBM] [Info] Number of data points in the train set: 8494, number of used features: 28\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "Stacking Classifier Performance:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.94      0.89      4673\n",
            "           1       0.65      0.38      0.48      1327\n",
            "\n",
            "    accuracy                           0.82      6000\n",
            "   macro avg       0.74      0.66      0.68      6000\n",
            "weighted avg       0.80      0.82      0.80      6000\n",
            "\n"
          ]
        }
      ]
    }
  ]
}