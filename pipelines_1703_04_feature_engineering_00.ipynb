{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/pipelines/blob/main/pipelines_1703_04_feature_engineering_00.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Engineering Otpions\n",
        "\n",
        "### **Option 1: Integrate Feature Engineering into the ColumnTransformer**\n",
        "\n",
        "In this approach, you place each feature engineering step into the appropriate transformer within the `ColumnTransformer`. This means:\n",
        "\n",
        "- **Numeric Features Transformer:** Feature engineering steps that involve numeric data (like interactions, ratios, binning) should be placed within the numeric transformer pipeline.\n",
        "- **Categorical Features Transformer:** Any feature engineering involving categorical data (though less common) should be placed within the categorical transformer pipeline.\n",
        "\n",
        "Here’s how it looks:\n",
        "\n",
        "```python\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', Pipeline(steps=[\n",
        "            ('imputer', SimpleImputer(strategy='median')),\n",
        "            ('feat_eng', InteractionFeatures(numeric_features=numeric_features)),  # Apply interaction features\n",
        "            ('scaler', StandardScaler())\n",
        "        ]), make_column_selector(dtype_include=['int64', 'float64'])),\n",
        "        \n",
        "        ('cat', Pipeline(steps=[\n",
        "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "            ('encoder', OneHotEncoder(drop='first'))\n",
        "            # Add categorical feature engineering steps here if any\n",
        "        ]), categorical_features)\n",
        "    ])\n",
        "```\n",
        "\n",
        "### **Option 2: Separate Feature Engineering Pipeline**\n",
        "\n",
        "In this approach, you create a separate pipeline for feature engineering that operates on the entire dataset before passing the data to the `ColumnTransformer` for preprocessing. This feature engineering pipeline would operate on the raw data, which may include missing values or unencoded categorical data.\n",
        "\n",
        "Here’s the breakdown:\n",
        "\n",
        "- **Feature Engineering Pipeline:** Applied first, operates over raw data, potentially creating new features that are then handled by the preprocessing steps.\n",
        "- **Preprocessing:** Applied after feature engineering, handling imputation, scaling, encoding, etc.\n",
        "\n",
        "```python\n",
        "# Feature engineering pipeline\n",
        "feature_eng_pipeline = Pipeline(steps=[\n",
        "    ('feat_eng', InteractionFeatures(numeric_features=numeric_features)),\n",
        "    # Add other feature engineering steps here\n",
        "])\n",
        "\n",
        "# Apply feature engineering first\n",
        "X_train_eng = feature_eng_pipeline.fit_transform(X_train_res)\n",
        "X_test_eng = feature_eng_pipeline.transform(X_test)\n",
        "\n",
        "# Then proceed with the rest of the preprocessing\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', Pipeline(steps=[\n",
        "            ('imputer', SimpleImputer(strategy='median')),\n",
        "            ('scaler', StandardScaler())\n",
        "        ]), make_column_selector(dtype_include=['int64', 'float64'])),\n",
        "        \n",
        "        ('cat', Pipeline(steps=[\n",
        "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "            ('encoder', OneHotEncoder(drop='first'))\n",
        "        ]), categorical_features)\n",
        "    ])\n",
        "```\n",
        "\n",
        "### **Which Method to Recommend?**\n",
        "\n",
        "#### **Option 1: Integrating into ColumnTransformer**\n",
        "**Pros:**\n",
        "- **Consistency:** Ensures that feature engineering is integrated with the preprocessing pipeline, keeping data handling consistent.\n",
        "- **Simplicity:** All transformations are managed within the same framework, reducing complexity.\n",
        "- **Controlled Processing:** Since preprocessing (e.g., imputation) happens before feature engineering, you avoid issues related to missing values.\n",
        "\n",
        "**Cons:**\n",
        "- **Limited Scope:** Feature engineering must be applied to specific columns and can’t easily operate across different data types or on the whole dataset.\n",
        "\n",
        "#### **Option 2: Separate Feature Engineering Pipeline**\n",
        "**Pros:**\n",
        "- **Flexibility:** Allows complex feature engineering that might involve multiple columns or different data types.\n",
        "- **Exploratory Power:** You can create complex features before handling missing values or encoding, potentially discovering useful interactions.\n",
        "- **Sequential Processing:** You have clear control over the order of transformations, which can be useful for complex datasets.\n",
        "\n",
        "**Cons:**\n",
        "- **Handling Missing Values:** You may need to carefully manage missing values since they exist before feature engineering.\n",
        "- **Complexity:** Adds an additional step in the pipeline, making the process slightly more complex.\n",
        "\n",
        "### **Recommendation:**\n",
        "\n",
        "If your feature engineering steps are relatively straightforward and only involve numeric data, **Option 1** is usually the better choice. It keeps everything neatly within the `ColumnTransformer`, ensuring consistency in processing.\n",
        "\n",
        "However, if your feature engineering involves complex interactions across different types of data or requires creating features before any preprocessing, **Option 2** would be the preferred approach. It provides more flexibility and control, especially for more advanced feature engineering tasks.\n",
        "\n",
        "For your case, if you’re mostly working with numeric transformations like interactions, ratios, and binning, **Option 1** is likely sufficient and will keep your pipeline cleaner."
      ],
      "metadata": {
        "id": "Ru0n07Y6wnom"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load & Preprocess Baseline Performance"
      ],
      "metadata": {
        "id": "gkaYMac3ec_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures, OrdinalEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.decomposition import PCA, FactorAnalysis\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import json\n",
        "import warnings\n",
        "from loan_data_utils import load_and_preprocess_data, plot_classification_report_metrics, ThresholdClassifier\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "# Define your URL, categorical columns, and target\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\"\n",
        "categorical_columns = ['sex', 'marriage', 'education']\n",
        "target = 'default_payment_next_month'\n",
        "\n",
        "# Load and preprocess data\n",
        "X, y = load_and_preprocess_data(url, categorical_columns, target)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Identify numeric and categorical columns\n",
        "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "categorical_columns = ['sex', 'marriage']\n",
        "# Define the custom ordering for the 'education' variable as ordered category\n",
        "education_order = [0, 1, 2, 3, 4, 5, 6]\n",
        "\n",
        "# Define the preprocessing steps\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', Pipeline(steps=[\n",
        "            ('imputer', SimpleImputer(strategy='median')),\n",
        "            ('scaler', StandardScaler())\n",
        "        ]), numeric_features),\n",
        "        ('education_ord', Pipeline(steps=[\n",
        "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "            ('ordinal_encoder', OrdinalEncoder(categories=[education_order]))\n",
        "        ]), ['education']),  # Treat education separately with ordinal encoding\n",
        "        ('cat', Pipeline(steps=[\n",
        "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "            ('encoder', OneHotEncoder(drop='first'))\n",
        "        ]), categorical_columns)\n",
        "    ])\n",
        "\n",
        "# Fit and transform the data\n",
        "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
        "X_test_preprocessed = preprocessor.transform(X_test)\n",
        "\n",
        "# Apply SMOTE after preprocessing\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_res, y_train_res = smote.fit_resample(X_train_preprocessed, y_train)\n",
        "\n",
        "# Use the custom classifier with Logistic Regression and a threshold of 0.3 for class 1\n",
        "base_classifier = LogisticRegression(max_iter=5000)\n",
        "classifier = ThresholdClassifier(base_classifier, threshold=0.3)\n",
        "\n",
        "# Train the classifier on the resampled data\n",
        "classifier.fit(X_train_res, y_train_res)\n",
        "\n",
        "# Predict and evaluate on the preprocessed test set\n",
        "y_pred = classifier.predict(X_test_preprocessed)\n",
        "baseline_report = classification_report(y_test, y_pred, output_dict=True)\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Initialize a dictionary to store the classification reports\n",
        "experiment_results = {}\n",
        "\n",
        "# Add the baseline report to the dictionary\n",
        "experiment_results['baseline'] = baseline_report\n",
        "\n",
        "# Define the path and filename for the JSON file\n",
        "json_filename = 'classification_reports_feature_engineering.json'\n",
        "\n",
        "# Function to save the experiment results to a JSON file\n",
        "def save_experiment_results(results, filename):\n",
        "    with open(filename, 'w') as json_file:\n",
        "        json.dump(results, json_file, indent=4)\n",
        "    print(f'Experiment results saved to {filename}')\n",
        "\n",
        "# Save the baseline report to the JSON file\n",
        "save_experiment_results(experiment_results, json_filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QboQbqiwg1rI",
        "outputId": "f7222e22-72a5-42d0-a625-4b13c484e8ec"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.24      0.38      4673\n",
            "           1       0.25      0.88      0.39      1327\n",
            "\n",
            "    accuracy                           0.38      6000\n",
            "   macro avg       0.56      0.56      0.38      6000\n",
            "weighted avg       0.74      0.38      0.38      6000\n",
            "\n",
            "Experiment results saved to classification_reports_feature_engineering.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Histograms to determine Bin Size"
      ],
      "metadata": {
        "id": "Ty1DzG6BgdzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from loan_data_utils import plot_feature_groups\n",
        "\n",
        "# # Define groups of features\n",
        "# feature_groups = {\n",
        "#     'Bill Amount': ['bill_amt1', 'bill_amt2', 'bill_amt3', 'bill_amt4', 'bill_amt5', 'bill_amt6'],\n",
        "#     'Pay': ['pay_1', 'pay_2', 'pay_3', 'pay_4', 'pay_5', 'pay_6'],\n",
        "#     'Pay Amount': ['pay_amt1', 'pay_amt2', 'pay_amt3', 'pay_amt4', 'pay_amt5', 'pay_amt6']\n",
        "# }\n",
        "\n",
        "# # Apply the function to your preprocessed DataFrame, coloring by 'sex' or 'marriage'\n",
        "# plot_feature_groups(X_train, feature_groups, hue='sex')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Jn8i3F2LYI2m"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Engineering\n",
        "\n",
        "Setting up the feature engineering steps in a pipeline is a best practice to ensure that you avoid data leakage and apply the transformations separately to the training and test data.\n",
        "\n",
        "### Key Points on Data Leakage and Pipelines:\n",
        "\n",
        "1. **Data Leakage**:\n",
        "   - **What It Is**: Data leakage occurs when information from outside the training dataset is used to create the model. This can happen if data from the test set influences the training process, leading to overly optimistic performance estimates.\n",
        "   - **Why It Matters**: Leakage can cause your model to perform well during development but fail when deployed in a real-world scenario.\n",
        "\n",
        "2. **Pipelines Prevent Data Leakage**:\n",
        "   - **Transformations Applied Separately**: When you use a pipeline, transformations (like scaling, encoding, or feature engineering) are applied separately to the training and test sets. This means that when fitting the pipeline, it only has access to the training data, and it applies the learned transformations to the test data without refitting.\n",
        "   - **No Data Leakage**: By ensuring that each step of preprocessing and feature engineering is encapsulated in a pipeline, you prevent any possibility of accidentally allowing the test data to influence the training process.\n",
        "\n",
        "3. **Application to Train and Test Data**:\n",
        "   - **Train Data**: The pipeline is first fit on the training data, learning any necessary parameters (like the mean and standard deviation for scaling).\n",
        "   - **Test Data**: The same pipeline is then applied to the test data, but it does not refit on the test data. Instead, it uses the parameters learned from the training data to transform the test data.\n",
        "\n",
        "\n",
        "### Why This Matters:\n",
        "- **Separate Fitting**: The `.fit_transform()` method is used on the training data to learn the transformations. The `.transform()` method is used on the test data, applying the same transformations without refitting.\n",
        "- **Consistency**: This approach ensures that the test data is treated in exactly the same way as the training data, but without any knowledge of the test data influencing the training process.\n",
        "\n",
        "### Summary:\n",
        "Using pipelines for feature engineering (and other preprocessing steps) is crucial for avoiding data leakage and ensuring that your model's performance estimates are accurate and generalizable to unseen data. By following this approach, you ensure that the training and test data are handled separately, with no cross-contamination.\n",
        "\n",
        "### Explanation for Splitting Feature Engineering into Two Separate Pipelines:\n",
        "\n",
        "In this notebook, we split the feature engineering into two separate pipelines to handle different types of transformations more effectively:\n",
        "\n",
        "1. **Interaction Pipeline (First Pipeline)**:\n",
        "   - **Purpose**: The first pipeline is dedicated to generating interaction terms between specific features. Interaction terms are created by multiplying pairs of features, capturing the combined effect of these features on the target variable.\n",
        "   - **Reason**: By applying the interaction terms first, we ensure that these terms are created based on the original feature values before any transformations (like binning or scaling) are applied. This preserves the raw relationships between the features, which is crucial for capturing meaningful interactions.\n",
        "\n",
        "2. **Feature Engineering Pipeline (Second Pipeline)**:\n",
        "   - **Purpose**: The second pipeline handles other feature engineering steps, such as creating ratio features, binning, and scaling. These transformations modify the original features to enhance model performance.\n",
        "   - **Reason**: Applying these transformations after creating interaction terms ensures that the interaction terms themselves are not distorted by the subsequent feature engineering steps. Additionally, this approach allows us to apply different types of transformations in a controlled and sequential manner, ensuring that each step builds on the previous one.\n",
        "\n",
        "### Summary:\n",
        "- **Sequential Application**: Splitting the feature engineering into two pipelines allows us to apply interaction terms on the raw data first and then proceed with other transformations. This sequence preserves the integrity of the original features and ensures that the generated interaction terms are meaningful and consistent.\n",
        "- **Avoiding Issues**: By splitting the pipelines, we avoid issues such as missing columns due to transformations that could alter or remove the original features, ensuring that all necessary features are available for each step of the pipeline.\n",
        "\n",
        "This structure enhances the robustness and clarity of the feature engineering process, making it easier to understand, debug, and maintain."
      ],
      "metadata": {
        "id": "vlTCHUxX91sZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from loan_data_feature_engineering import RatioFeatures, BinningFeatures, SelectiveBinningFeaturesOneHot, CustomInteractionFeatures\n",
        "\n",
        "# Convert the preprocessed training data back to DataFrame\n",
        "\n",
        "# Get the feature names from the preprocessor\n",
        "numeric_feature_names = numeric_features\n",
        "ordinal_feature_names = ['education']  # Since it's now processed as an ordinal category\n",
        "onehot_feature_names = list(preprocessor.named_transformers_['cat'].named_steps['encoder'].get_feature_names_out(categorical_columns))\n",
        "\n",
        "# Combine the feature names\n",
        "all_feature_names = numeric_feature_names + ordinal_feature_names + onehot_feature_names\n",
        "\n",
        "# Convert the preprocessed data to DataFrames with the correct feature names\n",
        "X_train_preprocessed_df = pd.DataFrame(X_train_preprocessed, columns=all_feature_names)\n",
        "X_test_preprocessed_df = pd.DataFrame(X_test_preprocessed, columns=all_feature_names)\n",
        "\n",
        "# Define your custom interaction pairs\n",
        "interaction_pairs = [\n",
        "    ('limit_bal', 'age'),\n",
        "    ('limit_bal', 'pay_1'),\n",
        "    ('limit_bal', 'pay_2'),\n",
        "    ('bill_amt1', 'pay_amt1'),\n",
        "    ('marriage_1', 'pay_1'),\n",
        "    ('education_5', 'pay_1'),\n",
        "    ('sex_2', 'pay_1'),\n",
        "    ('marriage_1', 'pay_2'),\n",
        "    ('sex_2', 'marriage_2'),\n",
        "]\n",
        "\n",
        "# Step 1: Apply Interaction Features First\n",
        "interaction_pipeline = Pipeline(steps=[\n",
        "    ('interaction', CustomInteractionFeatures(interaction_pairs=interaction_pairs))\n",
        "])\n",
        "\n",
        "# Step 2: Then Apply Feature Engineering Pipeline\n",
        "features_to_bin = ['bill_amt1', 'pay_amt1', 'limit_bal']\n",
        "\n",
        "feature_engineering_pipeline = Pipeline(steps=[\n",
        "    ('ratio', RatioFeatures()),\n",
        "    ('binning', SelectiveBinningFeaturesOneHot(features_to_bin, n_bins=5, encode='onehot', strategy='quantile'))\n",
        "])\n",
        "\n",
        "# Combine Interaction Pipeline with the Feature Engineering Pipeline\n",
        "full_pipeline = Pipeline(steps=[\n",
        "    ('interaction', interaction_pipeline),\n",
        "    ('feature_engineering', feature_engineering_pipeline)\n",
        "])\n",
        "\n",
        "# Apply the full pipeline to the training data\n",
        "X_train_fe = full_pipeline.fit_transform(X_train_preprocessed_df)\n",
        "X_test_fe = full_pipeline.transform(X_test_preprocessed_df)\n",
        "\n",
        "# Apply SMOTE after feature engineering\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_res, y_train_res = smote.fit_resample(X_train_fe, y_train)\n",
        "\n",
        "# Now you can proceed with model training\n",
        "classifier = LogisticRegression(max_iter=5000, solver='saga', C=0.1)\n",
        "classifier.fit(X_train_res, y_train_res)\n",
        "\n",
        "# Evaluate on the test set\n",
        "y_pred_fe = classifier.predict(X_test_fe)\n",
        "fe_report = classification_report(y_test, y_pred_fe, output_dict=True)\n",
        "print(classification_report(y_test, y_pred_fe))\n",
        "\n",
        "# Add the current experiment report to the dictionary\n",
        "experiment_results['feature_engineering'] = fe_report\n",
        "\n",
        "# Save the updated experiment results to the JSON file\n",
        "save_experiment_results(experiment_results, json_filename)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74kqdnp_ERfK",
        "outputId": "6499d838-f8c8-46db-8488-959744316a7c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.77      0.82      4673\n",
            "           1       0.44      0.62      0.51      1327\n",
            "\n",
            "    accuracy                           0.74      6000\n",
            "   macro avg       0.66      0.69      0.67      6000\n",
            "weighted avg       0.78      0.74      0.75      6000\n",
            "\n",
            "Experiment results saved to classification_reports_feature_engineering.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Feature Engineering Script\n",
        "\n",
        "When you define arguments in the `__init__` method of a class, those arguments are only available within the `__init__` method unless you explicitly attach them to the instance of the class using `self`.\n",
        "\n",
        "Here's a breakdown:\n",
        "\n",
        "### What Happens in `__init__`:\n",
        "\n",
        "1. **Arguments Defined in `__init__`**:\n",
        "   - When you define arguments like `degree`, `interaction_only`, and `include_bias` in the `__init__` method, they are passed when the class is instantiated.\n",
        "   - However, these arguments are local to the `__init__` method unless you assign them to `self`.\n",
        "\n",
        "2. **Assigning to `self`**:\n",
        "   - By using `self.degree = degree`, you are attaching the value of `degree` to the instance of the class. This means that `self.degree` will be available throughout the instance's lifecycle and can be accessed in other methods like `fit` and `transform`.\n",
        "\n",
        "3. **Why This is Important**:\n",
        "   - Scikit-learn relies on being able to retrieve the parameters of a transformer (or any estimator) using `get_params` for things like cross-validation, pipeline construction, and hyperparameter tuning.\n",
        "\n",
        "\n",
        "### Summary:\n",
        "- **`self.degree = degree`**: This makes `degree` available throughout the instance, not just in `__init__`.\n",
        "- **Access in Other Methods**: Other methods, like `fit` and `transform`, can then use `self.degree` (and the other parameters) to perform their operations.\n",
        "\n",
        "This is a crucial part of working with classes in Python, especially when creating custom transformers or estimators for scikit-learn.\n",
        "\n",
        "### Benefits of This Approach:\n",
        "\n",
        "1. **Clarity**: By explicitly assigning attributes, it's easier to see and understand the parameters that control the behavior of each transformer.\n",
        "2. **Flexibility**: If you need to reference these parameters later in the class (or outside the class), they are readily available.\n",
        "3. **Debugging**: Makes it easier to debug and inspect the state of each transformer, as you can quickly check the values of `self.degree`, `self.n_bins`, etc.\n"
      ],
      "metadata": {
        "id": "dtbfwu-QsJ82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "script_content=r'''\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.preprocessing import KBinsDiscretizer, PolynomialFeatures\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "# ------ Ratio Features ------ #\n",
        "\n",
        "class RatioFeatures(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        X['bill_ratio'] = X['bill_amt1'] / X['limit_bal']\n",
        "        X['pay_ratio'] = X['pay_amt1'] / X['limit_bal']\n",
        "        X['age_income_ratio'] = X['age'] / X['limit_bal']\n",
        "        return X\n",
        "\n",
        "# ----- Binning Features ------ #\n",
        "\n",
        "class BinningFeatures(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, n_bins=5, encode='ordinal', strategy='quantile'):\n",
        "        # Explicitly store the parameters as class attributes\n",
        "        self.n_bins = n_bins\n",
        "        self.encode = encode\n",
        "        self.strategy = strategy\n",
        "        self.binner = KBinsDiscretizer(n_bins=n_bins, encode=encode, strategy=strategy)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        self.binner.fit(X)\n",
        "        self.feature_names_in_ = X.columns if isinstance(X, pd.DataFrame) else [f'feature_{i}' for i in range(X.shape[1])]\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_binned = self.binner.transform(X)\n",
        "        return pd.DataFrame(X_binned, columns=[f'{col}_binned' for col in self.feature_names_in_])\n",
        "\n",
        "\n",
        "class SelectiveBinningFeaturesOrdinal(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, features_to_bin, n_bins=5, encode='ordinal', strategy='quantile'):\n",
        "        self.features_to_bin = features_to_bin\n",
        "        self.binner = KBinsDiscretizer(n_bins=n_bins, encode=encode, strategy=strategy)\n",
        "        self.other_features = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # Fit only the features that need binning\n",
        "        self.binner.fit(X[self.features_to_bin])\n",
        "        self.other_features = X.drop(columns=self.features_to_bin).columns.tolist()\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_binned = self.binner.transform(X[self.features_to_bin])\n",
        "        X_binned_df = pd.DataFrame(X_binned, columns=[f'{col}_binned' for col in self.features_to_bin])\n",
        "        return pd.concat([X_binned_df, X[self.other_features].reset_index(drop=True)], axis=1)\n",
        "\n",
        "\n",
        "class SelectiveBinningFeaturesOneHot(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, features_to_bin, n_bins=5, encode='onehot', strategy='quantile'):\n",
        "        self.features_to_bin = features_to_bin\n",
        "        self.binner = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy=strategy)\n",
        "        self.onehot_encoder = OneHotEncoder(sparse_output=False)\n",
        "        self.other_features = None\n",
        "        self.encode = encode\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # Fit only the features that need binning\n",
        "        self.binner.fit(X[self.features_to_bin])\n",
        "        X_binned = self.binner.transform(X[self.features_to_bin])\n",
        "\n",
        "        # Fit the one-hot encoder if needed\n",
        "        if self.encode == 'onehot':\n",
        "            self.onehot_encoder.fit(X_binned)\n",
        "\n",
        "        self.other_features = X.drop(columns=self.features_to_bin).columns.tolist()\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_binned = self.binner.transform(X[self.features_to_bin])\n",
        "\n",
        "        # Apply one-hot encoding if needed\n",
        "        if self.encode == 'onehot':\n",
        "            X_binned = self.onehot_encoder.transform(X_binned)\n",
        "            bin_columns = [f'{col}_bin_{i}' for col in self.features_to_bin for i in range(X_binned.shape[1] // len(self.features_to_bin))]\n",
        "        else:\n",
        "            bin_columns = [f'{col}_binned' for col in self.features_to_bin]\n",
        "\n",
        "        X_binned_df = pd.DataFrame(X_binned, columns=bin_columns)\n",
        "        return pd.concat([X_binned_df, X[self.other_features].reset_index(drop=True)], axis=1)\n",
        "\n",
        "\n",
        "# ----- Interaction Features ------ #\n",
        "\n",
        "class TargetedInteractionFeatures(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, interaction_pairs=None, degree=2, interaction_only=True, include_bias=False):\n",
        "        self.degree = degree\n",
        "        self.interaction_only = interaction_only\n",
        "        self.include_bias = include_bias\n",
        "        self.poly = PolynomialFeatures(degree=self.degree, interaction_only=self.interaction_only, include_bias=self.include_bias)\n",
        "        self.feature_names_in_ = None\n",
        "        self.interaction_pairs = interaction_pairs  # List of tuples specifying which pairs to interact\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        if self.interaction_pairs is None:\n",
        "            self.poly.fit(X)\n",
        "            self.feature_names_in_ = X.columns if isinstance(X, pd.DataFrame) else [f'feature_{i}' for i in range(X.shape[1])]\n",
        "        else:\n",
        "            # Limit the features to the interaction pairs\n",
        "            interaction_columns = [pair[0] for pair in self.interaction_pairs] + [pair[1] for pair in self.interaction_pairs]\n",
        "            X_selected = X[interaction_columns]\n",
        "            self.poly.fit(X_selected)\n",
        "            self.feature_names_in_ = X_selected.columns\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        if self.interaction_pairs is None:\n",
        "            X_poly = self.poly.transform(X)\n",
        "            return pd.DataFrame(X_poly, columns=self.poly.get_feature_names_out(self.feature_names_in_))\n",
        "        else:\n",
        "            # Create interaction features for the selected pairs only\n",
        "            interaction_data = pd.DataFrame(index=X.index)\n",
        "            for pair in self.interaction_pairs:\n",
        "                interaction_term = X[pair[0]] * X[pair[1]]\n",
        "                interaction_data[f\"{pair[0]}_{pair[1]}\"] = interaction_term\n",
        "            return interaction_data\n",
        "\n",
        "\n",
        "class CustomInteractionFeatures(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, interaction_pairs):\n",
        "        self.interaction_pairs = interaction_pairs\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # Store the column names from the DataFrame\n",
        "        self.feature_names_in_ = X.columns if isinstance(X, pd.DataFrame) else [f'feature_{i}' for i in range(X.shape[1])]\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        # Debug: Print the columns available in the DataFrame\n",
        "        # print(\"Columns available for interaction:\", X.columns)\n",
        "\n",
        "        interaction_data = pd.DataFrame(index=X.index)\n",
        "        for pair in self.interaction_pairs:\n",
        "            # Check if both columns in the pair exist in the DataFrame\n",
        "            if pair[0] in X.columns and pair[1] in X.columns:\n",
        "                interaction_data[f\"{pair[0]}_{pair[1]}\"] = X[pair[0]] * X[pair[1]]\n",
        "            else:\n",
        "                # Print a warning with more detail if columns are missing\n",
        "                missing_columns = [col for col in pair if col not in X.columns]\n",
        "                # print(f\"Warning: Columns {missing_columns} from pair {pair} not found in DataFrame columns.\")\n",
        "\n",
        "        # Return the original DataFrame combined with the interaction terms\n",
        "        return pd.concat([X, interaction_data], axis=1)\n",
        "\n",
        "\n",
        "# ----- Plot Feature Groups ------ #\n",
        "\n",
        "def plot_feature_groups(dataframe, feature_groups, hue):\n",
        "    \"\"\"\n",
        "    Plot pairplots for groups of features with color coding by a categorical variable.\n",
        "\n",
        "    Parameters:\n",
        "    dataframe (pd.DataFrame): The DataFrame containing the features.\n",
        "    feature_groups (dict): A dictionary where keys are group names and values are lists of feature names.\n",
        "    hue (str): The name of the categorical column to color by.\n",
        "    \"\"\"\n",
        "    for group_name, features in feature_groups.items():\n",
        "        sns.pairplot(dataframe[features + [hue]], hue=hue, palette='magma')\n",
        "        plt.suptitle(f\"Pairplot of {group_name} Features colored by {hue}\", y=1.02)\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "# Write the script to a file\n",
        "with open(\"loan_data_feature_engineering.py\", \"w\") as file:\n",
        "    file.write(script_content)\n",
        "\n",
        "print(\"Script successfully written to loan_data_feature_engineering.py\")\n",
        "\n",
        "# Reload script to make functions available for use\n",
        "import importlib\n",
        "import loan_data_feature_engineering\n",
        "importlib.reload(loan_data_feature_engineering)\n",
        "from loan_data_feature_engineering import *"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5eYr5yAXoev",
        "outputId": "cebcec6e-e260-4ea0-8922-96795abe5533"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Script successfully written to loan_data_feature_engineering.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vti10kKP3vHE"
      },
      "source": [
        "#### Data Utils Script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "etwO82MT3xeu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14c0703d-3186-478c-d5c3-3b75b0afccd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Script successfully written to loan_data_utils.py\n"
          ]
        }
      ],
      "source": [
        "script_content=r'''\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "import joblib\n",
        "import json\n",
        "import logging\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "#--------   Load and Preprocess Data   --------#\n",
        "\n",
        "def load_data_from_url(url):\n",
        "    try:\n",
        "        df = pd.read_excel(url, header=1)\n",
        "        logging.info(\"Data loaded successfully from URL.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error loading data from URL: {e}\")\n",
        "        return None\n",
        "    return df\n",
        "\n",
        "def clean_column_names(df):\n",
        "    df.columns = [col.lower().replace(' ', '_') for col in df.columns]\n",
        "    return df\n",
        "\n",
        "def remove_id_column(df):\n",
        "    if 'id' in df.columns:\n",
        "        df = df.drop(columns=['id'])\n",
        "    return df\n",
        "\n",
        "def rename_columns(df):\n",
        "    rename_dict = {'pay_0': 'pay_1'}\n",
        "    df = df.rename(columns=rename_dict)\n",
        "    return df\n",
        "\n",
        "def convert_categorical(df, categorical_columns):\n",
        "    df[categorical_columns] = df[categorical_columns].astype('category')\n",
        "    return df\n",
        "\n",
        "def split_features_target(df, target):\n",
        "    X = df.drop(columns=[target])\n",
        "    y = df[target]\n",
        "    return X, y\n",
        "\n",
        "def load_and_preprocess_data(url, categorical_columns, target):\n",
        "    df = load_data_from_url(url)\n",
        "    if df is not None:\n",
        "        df = clean_column_names(df)\n",
        "        df = remove_id_column(df)\n",
        "        df = rename_columns(df)\n",
        "        df = convert_categorical(df, categorical_columns)\n",
        "        X, y = split_features_target(df, target)\n",
        "        return X, y\n",
        "    return None, None\n",
        "\n",
        "#--------   Plot Class Distribution   --------#\n",
        "\n",
        "\n",
        "def plot_class_distribution(y_train, target_name):\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    sns.countplot(x=y_train, hue=y_train, palette='mako')\n",
        "    plt.title(f'Class Distribution in Training Set: {target_name}')\n",
        "    plt.xlabel('Class')\n",
        "    plt.ylabel('Count')\n",
        "    plt.legend([], [], frameon=False)\n",
        "\n",
        "    # Calculate the percentage for each class\n",
        "    total = len(y_train)\n",
        "    class_counts = y_train.value_counts()\n",
        "    for i, count in enumerate(class_counts):\n",
        "        percentage = 100 * count / total\n",
        "        plt.text(i, count, f'{percentage:.1f}%', ha='center', va='bottom')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "#--------   Plot Classification Report Metrics   --------#\n",
        "\n",
        "def plot_classification_report_metrics_threshold(report, method_name, threshold):\n",
        "    \"\"\"\n",
        "    Function to plot the precision, recall, and f1-score metrics for class 0 and class 1.\n",
        "    \"\"\"\n",
        "    # Extract metrics from the report\n",
        "    metrics_df = pd.DataFrame({\n",
        "        'Class': ['Class 0', 'Class 0', 'Class 0', 'Class 1', 'Class 1', 'Class 1'],\n",
        "        'Metric': ['Precision', 'Recall', 'F1-score', 'Precision', 'Recall', 'F1-score'],\n",
        "        'Value': [\n",
        "            report['0']['precision'],\n",
        "            report['0']['recall'],\n",
        "            report['0']['f1-score'],\n",
        "            report['1']['precision'],\n",
        "            report['1']['recall'],\n",
        "            report['1']['f1-score']\n",
        "        ]\n",
        "    })\n",
        "\n",
        "    # Plot the metrics\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(x='Class', y='Value', hue='Metric', data=metrics_df, palette='mako')\n",
        "    plt.title(f'Classification Report Metrics for {method_name} at Threshold {threshold}')\n",
        "    plt.ylabel('Score')\n",
        "    plt.ylim(0, 1)\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.show()\n",
        "\n",
        "# Plotting function with annotations\n",
        "def plot_classification_report_metrics(report, model_name):\n",
        "    metrics_df = pd.DataFrame({\n",
        "        'Class': ['Class 0', 'Class 0', 'Class 0', 'Class 1', 'Class 1', 'Class 1'],\n",
        "        'Metric': ['Precision', 'Recall', 'F1-score', 'Precision', 'Recall', 'F1-score'],\n",
        "        'Value': [\n",
        "            report['0']['precision'],\n",
        "            report['0']['recall'],\n",
        "            report['0']['f1-score'],\n",
        "            report['1']['precision'],\n",
        "            report['1']['recall'],\n",
        "            report['1']['f1-score']\n",
        "        ]\n",
        "    })\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    ax = sns.barplot(x='Class', y='Value', hue='Metric', data=metrics_df, palette='mako')\n",
        "    plt.title(f'Classification Report Metrics for {model_name}')\n",
        "    plt.ylabel('Score')\n",
        "    plt.ylim(0, 1)\n",
        "    plt.legend(loc='lower right')\n",
        "\n",
        "    # Annotate the bars with the values\n",
        "    for p in ax.patches:\n",
        "        height = p.get_height()\n",
        "        ax.annotate(f'{height:.2f}',\n",
        "                    (p.get_x() + p.get_width() / 2., height),\n",
        "                    ha='center', va='center',\n",
        "                    xytext=(0, 5),\n",
        "                    textcoords='offset points')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "#--------   Evaluate and Capture Metrics   --------#\n",
        "\n",
        "# Function to evaluate model\n",
        "def evaluate_model(pipeline, X_train, X_test, y_train, y_test, model_name, experiment_name):\n",
        "    logger.info(f\"Training and evaluating model: {model_name} ({experiment_name})\")\n",
        "\n",
        "    # Fit the pipeline\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = pipeline.predict(X_test)\n",
        "\n",
        "    # Capture classification report\n",
        "    report = classification_report(y_test, y_pred, output_dict=True)\n",
        "\n",
        "    # Extract relevant metrics\n",
        "    metrics = {\n",
        "        'Model': model_name,\n",
        "        'Experiment': experiment_name,\n",
        "        'Recall_0': report['0']['recall'],\n",
        "        'Precision_0': report['0']['precision'],\n",
        "        'F1_0': report['0']['f1-score'],\n",
        "        'Recall_1': report['1']['recall'],\n",
        "        'Precision_1': report['1']['precision'],\n",
        "        'F1_1': report['1']['f1-score'],\n",
        "        'F1_Macro': report['macro avg']['f1-score'],\n",
        "        'Accuracy': report['accuracy']\n",
        "    }\n",
        "    return metrics\n",
        "\n",
        "#--------   Custom Threshold Classifier   --------#\n",
        "\n",
        "# Define a custom classifier to handle class-specific threshold\n",
        "class ThresholdClassifier(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, base_classifier, threshold=0.3):\n",
        "        self.base_classifier = base_classifier\n",
        "        self.threshold = threshold\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.base_classifier.fit(X, y)\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        return self.base_classifier.predict_proba(X)\n",
        "\n",
        "    def predict(self, X):\n",
        "        proba = self.predict_proba(X)\n",
        "        return (proba[:, 1] >= self.threshold).astype(int)\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "# Write the script to a file\n",
        "with open(\"loan_data_utils.py\", \"w\") as file:\n",
        "    file.write(script_content)\n",
        "\n",
        "print(\"Script successfully written to loan_data_utils.py\")\n",
        "# Reload script to make functions available for use\n",
        "import importlib\n",
        "import loan_data_utils\n",
        "importlib.reload(loan_data_utils)\n",
        "\n",
        "from loan_data_utils import *"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPBGFBWdm8ujs1PbD9EB7Al",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}