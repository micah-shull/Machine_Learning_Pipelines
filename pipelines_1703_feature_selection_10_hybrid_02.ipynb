{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOdvuGqTQPzirYyiGDXgmZ+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/pipelines/blob/main/pipelines_1703_feature_selection_10_hybrid_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary of the Hybrid Feature Selection Pipeline:\n",
        "\n",
        "This pipeline implements a multi-stage feature selection process by combining filter, wrapper, and embedded methods:\n",
        "\n",
        "1. **Preprocessing**:\n",
        "   - The data is preprocessed using a `ColumnTransformer` that handles both numerical and categorical features. Numerical features are imputed and scaled, while categorical features are imputed and one-hot encoded.\n",
        "\n",
        "2. **SMOTE (Synthetic Minority Over-sampling Technique)**:\n",
        "   - SMOTE is applied to the preprocessed data to address class imbalance by oversampling the minority class.\n",
        "\n",
        "3. **Filter Methods**:\n",
        "   - **Variance Threshold**: Removes features with low variance (below 0.01). This step is crucial for quickly discarding features that do not vary much and are therefore unlikely to be informative.\n",
        "   - **Univariate Feature Selection (SelectKBest with Mutual Information)**: Selects the top 20 features based on mutual information, focusing on features most correlated with the target variable.\n",
        "\n",
        "4. **Wrapper Methods**:\n",
        "   - **Recursive Feature Elimination (RFE)**: Iteratively selects features based on model performance, starting with all features and removing the least important ones until the optimal subset is identified.\n",
        "\n",
        "5. **Embedded Methods**:\n",
        "   - **SelectFromModel with RandomForestClassifier**: Final feature selection is performed using a tree-based model that ranks features based on their importance in the model.\n",
        "\n",
        "6. **Classification**:\n",
        "   - The final model uses Logistic Regression with a custom threshold of 0.3 to improve the focus on specific classes (e.g., higher recall for the minority class).\n",
        "\n",
        "### Why Using a Pipeline Is Beneficial:\n",
        "\n",
        "1. **Consistency and Reproducibility**:\n",
        "   - A pipeline ensures that all preprocessing, feature selection, and model training steps are applied consistently every time, reducing the chances of errors and improving reproducibility.\n",
        "\n",
        "2. **Modularity and Reusability**:\n",
        "   - Each step of the pipeline is modular, making it easy to adjust or replace components (e.g., swapping out `SelectKBest` for a different method). This modularity allows for quick experimentation without rewriting the entire process.\n",
        "\n",
        "3. **Ease of Hyperparameter Tuning**:\n",
        "   - Pipelines integrate seamlessly with tools like `GridSearchCV`, enabling hyperparameter tuning across all stages of the process (e.g., number of features to select in RFE). This centralizes tuning and simplifies optimization.\n",
        "\n",
        "4. **Streamlined Workflows**:\n",
        "   - By chaining steps together, pipelines reduce the need to manually track and pass data between each stage. This is especially useful when complex transformations and selections are involved, ensuring everything is automated.\n",
        "\n",
        "5. **Built-In Cross-Validation**:\n",
        "   - Pipelines naturally support cross-validation, ensuring that the entire process, including feature selection, is cross-validated, preventing data leakage and giving more reliable performance metrics.\n",
        "\n",
        "6. **Documentation and Clarity**:\n",
        "   - The pipeline structure is easy to document and understand. Each step is clearly defined, making it simpler for others to follow and reproduce the process. It also provides a single source of truth for the end-to-end workflow.\n",
        "\n",
        "This pipeline efficiently combines multiple feature selection methods to progressively narrow down the feature set, ensuring the final model is trained on the most relevant and impactful features, all while leveraging the strengths of each method."
      ],
      "metadata": {
        "id": "Sr1dn2eaNyvo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.feature_selection import SelectKBest, mutual_info_classif, SelectFromModel, VarianceThreshold, RFE\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
        "import joblib\n",
        "import json\n",
        "import warnings\n",
        "from loan_data_utils import load_and_preprocess_data, plot_classification_report_metrics, plot_selected_features, ThresholdClassifier\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "# Define your URL, categorical columns, and target\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\"\n",
        "categorical_columns = ['sex', 'education', 'marriage']\n",
        "target = 'default_payment_next_month'\n",
        "\n",
        "# Load and preprocess data\n",
        "X, y = load_and_preprocess_data(url, categorical_columns, target)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Identify numeric and categorical columns\n",
        "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "categorical_features = X.select_dtypes(include=['category']).columns.tolist()\n",
        "\n",
        "# Define the column transformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', Pipeline(steps=[\n",
        "            ('imputer', SimpleImputer(strategy='median')),\n",
        "            ('scaler', StandardScaler())\n",
        "        ]), numeric_features),\n",
        "        ('cat', Pipeline(steps=[\n",
        "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "            ('encoder', OneHotEncoder(drop='first'))\n",
        "        ]), categorical_features)\n",
        "    ])\n",
        "\n",
        "# Use the custom classifier with Logistic Regression and a threshold of 0.3 for class 1\n",
        "base_classifier = LogisticRegression(max_iter=3000)\n",
        "\n",
        "# Define base classifiers for stacking\n",
        "base_classifiers = [\n",
        "    ('lr', LogisticRegression(max_iter=3000)),\n",
        "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
        "    ('gb', GradientBoostingClassifier(n_estimators=100, random_state=42))\n",
        "]\n",
        "\n",
        "# Define the stacking ensemble with the selected meta-classifier\n",
        "stacking_ensemble = StackingClassifier(\n",
        "    estimators=base_classifiers,\n",
        "    final_estimator=LogisticRegression(max_iter=3000),\n",
        "    cv=5\n",
        ")\n",
        "\n",
        "# Wrap the stacking ensemble with the ThresholdClassifier\n",
        "classifier = ThresholdClassifier(stacking_ensemble, threshold=0.3)\n",
        "\n",
        "# Set up the pipeline\n",
        "hybrid_stacking_pipeline = ImbPipeline(steps=[\n",
        "    ('preprocessor', preprocessor),  # Preprocessing step (imputer, scaler, encoder)\n",
        "    ('smote', SMOTE(random_state=42)),  # Apply SMOTE\n",
        "    # Filter stage: Variance Thresholding and SelectKBest (using mutual_info_classif)\n",
        "    ('var_thresh', VarianceThreshold(threshold=0.01)),\n",
        "    ('select_kbest', SelectKBest(mutual_info_classif, k=20)),\n",
        "    # Wrapper stage: Recursive Feature Elimination (RFE)\n",
        "    ('rfe', RFE(estimator=LogisticRegression(max_iter=3000), n_features_to_select=10)),\n",
        "    # Embedded stage: SelectFromModel using RandomForest\n",
        "    ('select_from_model', SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42))),\n",
        "    # Final Stacking Ensemble with Threshold Adjustment\n",
        "    ('classifier', classifier)\n",
        "])\n",
        "\n",
        "# Set up the parameter grid to tune the RFE and Stacking\n",
        "param_grid = {\n",
        "    'rfe__n_features_to_select': np.linspace(5, 20, 5, dtype=int).tolist(),\n",
        "    'classifier__base_classifier__final_estimator__C': [0.01, 0.1, 1]  # Tuning Logistic Regression in the stacking meta-classifier\n",
        "}\n",
        "\n",
        "# Use StratifiedKFold for cross-validation\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Set up GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=hybrid_stacking_pipeline, param_grid=param_grid, cv=cv, scoring='recall', n_jobs=-1)\n",
        "\n",
        "# Fit the grid search\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best pipeline\n",
        "best_pipeline = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate on the test set\n",
        "y_pred = best_pipeline.predict(X_test)\n",
        "\n",
        "# Generate the classification report\n",
        "report = classification_report(y_test, y_pred, output_dict=True)\n",
        "\n",
        "# Save the classification report to a JSON file\n",
        "with open('classification_reports_stacking_hybrid_with_threshold.json', 'w') as f:\n",
        "    json.dump(report, f, indent=4)\n",
        "print(\"Classification report saved to 'classification_reports_stacking_hybrid_with_threshold.json'\")\n",
        "\n",
        "# Extract the selected feature names\n",
        "selected_features = np.array(preprocessor.get_feature_names_out())[best_pipeline.named_steps['var_thresh'].get_support()][best_pipeline.named_steps['select_kbest'].get_support()][best_pipeline.named_steps['rfe'].support_]\n",
        "selected_feature_names = selected_features.tolist()\n",
        "\n",
        "# Save the selected features to a JSON file\n",
        "with open('selected_features_stacking_hybrid.json', 'w') as f:\n",
        "    json.dump(selected_feature_names, f, indent=4)\n",
        "print(\"Selected features saved to 'selected_features_stacking_hybrid.json'\")\n",
        "\n",
        "# Plot the classification report metrics\n",
        "plot_classification_report_metrics(report, 'Stacking Hybrid Pipeline with Threshold')\n",
        "\n",
        "# Print the classification report\n",
        "print(\"Test Set Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Plot the selected features for visual analysis\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=range(len(selected_feature_names)), y=selected_feature_names, palette='viridis')\n",
        "plt.xlabel('Importance')\n",
        "plt.title('Selected Feature Names from Stacking Hybrid Pipeline')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3GNguOstpGES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Structure of a pipeline in scikit-learn\n",
        "\n",
        "The structure of a pipeline in scikit-learn can be visualized like a nested dictionary, where each step (or component) is a key that points to a specific model or transformer. Just like in a dictionary, you need to know the correct keys to access the nested components.\n",
        "\n",
        "### How It Works:\n",
        "In a pipeline, each step has a name (a key) and a model (a value). When you're using GridSearchCV or accessing components directly, you need to reference these keys correctly. Here’s a breakdown:\n",
        "\n",
        "#### Example Pipeline Structure:\n",
        "Let’s say you have a pipeline like this:\n",
        "\n",
        "```python\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', ThresholdClassifier(\n",
        "        base_classifier=StackingClassifier(\n",
        "            estimators=[\n",
        "                ('lr', LogisticRegression(max_iter=3000)),\n",
        "                ('rf', RandomForestClassifier(random_state=42)),\n",
        "                ('gb', GradientBoostingClassifier(random_state=42))\n",
        "            ],\n",
        "            final_estimator=LogisticRegression(max_iter=3000)\n",
        "        ),\n",
        "        threshold=0.3\n",
        "    ))\n",
        "])\n",
        "```\n",
        "\n",
        "This can be visualized like this:\n",
        "\n",
        "```plaintext\n",
        "Pipeline\n",
        "│\n",
        "├── preprocessor (ColumnTransformer)\n",
        "└── classifier (ThresholdClassifier)\n",
        "    ├── base_classifier (StackingClassifier)\n",
        "    │   ├── estimators\n",
        "    │   │   ├── lr (LogisticRegression)\n",
        "    │   │   ├── rf (RandomForestClassifier)\n",
        "    │   │   └── gb (GradientBoostingClassifier)\n",
        "    │   └── final_estimator (LogisticRegression)\n",
        "    └── threshold (float)\n",
        "```\n",
        "\n",
        "When you want to access the `C` parameter of the `final_estimator` (which is nested inside the `base_classifier`), you need to reference it like this:\n",
        "\n",
        "```plaintext\n",
        "classifier__base_classifier__final_estimator__C\n",
        "```\n",
        "\n",
        "This is equivalent to accessing a deeply nested value in a dictionary:\n",
        "\n",
        "```python\n",
        "nested_dict = {\n",
        "    'classifier': {\n",
        "        'base_classifier': {\n",
        "            'final_estimator': {\n",
        "                'C': [0.01, 0.1, 1]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "```\n",
        "\n",
        "### Why This is Important:\n",
        "- **Correct Key Path**: Just like in a dictionary, you need to know the exact key path to access the nested values. If you miss a key or use the wrong one, scikit-learn can’t find the parameter, leading to errors.\n",
        "- **Nested Models**: When working with complex pipelines, especially those involving custom wrappers or ensemble models, the nesting can get deep. Knowing the correct path is essential to ensure GridSearchCV and similar tools can properly access and tune your parameters.\n",
        "\n",
        "### Key Takeaway:\n",
        "- Think of the pipeline steps as a nested dictionary. To access or tune a parameter, you need to specify the exact path using keys like `classifier__base_classifier__final_estimator__C`.\n",
        "- Understanding this structure allows you to unlock the full power of scikit-learn pipelines and effectively use hyperparameter tuning in complex models.\n",
        "\n",
        "So yes, you’re spot on—treating it like a nested dictionary and understanding the hierarchy is exactly how to think about it!"
      ],
      "metadata": {
        "id": "8-DYeFwsotmr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loand Data Utils"
      ],
      "metadata": {
        "id": "GcDYGnyMNrz6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Noyu4FFNmAM",
        "outputId": "d53b785a-9124-47bb-a03a-cc4fb07d8d02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Script successfully written to loan_data_utils.py\n"
          ]
        }
      ],
      "source": [
        "script_content=r'''\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.model_selection import train_test_split\n",
        "import joblib\n",
        "import json\n",
        "import logging\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "#--------   Load and Preprocess Data   --------#\n",
        "\n",
        "def load_data_from_url(url):\n",
        "    try:\n",
        "        df = pd.read_excel(url, header=1)\n",
        "        logging.info(\"Data loaded successfully from URL.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error loading data from URL: {e}\")\n",
        "        return None\n",
        "    return df\n",
        "\n",
        "def clean_column_names(df):\n",
        "    df.columns = [col.lower().replace(' ', '_') for col in df.columns]\n",
        "    return df\n",
        "\n",
        "def remove_id_column(df):\n",
        "    if 'id' in df.columns:\n",
        "        df = df.drop(columns=['id'])\n",
        "    return df\n",
        "\n",
        "def rename_columns(df):\n",
        "    rename_dict = {'pay_0': 'pay_1'}\n",
        "    df = df.rename(columns=rename_dict)\n",
        "    return df\n",
        "\n",
        "def convert_categorical(df, categorical_columns):\n",
        "    df[categorical_columns] = df[categorical_columns].astype('category')\n",
        "    return df\n",
        "\n",
        "def split_features_target(df, target):\n",
        "    X = df.drop(columns=[target])\n",
        "    y = df[target]\n",
        "    return X, y\n",
        "\n",
        "def load_and_preprocess_data(url, categorical_columns, target):\n",
        "    df = load_data_from_url(url)\n",
        "    if df is not None:\n",
        "        df = clean_column_names(df)\n",
        "        df = remove_id_column(df)\n",
        "        df = rename_columns(df)\n",
        "        df = convert_categorical(df, categorical_columns)\n",
        "        X, y = split_features_target(df, target)\n",
        "        return X, y\n",
        "    return None, None\n",
        "\n",
        "#--------   Plot Class Distribution   --------#\n",
        "\n",
        "\n",
        "def plot_class_distribution(y_train, target_name):\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    sns.countplot(x=y_train, hue=y_train, palette='mako')\n",
        "    plt.title(f'Class Distribution in Training Set: {target_name}')\n",
        "    plt.xlabel('Class')\n",
        "    plt.ylabel('Count')\n",
        "    plt.legend([], [], frameon=False)\n",
        "\n",
        "    # Calculate the percentage for each class\n",
        "    total = len(y_train)\n",
        "    class_counts = y_train.value_counts()\n",
        "    for i, count in enumerate(class_counts):\n",
        "        percentage = 100 * count / total\n",
        "        plt.text(i, count, f'{percentage:.1f}%', ha='center', va='bottom')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "#--------   Custom Classifier   --------#\n",
        "\n",
        "# Define a custom classifier to handle class-specific threshold\n",
        "class ThresholdClassifier(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, base_classifier, threshold=0.3):\n",
        "        self.base_classifier = base_classifier\n",
        "        self.threshold = threshold\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.base_classifier.fit(X, y)\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        return self.base_classifier.predict_proba(X)\n",
        "\n",
        "    def predict(self, X):\n",
        "        proba = self.predict_proba(X)\n",
        "        return (proba[:, 1] >= self.threshold).astype(int)\n",
        "\n",
        "\n",
        "#--------   Evaluate and Capture Metrics   --------#\n",
        "\n",
        "# Function to evaluate model\n",
        "def evaluate_model(pipeline, X_train, X_test, y_train, y_test, model_name, experiment_name):\n",
        "    logger.info(f\"Training and evaluating model: {model_name} ({experiment_name})\")\n",
        "\n",
        "    # Fit the pipeline\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = pipeline.predict(X_test)\n",
        "\n",
        "    # Capture classification report\n",
        "    report = classification_report(y_test, y_pred, output_dict=True)\n",
        "\n",
        "    # Extract relevant metrics\n",
        "    metrics = {\n",
        "        'Model': model_name,\n",
        "        'Experiment': experiment_name,\n",
        "        'Recall_0': report['0']['recall'],\n",
        "        'Precision_0': report['0']['precision'],\n",
        "        'F1_0': report['0']['f1-score'],\n",
        "        'Recall_1': report['1']['recall'],\n",
        "        'Precision_1': report['1']['precision'],\n",
        "        'F1_1': report['1']['f1-score'],\n",
        "        'F1_Macro': report['macro avg']['f1-score'],\n",
        "        'Accuracy': report['accuracy']\n",
        "    }\n",
        "    return metrics\n",
        "\n",
        "\n",
        "# #--------   Plot Classification Report  --------#\n",
        "\n",
        "# Plotting function with annotations\n",
        "def plot_classification_report_metrics(report, model_name):\n",
        "    metrics_df = pd.DataFrame({\n",
        "        'Class': ['Class 0', 'Class 0', 'Class 0', 'Class 1', 'Class 1', 'Class 1'],\n",
        "        'Metric': ['Precision', 'Recall', 'F1-score', 'Precision', 'Recall', 'F1-score'],\n",
        "        'Value': [\n",
        "            report['0']['precision'],\n",
        "            report['0']['recall'],\n",
        "            report['0']['f1-score'],\n",
        "            report['1']['precision'],\n",
        "            report['1']['recall'],\n",
        "            report['1']['f1-score']\n",
        "        ]\n",
        "    })\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    ax = sns.barplot(x='Class', y='Value', hue='Metric', data=metrics_df, palette='mako')\n",
        "    plt.title(f'Classification Report Metrics for {model_name}')\n",
        "    plt.ylabel('Score')\n",
        "    plt.ylim(0, 1)\n",
        "    plt.legend(loc='lower right')\n",
        "\n",
        "    # Annotate the bars with the values\n",
        "    for p in ax.patches:\n",
        "        height = p.get_height()\n",
        "        ax.annotate(f'{height:.2f}',\n",
        "                    (p.get_x() + p.get_width() / 2., height),\n",
        "                    ha='center', va='center',\n",
        "                    xytext=(0, 5),\n",
        "                    textcoords='offset points')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def plot_classification_report_thresholds(report, method_name, threshold):\n",
        "    \"\"\"\n",
        "    Function to plot the precision, recall, and f1-score metrics for class 0 and class 1.\n",
        "    \"\"\"\n",
        "    # Extract metrics from the report\n",
        "    metrics_df = pd.DataFrame({\n",
        "        'Class': ['Class 0', 'Class 0', 'Class 0', 'Class 1', 'Class 1', 'Class 1'],\n",
        "        'Metric': ['Precision', 'Recall', 'F1-score', 'Precision', 'Recall', 'F1-score'],\n",
        "        'Value': [\n",
        "            report['0']['precision'],\n",
        "            report['0']['recall'],\n",
        "            report['0']['f1-score'],\n",
        "            report['1']['precision'],\n",
        "            report['1']['recall'],\n",
        "            report['1']['f1-score']\n",
        "        ]\n",
        "    })\n",
        "\n",
        "    # Plot the metrics\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(x='Class', y='Value', hue='Metric', data=metrics_df, palette='mako')\n",
        "    plt.title(f'Classification Report Metrics for {method_name} at Threshold {threshold}')\n",
        "    plt.ylabel('Score')\n",
        "    plt.ylim(0, 1)\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.show()\n",
        "\n",
        "# #--------   Plot Selected Features  --------#\n",
        "\n",
        "def plot_selected_features(selected_features, model_name):\n",
        "    features = list(selected_features.keys())\n",
        "    importances = list(selected_features.values())\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.barplot(x=importances, y=features, palette='viridis')\n",
        "    plt.title(f'Selected Features and their Importance for {model_name}')\n",
        "    plt.xlabel('Importance')\n",
        "    plt.ylabel('Feature')\n",
        "    plt.show()\n",
        "'''\n",
        "\n",
        "# Write the script to a file\n",
        "with open(\"loan_data_utils.py\", \"w\") as file:\n",
        "    file.write(script_content)\n",
        "\n",
        "print(\"Script successfully written to loan_data_utils.py\")\n",
        "# Reload script to make functions available for use\n",
        "import importlib\n",
        "import loan_data_utils\n",
        "importlib.reload(loan_data_utils)\n",
        "\n",
        "from loan_data_utils import *"
      ]
    }
  ]
}